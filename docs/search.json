[
  {
    "objectID": "report.html",
    "href": "report.html",
    "title": "Predict the President",
    "section": "",
    "text": "In this study, the primary objective was to develop a classifier capable of attributing sentences to specific South African presidents. This is to be done with three different classifiers To achieve this, the research delved into a critical task of feature set selection, exploring a range of potential options, including lexical, character-based, and TF-IDF features set. Among the array of feature set options explored, two feature sets stood out as the most promising choices for the final models: TF-IDF and a character set consisting of 4500 features. Notably, the Feedforward Neural Network (FFNN) consistently outperformed other classifiers in this study. However, it is essential to acknowledge that TF-IDF exhibited a certain degree of variability in its performance as a feature set. In light of these findings, it is recommended that future research considers employing the FFNN in conjunction with a complete TF-IDF feature set, as this combination displayed superior performance when compared to other models under investigation. This insight serves as a valuable contribution to the development of classifiers for sentence attribution to specific authors, and it highlights the potential for further refinement in this fascinating domain of study."
  },
  {
    "objectID": "report.html#pre-processing",
    "href": "report.html#pre-processing",
    "title": "Predict the President",
    "section": "Pre-Processing",
    "text": "Pre-Processing\nAfter obtaining the data, the first line of each speech is removed. This first line is the date the address is held. The data is then tokenized by sentence. Only sentences with over two words remained for the rest of the analysis. The reason this is done is because the sentences with less than 3 words seemed to either either be made up of digits or unfinished words, most of the time.\nFollowing this many different considerations have been made in the Supplementary to find the best representation of the data for classification. Both lexical and character features are considered; the inclusion and exclusion of function words (stop words include function words); using token frequencies and TF-IDF are considered; different sized word bags are considered; the inclusion of case and punctuation are are also considered. Lexical features regard tokens as a sequence of words, where a unigram is a single word, a bigram are two contiguous words, a trigram is three and so forth. Character features are a sequence of contiguous characters also using n-grams to characterise how many characters are present in each feature. These two types of features qualify as two of the most basic markers used for identifying an author’s style, where lexical features are slightly more complex than character features (Stamatatos (2009)). Of the many options explored in the Supplementary, only two of the models will be represented here. The first approach will use unigrams of words to represent lexical features, while the second uses character features with a 4 character n-gram, this is how we will distinguish between the two going forward. These are important distinctions at this stage as they determine how the data is organised.\nIn the Supplementary, both a topic-based approach and style-based approach were looked into. It was found that the style-based approach performed better than topic-based approach, in most scenarios. This was illustrated in the improvement of model performance of the models with the inclusion of function words. This idea will be furthered explored below.\n\nLexical Features\nThere is complexity that comes with using lexical features; the sparsity within the bag-of-words of such features was a the reason why capitalization was not deemed appropriate in conjunction with lexical features (Stamatatos (2008)). Capitalization within the transcription of a speech can only indicate the beginning of an idea, sentiment cannot be captured by such. Whereas, in the context of a book, capitalization can be use to convey a strong meaning in a character’s words.\nFirst the speeches are tokenized into sentences and are given sentence ids. Thereafter sentences were cleaned quite strictly. All digits are removed using a regex operation. Contractions are replaced with their long form via regex pattern matching (Rinker (2018)). Punctuation is maintained as punctuation is able to characterise syntatic information (Zheng et al. (2006)). Punctuation is maintained, however, special unicode characters, astrixes, and other punctuation that is not commonly seen in text are removed, only full stops, question marks, commas, exclamation marks, brackets, apostrophes and curly brackets are maintained. Lemmatization is performed as commonly appearing words appear in different forms for each president. A lemma is a set of words that have the same stem, where each variation of the word is called a word form. Lemmatization is a process that is able to determine that two words have the same root. To lemmatize is to change wordforms into their root. This process of lemmatization is done by morphological parsing, where this separates the morphenes into parts. A morphene is a smaller building block of a word. Lemmatization was done by using a dictionary of common wordforms, if a word is a part of a lemma, then it will be matched to the dictionary and replaced by the morphenes that make it up. After performing this lemmatization, the affixes, additional morphenes that aren’t the stem, are still present in the data. Affixes are often random letters that will appear (Jurafsky and Martin (2023)). The final step in cleaning is to remove these affixes\n\n\nCharacter Features\nCharacter features are more simplistic than lexical features and are able to capture stylistic nuances. They are able to capture some lexical and contextual information. They do however carry a lot of redundancies resulting in high dimensionality.\nThe data here is cleaned differently. In the Supplementary, it is shown that the feature space that includes both punctuation and capitalization performs the best of all the character spaces. A character n-gram of both 3 and 4 features were assessed, and both are decently sized n-grams. Here the character n-gram of 4 contiguous characters will be used.\nFirst the speeches are tokenized into sentences, and given an id. Where previously the text would have been set to lower case and the punctuation was included, here both the text maintains it’s capitalization and the punctuation remain. To clean these sentences, all digits were removed, followed by the lemmatization of wordforms to stems.\n\n\nImbalanced Classes\nThe data has imbalanced classes present. This means that the distribution of the data across classes is skewed. Motlanthe and de Klerk both only gave one speech. This was Motlanthe’s and de Klerk’s outgoing speeches; every other presidents managed to give more than one speech.\nThere are a number of ways to deal with imbalanced classes, for future work one might look into Stamatatos (2008) to get an idea of what strategies do actually work on text classification tasks. In this report, these outlying classes were removed from the remainder of the analysis."
  },
  {
    "objectID": "report.html#bag-of-words",
    "href": "report.html#bag-of-words",
    "title": "Predict the President",
    "section": "Bag of Words",
    "text": "Bag of Words\nIt is important that we get the data into a format that can be used by the classifiers. A bag of words is one such way to do this. The text is placed into something called a bag of words. This is an unordered set of words that contain the frequency of each word in the document (Jurafsky and Martin (2023)). For both lexical and character features, the pre-processed sentences need to be tokenized down to their desired level. The lexical features are tokenized as unigrams of words, while including punctuation as individual words; the character features are tokenized as n-grams with 4 characters, with the inclusion of punctuation and capitalization. Both types of features included function words, as this was decided to be a stylometric exercise as per the Supplementary. In both cases, the sentence id was maintained over each word to allocate which sentence it belongs to.\nIn both cases, lexical and character features, the most important features are the most frequent. With this understanding, a crude feature selection, can be conducted for both.\nAfter tokenizing, most frequent features were selected. This was tried as 200, 500, 2000, 4500 and all the tokens for both the lexical features and the character features. After making these selections, a word bag was created for each selection of top features."
  },
  {
    "objectID": "report.html#term-frequency---inverse-document-frequency-tf-idf",
    "href": "report.html#term-frequency---inverse-document-frequency-tf-idf",
    "title": "Predict the President",
    "section": "Term Frequency - Inverse Document Frequency (TF-IDF)",
    "text": "Term Frequency - Inverse Document Frequency (TF-IDF)\nFor comparison, TF-IDF was also considered. Term frequency, TF, is a measure of how frequently a term occurs in a document. TF can be found as,\n\\[\\text{tf(term t in document i)} = \\frac{\\text{Number of times term t appears in document i}}{\\text{Number of terms in document i}}.\\]\nInverse document term frequency downweights terms that are frequent in a collection of documents and upweights terms that are not frequent within a collection of documents. It is calculated as,\n\\[\\text{idf(term t)} = \\text{ln}\\left(\\frac{\\text{Number of documents in corpus}}{\\text{Number of documents containing term t}}\\right).\\]\nTF-IDF is the multiplication of the two metrics (Silge and Robinson (2017), Durbach (2023) ). TF-IDF measures the importance of a word to a document in a collection. In this analysis this has been done by considering one document to be a sentence. Following this intuition, the quantities above can be calculated.\nSince TF-IDF determines what features are important to which document, this can be seen as a topic-based classification, as function words and commonly occurring tokens among the documents will be downweighted resulting in semantic features being strongly weighted. For TF-IDF, all tokens were kept, because some of the highly weighted scores may only appear once in a document and as a result may act as quite a poor classifier out-of-sample, and so it is necessary to leave all possible tokens."
  },
  {
    "objectID": "report.html#train-validation-and-testing-splits",
    "href": "report.html#train-validation-and-testing-splits",
    "title": "Predict the President",
    "section": "Train, Validation and Testing Splits",
    "text": "Train, Validation and Testing Splits\nFor the training, validation and test splits, it would be wise to select sample sizes by sentence and not by word, this will represent each row in the bag of words.\nThe data will have a 60:20:20 split of training, validation and test data. The training set will represent 60% of the total number of sentences available, the validation set will represent 20% and the test set will be the remaining 20%. These splits are done for each word bag and for tf-idf."
  },
  {
    "objectID": "report.html#naive-bayes-classifier",
    "href": "report.html#naive-bayes-classifier",
    "title": "Predict the President",
    "section": "Naive Bayes Classifier",
    "text": "Naive Bayes Classifier\nA multinomial naive Bayes classifier was used to classify text. This is a probabilistic classifier, given the document \\(d\\) and classes \\(c \\in C\\), the classifier will predict the class, \\(\\hat{c}\\), with the maximum posterior probability,\n\\[\\hat{c} = \\underset{c\\in C}{\\arg\\max} P(c|d)\\]\nUsing Bayes rule, the above prediction can be further broken down into,\n\\[\\hat{c} = \\underset{c\\in C}{\\arg\\max} P(c|d) = \\underset{c\\in C}{\\arg\\max} \\frac{P(d|c)P(c)}{P(d)}\\]\nThis can be further simplified by to,\n\\[\\hat{c} = \\underset{c\\in C}{\\arg\\max} P(d|c)P(c) \\]\nas \\(P(d)\\) stays constant for all classes, which allows us to cancel this out. This equation above is made up of the likelihood of the document and prior probability of the class. The documents can subsequently be divided up into features, be it our lexical or character features above. Following this great number of features, \\(\\{f_i : i = 1,2,...,n\\}\\), this classifier makes two simplifying assumptions that would infer it’s naivety, that the tokens are position independent and that the joint likelihood of the features also maintain independence,\n\\[P(d|c) = P(f_1, f_2, ..., f_n|c) = P(f_1|c)P(f_2|c)...P(f_n|c)\\] The final equation of the naive Bayes classifier then becomes,\n\\[\\hat{c}_{NB} = \\underset{c\\in C}{\\arg\\max} P(c) \\prod_{f\\in F} P(f|c)\\]\n(Jurafsky and Martin (2023)).\nThis classifier was made with no Laplace smoothing. Laplace smoothing, will be looked into for the final models. This is a simple smoothing algorithm that can be easily implemented in this classifier. It simply adds to the count the number specified before finding probabilities (Jurafsky and Martin (2023))."
  },
  {
    "objectID": "report.html#support-vector-machines",
    "href": "report.html#support-vector-machines",
    "title": "Predict the President",
    "section": "Support Vector Machines",
    "text": "Support Vector Machines\nSupport vector machines, SVM, are widely used in the space of text classification (Stamatatos (2009)). SVMs perform linear operations in upto an infinite number of dimensions. If data is not linearly separable, SVMs can be used to linearly separable in a higher dimension. SVMs allow to find an optimal separating hyperplane for separable and non-separable data.\nThe goal is to evaluate the hyperplane such that it creates the greatest margin between the training points of each class and itself. The margin is the distance between the nearest data point to the plane.\nIf the feature space is non-linenar we would look to use a set of basis expansions as our input space,\n\\[\nh(x_i) = (h_1(x_i), h_2(x_i), ..., h_M(x_i)), \\quad i = 1, 2, ..., N\n\\]\nthe inner product of these basis expansions, \\(h(x)\\), will represent the desired kernal, \\(K(x, x')\\), used to move feature space to a higher dimension. The hyperplane will therefore be defined as,\n\\[\n{f}(x) = h(x)^T\\beta + \\beta_0\n\\]\nwhere \\(h(x)\\) is the kernal. It is difficult to have a hyperplane that perfectly classifies, to tune the model some slack is allowed. This is a soft-margin SVM. Slack are observations that are allowed to be missclassified. SVMs maximise the margin with some regularization,\n\\[\n\\frac{1}{2}\\beta^T\\beta + C\\sum_{n=1}^N \\zeta_n\n\\]\nsubject to,\n\\[\ny_i(h(x)^T\\beta + \\beta_0) \\geq 1 - \\zeta_n, \\quad n = 1,2,...,N \\\\\n\\zeta_n \\geq 0, \\quad\\quad n = 1,2,...,N\n\\]\nthis is a convex optimisation problem (Hastie, Tibshirani, and Friedman (2001), Pienaar (2023b) ) . For multiclass problems the package provided by Meyer et al. (2023) uses a one-versus-one approach. This will fit all pairwise classifiers and use a voting system to pick the best ones.\nA base model has been used to explore our data, hyperparameter tuning is used to regularize the model, by using cost constraint, \\(C\\). This constraint allows us to specify the cost of the violation to the margin. A small cost indicates a large margin, meaning many support vectors will fall on or within the margin. A large cost indicates a narrow margin, indicating much less support vectors on the margin or violating it. The cost was tested at values: 0.1, 1, 10, 50, 100 and 1000"
  },
  {
    "objectID": "report.html#feed-forward-neural-networks",
    "href": "report.html#feed-forward-neural-networks",
    "title": "Predict the President",
    "section": "Feed Forward Neural Networks",
    "text": "Feed Forward Neural Networks\nA neural network is a series of non-linear basis functions, this is handled via interconnected nodes over a number of layers. Each layer has a number of nodes and is connected to every other node in the layer before (if any) and after it (if any). The case where there are no layers before the current layer is for the input layer, which is a layer that takes in data. The number of nodes present at the input layer are equal to the number of features in the bag of words. Similarly, the case where there are no layers after the current layer is for the output layer, in this text classification task. Since the task is that of multinomial classification, the number of nodes on this output layer is equivalent to the number of classes available. The layers between the input layer and the output layer are called hidden layers. The output of each layer is passed on as the input to the next layer. The hidden layers are capable of taking a weighted sum of their inputs and applying a non-linear activation function to this to obtain its output (Jurafsky and Martin (2023), Hastie, Tibshirani, and Friedman (2001)). Forward and backward passes of the data are used to change the parameters in order to optimize an objective function.\nA general way in which to represent this operation is the following,\n\\[a^l_j(i) = \\sigma_l\\left( \\sum_{k = 1}^{d_l - 1} a_k^{l-1}(i)w_{kj}^l + b_j^l \\right), \\quad l = 1,2,...,L; j = 1,2,...d_l \\]\nwhere \\(l\\) represents the number of layers, \\(d_l\\) the number of nodes in layer \\(l\\), \\(\\sigma_l(.)\\) denotes the activation function, \\(w_{kj}^l\\) denotes the weight linking the \\(k^{th}\\) node in layer \\(l-1\\) to the \\(j^{th}\\) node in layer \\(l\\), \\(b_j^l\\) denotes the \\(j^{th}\\) bias in layer \\(l\\) (Pienaar (2023a)).\nThe activation for the hidden layers has been chosen as the reLu activation function and the output layer uses a soft-max activation function. The classification task uses accuracy rate to evaluate model performance. Like the other models this is just a measure of correctly classified predictions.\nTo find the best data in terms of FFNN, the bag of words at different word bags was assessed against a default model with 32 nodes, an input layer with relu activation function, a dropout rate of 0.5, an output layer with a softmax activation function and performance calculated via accuracy. Later on, hyperparameter tuning to find a better model occurs.\nA single layer FFNN has been chosen. For the hyperparameter tuning of both models, the hidden layer explores 10, 32, 64 and 128 nodes. The dropout rate has been tried as 0.01 and 0.5. The batch size explored is 50 and 100. The learning rate on the adam optimizer has been set to 0.01.\nKeras was used for the FFNN, this library uses cross-validation to find model accuracy. Before for the FFNN the train and validation sets were separated so that the model could be compared to the other classifiers. Since k-fold cross validation is performed from within keras, the train and validation sets can now be combined prior to hyperparameter tuning. The results from this tuning will aid in finding the best model."
  },
  {
    "objectID": "report.html#validation-results",
    "href": "report.html#validation-results",
    "title": "Predict the President",
    "section": "Validation Results",
    "text": "Validation Results\nFor the FFNN, we see that loss decreases as accuracy increases, which indicates that the model is learning. The in-sample (IS) model accuracy looks to improve as the data set increases for all different types of feature sets. With the model built off TFIDF holding the best IS accuracy.\n\n\n\n\n\nFigure 1: In-sample Loss and accuracy for lexical feature space\n\n\n\n\nA similar observation can be made in the below figure.\n\n\n\n\n\nFigure 2: In-sample Loss and accuracy for character feature space\n\n\n\n\nThis figure as mentioned above seems to have the best IS accuracy.\n\n\n\n\n\nFigure 3: In-sample Loss and accuracy for TFIDF feature space\n\n\n\n\nTable 1 depicts the results of our analysis of different features over differently sized word bags. It is an analysis, that is capable of depicting the number of words necessary for the models and which models we can look to take forward and which data. We see that for both the NB and the SVM the missclassification is lowest when using character features at a bag size of 4500 features. The FFNN had the best (lowest) missclassification when tfidf is used. In order to find the model that performs best, both these two well performing feature sets will undergo hyperparameter tuning for NB, SVM and FFNN.\n\n\n\nTable 1: Validation missclassification rate results depicting performance of differently specified data in each model for different word bags and TFIDF\n\n\n\nNB\nSVM\nFFNN\n\n\nCount\nLex.n\nChar.n\nTFIDF.n\nLex.s\nChar.s\nTFIDF.s\nLex.f\nChar.f\nTFIDF.f\n\n\n200\n0.583\n0.594\n    \n0.473\n0.523\n    \n0.531\n0.566\n    \n\n\n500\n0.578\n0.563\n    \n0.455\n0.486\n    \n0.463\n0.514\n    \n\n\n2000\n0.714\n0.518\n    \n0.417\n0.433\n    \n0.407\n0.466\n    \n\n\n4500\n0.784\n0.481\n    \n0.613\n0.419\n    \n0.404\n0.435\n    \n\n\nAll\n0.772\n0.823\n0.708\n0.62 \n0.64 \n0.698\n0.394\n0.388\n0.376"
  },
  {
    "objectID": "report.html#hyperparameter-tuning-results",
    "href": "report.html#hyperparameter-tuning-results",
    "title": "Predict the President",
    "section": "Hyperparameter Tuning Results",
    "text": "Hyperparameter Tuning Results\nThe Laplace smoothing produced no significant results. Laplace smoothing at parameters 0, 1 and 2 were tried. Each result yielded the same validation missclassification rate. This is because Laplace smoothing is used to smooth over unseen features; here the word bag has been created and all sets were included during, therefore all sets essentially have the same features present (Jurafsky and Martin (2023)).\nThe figure below depicts the validation missclassification across different levels of cost complexity in the SVM. Below we see that for the character features that the best model came about when the cost was 100. The final model for the SVM will have a cost of 100. The cost complexity of the TFIDF features did not change. This may indicate that this data set does not work with a radial kernal. However, for simplicity the two sets of features will have a dataset\n\n\n\n\n\nValidation missclassification for both the character features and TFIDF features\n\n\n\n\nEarlier when looking for a good feature set, it was clear that 50 epochs was too many, so this has been set to 20 epochs for the hyperparameter tuning.\nIn Table 2 we see that the best hyperparameters for the model built off the character feature set are in fact the same as the default model described at the beginning of the paper. This table depicts the accuracy of the model, which is, \\[\\text{accuracy} = 1 - \\text{missclassification}.\\] So a high accuracy is good.\n\n\n\nTable 2: Cross validated results depicting the best hyperparameters for the character feature set in a single layer FFNN\n\n\nLoss\nAccuracy\nNodes\nBatchSize\nLearning Rate\n\n\n0.0043\n0.999\n32\n100\n0.01\n\n\n0.0045\n0.999\n10\n50\n0.01\n\n\n0.0044\n0.999\n128\n100\n0.01\n\n\n0.0046\n0.999\n64\n100\n0.01\n\n\n0.0048\n0.999\n32\n50\n0.01\n\n\n\n\n\n\n\nIn Table 3 we see that the best hyperparameters for the model built off the TFIDF feature set are in fact the same as the default model described at the beginning of the paper, but with a dropout rate of 0.01 instead of 0.5.\n\n\n\nTable 3: Cross validated results depicting the best hyperparameters for the TFIDF feature set in a single layer FFNN\n\n\nLoss\nAccuracy\nNodes\nBatchSize\nLearning Rate\n\n\n0.0085\n0.998\n32\n100\n0.01\n\n\n0.0052\n0.997\n64\n100\n0.01\n\n\n0.0057\n0.997\n64\n50\n0.01\n\n\n0.0065\n0.997\n32\n50\n0.01\n\n\n0.0073\n0.997\n128\n100\n0.01\n\n\n\n\n\n\n\nFrom here on the NB, SVM, and the FFNN can be described under the same constraint for both feature sets."
  },
  {
    "objectID": "report.html#final-model",
    "href": "report.html#final-model",
    "title": "Predict the President",
    "section": "Final Model",
    "text": "Final Model\nFor each model, we began by combining the training and validation data, then we can go ahead and train this new and final model. The\nTable 4 holds the out-of-sample missclassification rates of the final models. The results demonstrate that of the three classifiers observed, the FFNN performed the best over both feature sets. The FFNN obtained a missclassification rate of 0.401 (accuracy of 0.599) when 4500 of the most important character features were considered. It obtained a missclassification of 0.328 (accuracy of 0.672) when using a full feature set of TFIDF.\nAlthough, the TFIDF feature set trained in a neural network had the best result, it is still necessary to consider the character feature set. The character feaature set did not perform as well but was a lot more consistent over all classifiers. It’s shown to perform massively better under the NB and SVM classifiers over the TFIDF feature set.\n\n\n\nTable 4: Test set missclassification rate over each model and feature set\n\n\n\nNB\nSVM\nFFNN\n\n\nCharacter 4500\n0.479\n0.399\n0.401\n\n\nTFIDF\n0.712\n0.717\n0.328"
  },
  {
    "objectID": "supplementary.html",
    "href": "supplementary.html",
    "title": "Supplementary",
    "section": "",
    "text": "This supplementary serves as grounds on which most of the final decisions were based.\n\n\nThe first task is to select the stylometric features. This is deciding how one may choose to represent text. This can either be done by tokenizing by words (lexical features), a number of characters (character features), or in other more sophisticated ways like syntatic features or semantic features. In this study, only lexical features and character features are explored, while the other two require more domain knowledge to properly quantify (Stamatatos (2009)). Another point to address is the usage of token count or TF-IDF weightings, as observations for the study. This initial exploration will be done with frequency, as this is a very stable way in conducting author attribution (Škorić et al. (2022)). For each experiment, a Naive Bayes classifier (NB) and a Support Vector Machine (SVM) will be used to inform decisions.\n\n\n\n\n\nWhen it comes to lexical features, these can be classed into N-grams. A collection of N contiguous words. The greater N is, the more complex the representation of the feature. It often doesn’t show better results and the greater the N the higher the dimensionality of the problem, which will ultimately increase sparsity in the dataset, making it more difficult to classify (Coyotl-Morales et al. (2006), Škorić et al. (2022)). To that extent, only unigrams and bigrams will be explored. Often it would seem that unigrams are sufficient amongst the three. It is however, interesting to observe the behaviour of n-grams Tiffani (2020).\nFor the training, validation and test splits, it would be wise to select sample sizes by sentence and not by word, this will represent each row in the bag of words.\n\n\n\nTable A: Validation set missclassification rate over NB and SVM modelS and 3 different feature sets\n\n\n\nNo Punctuation\nPunctuation\n\n\n\nNo Function\nFunction\nFunction\n\n\nUnigram NB\n0.652\n0.567\n0.542\n\n\nUnigram SVM\n0.535\n0.483\n0.488\n\n\nBigram NB\n0.867\n0.586\n-\n\n\nBigram SVM\n0.534\n0.535\n-\n\n\n\n\n\n\n\nWhat we see above is that for a word bag of 200 most frequent content words (No Function) that the bigram does indeed perform worse. It is essential that we observe whether allowing functional words will improve the statement made earlier, that this is more directed towards style-based classification. First let’s look into whether using functional words will be useful to the analysis. Here we see that this is indeed more of style-based classification than a topic-based classification, this is shown in how the missclassification in general does decrease in both cases of of the bigram and unigram as we allow functional words into the analysis as seen in Table A. At this stage, I would suggest using unigrams and not using stop words for the analysis.\nHowever, since this is now a style-based classification task, using character features has in previous works been shown to capture style, lexical information and some contextual information. It is also able use punctuation and capitalization to infer stylistic approaches (Stamatatos (2009)).\n\n\n\n\n\n\n\n\n\n\n\n\n\nTable B: Validation set missclassification rate on NB and SVM models with word bag sizes of 200 and 2500 for different character n-grams\n\n\n\nWord Bag Size\n\n\n\n200\n2500\n\n\n3 Char N-Gram NB\n0.577\n0.509\n\n\n3 Char N-Gram SVM\n0.503\n0.429\n\n\n4 Char N-Gram NB\n0.577\n0.503\n\n\n4 Char N-Gram SVM\n0.52 \n0.434\n\n\n\n\n\n\n\nAt this stage we can see that the classification is not that much different from a unigram, however, points of which to consider, is that when using character features certain items become necessary and deserve exploration: the number of characters, the size of the word bag, the inclusion of punctuation. The number of characters have been chosen to be either 3 or 4, which is consistent with the study (Stamatatos (2009)). The size of the bag and the inclusion of punctuation are two points which need to be further explored. The missclassification has decreased from when the word bag contained 200 words.\n\n\n\nTable C: Validation set missclassification rate on NB and SVM models with word bag size of 4500 for a 4 character n-gram with punctuation\n\n\n\nPunctuation\n\n\n\nNo Capitalisation\nCapitalisation\n\n\n4 Char N-Gram NB\n0.48 \n0.478\n\n\n4 Char N-Gram SVM\n0.413\n0.409\n\n\n\n\n\n\n\nA word bag of 2000, 2500, 3000, 4000, 4500 and 5000 were looked at, these tests will not be shown, but can be replicated. The best performing model came out to be with a bag size of 4500 of the configurations tried. We clearly see here that there is an improvement when punctuation is included. We see here that if the capitalization is maintained we get the best performing SVM, and a decently performing NB, with a missclassification of 0.4 for the SVM.\n\n\n\n\n\n\n\nTable D: Validation set missclassification rate on NB and SVM models with word bag size of 4500 for a 4 character n-gram with punctuation\n\n\n\nNo Punctuation\nPunctuation\n\n\n\nNo Capitalisation\nCapitalisation\n\n\nTFIDF NB\n0.698\n0.696\n\n\nTFIDF SVM\n0.698\n0.694\n\n\n\n\n\n\n\nTF-IDF is able to take on both complex word tokens as well as character tokens. The best models above will be compared here. TF-IDF, cannot be divided up into the top n words by TF-IDF in this case, as these types of words only appear once or twice within each document. Here TF-IDF is in character features, and it’s performance is worse than above."
  },
  {
    "objectID": "supplementary.html#stylometric-features",
    "href": "supplementary.html#stylometric-features",
    "title": "Supplementary",
    "section": "",
    "text": "The first task is to select the stylometric features. This is deciding how one may choose to represent text. This can either be done by tokenizing by words (lexical features), a number of characters (character features), or in other more sophisticated ways like syntatic features or semantic features. In this study, only lexical features and character features are explored, while the other two require more domain knowledge to properly quantify (Stamatatos (2009)). Another point to address is the usage of token count or TF-IDF weightings, as observations for the study. This initial exploration will be done with frequency, as this is a very stable way in conducting author attribution (Škorić et al. (2022)). For each experiment, a Naive Bayes classifier (NB) and a Support Vector Machine (SVM) will be used to inform decisions.\n\n\n\n\n\nWhen it comes to lexical features, these can be classed into N-grams. A collection of N contiguous words. The greater N is, the more complex the representation of the feature. It often doesn’t show better results and the greater the N the higher the dimensionality of the problem, which will ultimately increase sparsity in the dataset, making it more difficult to classify (Coyotl-Morales et al. (2006), Škorić et al. (2022)). To that extent, only unigrams and bigrams will be explored. Often it would seem that unigrams are sufficient amongst the three. It is however, interesting to observe the behaviour of n-grams Tiffani (2020).\nFor the training, validation and test splits, it would be wise to select sample sizes by sentence and not by word, this will represent each row in the bag of words.\n\n\n\nTable A: Validation set missclassification rate over NB and SVM modelS and 3 different feature sets\n\n\n\nNo Punctuation\nPunctuation\n\n\n\nNo Function\nFunction\nFunction\n\n\nUnigram NB\n0.652\n0.567\n0.542\n\n\nUnigram SVM\n0.535\n0.483\n0.488\n\n\nBigram NB\n0.867\n0.586\n-\n\n\nBigram SVM\n0.534\n0.535\n-\n\n\n\n\n\n\n\nWhat we see above is that for a word bag of 200 most frequent content words (No Function) that the bigram does indeed perform worse. It is essential that we observe whether allowing functional words will improve the statement made earlier, that this is more directed towards style-based classification. First let’s look into whether using functional words will be useful to the analysis. Here we see that this is indeed more of style-based classification than a topic-based classification, this is shown in how the missclassification in general does decrease in both cases of of the bigram and unigram as we allow functional words into the analysis as seen in Table A. At this stage, I would suggest using unigrams and not using stop words for the analysis.\nHowever, since this is now a style-based classification task, using character features has in previous works been shown to capture style, lexical information and some contextual information. It is also able use punctuation and capitalization to infer stylistic approaches (Stamatatos (2009)).\n\n\n\n\n\n\n\n\n\n\n\n\n\nTable B: Validation set missclassification rate on NB and SVM models with word bag sizes of 200 and 2500 for different character n-grams\n\n\n\nWord Bag Size\n\n\n\n200\n2500\n\n\n3 Char N-Gram NB\n0.577\n0.509\n\n\n3 Char N-Gram SVM\n0.503\n0.429\n\n\n4 Char N-Gram NB\n0.577\n0.503\n\n\n4 Char N-Gram SVM\n0.52 \n0.434\n\n\n\n\n\n\n\nAt this stage we can see that the classification is not that much different from a unigram, however, points of which to consider, is that when using character features certain items become necessary and deserve exploration: the number of characters, the size of the word bag, the inclusion of punctuation. The number of characters have been chosen to be either 3 or 4, which is consistent with the study (Stamatatos (2009)). The size of the bag and the inclusion of punctuation are two points which need to be further explored. The missclassification has decreased from when the word bag contained 200 words.\n\n\n\nTable C: Validation set missclassification rate on NB and SVM models with word bag size of 4500 for a 4 character n-gram with punctuation\n\n\n\nPunctuation\n\n\n\nNo Capitalisation\nCapitalisation\n\n\n4 Char N-Gram NB\n0.48 \n0.478\n\n\n4 Char N-Gram SVM\n0.413\n0.409\n\n\n\n\n\n\n\nA word bag of 2000, 2500, 3000, 4000, 4500 and 5000 were looked at, these tests will not be shown, but can be replicated. The best performing model came out to be with a bag size of 4500 of the configurations tried. We clearly see here that there is an improvement when punctuation is included. We see here that if the capitalization is maintained we get the best performing SVM, and a decently performing NB, with a missclassification of 0.4 for the SVM.\n\n\n\n\n\n\n\nTable D: Validation set missclassification rate on NB and SVM models with word bag size of 4500 for a 4 character n-gram with punctuation\n\n\n\nNo Punctuation\nPunctuation\n\n\n\nNo Capitalisation\nCapitalisation\n\n\nTFIDF NB\n0.698\n0.696\n\n\nTFIDF SVM\n0.698\n0.694\n\n\n\n\n\n\n\nTF-IDF is able to take on both complex word tokens as well as character tokens. The best models above will be compared here. TF-IDF, cannot be divided up into the top n words by TF-IDF in this case, as these types of words only appear once or twice within each document. Here TF-IDF is in character features, and it’s performance is worse than above."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Welcome!",
    "section": "",
    "text": "This is a website for the submission of Assignment 1 for STA5073Z.\nGroup: Edward Baleni\nPLAGIARISM DECLARATION\n\nI know that plagiarism is wrong. Plagiarism is to use another’s work and pretend that it is one’s own.\nI have used a generally accepted citation and referencing style. Each contribution to, and quotation in, this tutorial/report/project from the work(s) of other people has been attributed, and has been cited and referenced.\nThis tutorial/report/project is my own work.\nI have not allowed, and will not allow, anyone to copy my work with the intention of passing it off as his or her own work.\nI acknowledge that copying someone else’s assignment or essay, or part of it, is wrong, and declare that this is my own work.\n\nNote that agreement to this statement does not exonerate you from the University’s plagiarism rules (http://www.uct.ac.za/uct/policies/plagiarism_students.pdf).\nSignature: E. Baleni\nDate: 19/10/2023"
  }
]