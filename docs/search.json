[
  {
    "objectID": "report.html#lemmatization",
    "href": "report.html#lemmatization",
    "title": "Predict the President",
    "section": "Lemmatization",
    "text": "Lemmatization\nIt may be necessary to perform lemmatization as commonly appearing words may appear in different forms for each president. So changing something like running to run may be necessary in finding commonly occuring words for example.\nUnfortunately, for this data split, even for unigrams, we see very small frequencies even for words that are specific to one journal. This may mean that we will have to look at a different way to obtain an inverse weighted distribution of samples.\n\n\n\n  deKlerk   Mandela     Mbeki Motlanthe Ramaphosa      Zuma \n       97      1665      2419       266      2286      2656 \n\n\n\n  deKlerk   Mandela     Mbeki Motlanthe Ramaphosa      Zuma \n       70        70        70        70        70        70 \n\n\nThe word counts are helpful indicator of commonly occuring words, however, in this dataset, many of the presidents start their speeches similarly. They also have repetitive speech and points of order. It is therefore worthwhile to use the tf-idf weigthed counts instead of the frequencies of each word to find frequently occuring words. The TF-IDF finds the frequently occuring words for a particular document.\n\n\n# A tibble: 6 × 2\n  president total\n  &lt;chr&gt;     &lt;int&gt;\n1 Mandela    1785\n2 Mbeki      1927\n3 Motlanthe  2021\n4 Ramaphosa  1451\n5 Zuma       1508\n6 deKlerk    1527\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFor the bigrams below, we see very useless words come into play that could be used to characterise presidents, it may be useful to consider removing stop words to improve the goodness of this study. However, these minute differences may be just a manner of speaking for some of these presidents and as such are necessary to the text. Like it could be a personality trait specific to that president.\nWe see below the usage of words that are not really useful to our analysis like, “last year”, this appears for 3 different presidents. Maybe remove this in a user-defined stop word dictionary.\n\n\n# A tibble: 6 × 2\n  president total\n  &lt;chr&gt;     &lt;int&gt;\n1 Mandela    1715\n2 Mbeki      1857\n3 Motlanthe  1951\n4 Ramaphosa  1381\n5 Zuma       1438\n6 deKlerk    1457\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nWhat we can see below is that a pentagram is not exactly useful in distiguishing different presidents. We see that there are some commonly occuring phrases. But these happen a total of 3 to 5 times out of over 30 000 entries for some presidents. Therefore it is not necessary to go this far.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nA trigram in this instance is as far as I would recommend looking. Although, it is still not very stable."
  },
  {
    "objectID": "report.html#eda",
    "href": "report.html#eda",
    "title": "Predict the President",
    "section": "EDA",
    "text": "EDA"
  },
  {
    "objectID": "report.html#naive-bayes-classifier",
    "href": "report.html#naive-bayes-classifier",
    "title": "Predict the President",
    "section": "Naive Bayes Classifier",
    "text": "Naive Bayes Classifier\nA multinomial naive Bayes classifier was used to classify text. This is a probabilistic classifier, given the document \\(d\\) and classes \\(c \\in C\\), the classifier will predict the class, \\(\\hat{c}\\), with the maximum posterior probability,\n\\[\\hat{c} = \\underset{c\\in C}{\\arg\\max} P(c|d)\\]\nUsing Bayes rule, the above prediction can be further broken down into,\n\\[\\hat{c} = \\underset{c\\in C}{\\arg\\max} P(c|d) = \\underset{c\\in C}{\\arg\\max} \\frac{P(d|c)P(c)}{P(d)}\\]\nThis can be further simplified by to,\n\\[\\hat{c} = \\underset{c\\in C}{\\arg\\max} P(d|c)P(c) \\]\nas \\(P(d)\\) stays constant for all classes, which allows us to cancel this out. This equation above is made up of the likelihood of the document and prior probability of the class. The documents can subsequently be divided up into features, be it our lexical or character features above. Following this great number of features, \\(\\{f_i : i = 1,2,...,n\\}\\), this classifier makes two simplifying assumptions that would infer it’s naivety, that the tokens are position independent and that the joint likelihood of the features also maintain independence,\n\\[P(d|c) = P(f_1, f_2, ..., f_n|c) = P(f_1|c)P(f_2|c)...P(f_n|c)\\] The final equation of the naive Bayes classifier then becomes,\n\\[\\hat{c}_{NB} = \\underset{c\\in C}{\\arg\\max} P(c) \\prod_{f\\in F} P(f|c)\\]\n(@Jurafsky2000SpeechAL).\nThis classifier was made with no Laplace smoothing. Laplace smoothing, will be looked into for the final models. This is a simple smoothing algorithm that can be easily implemented in this classifier. It simply adds to the count the number specified before finding probabilities (@Jurafsky2000SpeechAL)."
  },
  {
    "objectID": "report.html#feed-forward-neural-networks",
    "href": "report.html#feed-forward-neural-networks",
    "title": "Predict the President",
    "section": "Feed Forward Neural Networks",
    "text": "Feed Forward Neural Networks\nA neural network is a series of non-linear basis functions, this is handled via interconnected nodes over a number of layers. Each layer has a number of nodes and is connected to every other node in the layer before (if any) and after it (if any). The case where there are no layers before the current layer is for the input layer, which is a layer that takes in data. The number of nodes present at the input layer are equal to the number of features in the bag of words. Similarly, the case where there are no layers after the current layer is for the output layer, in this text classification task. Since the task is that of multinomial classification, the number of nodes on this output layer is equivalent to the number of classes available. The layers between the input layer and the output layer are called hidden layers. The output of each layer is passed on as the input to the next layer. The hidden layers are capable of taking a weighted sum of their inputs and applying a non-linear activation function to this to obtain its output (@Jurafsky2000SpeechAL, @Hastie). Forward and backward passes of the data are used to change the parameters in order to optimize an objective function.\nA general way in which to represent this operation is the following,\n\\[a^l_j(i) = \\sigma_l\\left( \\sum_{k = 1}^{d_l - 1} a_k^{l-1}(i)w_{kj}^l + b_j^l \\right), \\quad l = 1,2,...,L; j = 1,2,...d_l \\]\nwhere \\(l\\) represents the number of layers, \\(d_l\\) the number of nodes in layer \\(l\\), \\(\\sigma_l(.)\\) denotes the activation function, \\(w_{kj}^l\\) denotes the weight linking the \\(k^{th}\\) node in layer \\(l-1\\) to the \\(j^{th}\\) node in layer \\(l\\), \\(b_j^l\\) denotes the \\(j^{th}\\) bias in layer \\(l\\) (@Et).\nThe activation for the hidden layers has been chosen as the reLu activation function and the output layer uses a soft-max activation function. The classification task uses accuracy rate to evaluate model performance. Like the other models this is just a measure of correctly classified predictions.\nTo find the best data in terms of FFNN, the bag of words at different word bags was assessed against a default model with 32 nodes, an input layer with relu activation function, a dropout rate of 0.5, an output layer with a softmax activation function and performacne calculated via accuracy. Later on, hyperparameter tuning to find a better model will happen and used to test the final data."
  },
  {
    "objectID": "report.html#support-vector-machines",
    "href": "report.html#support-vector-machines",
    "title": "Predict the President",
    "section": "Support Vector Machines",
    "text": "Support Vector Machines\nSupport vector machines, SVM, are widely used in the space of text classification (@Stylometry). SVMs perform linear operations in upto an infinite number of dimensions. If data is not linearly separable, SVMs can be used to linearly separable in a higher dimension. SVMs allow to find an optimal separating hyperplane for separable and non-separable data.\nThe goal is to evaluate the hyperplane such that it creates the greatest margin between the training points of each class and itself. The margin is the distance between the nearest data point to the plane.\nIf the feature space is non-linenar we would look to use a set of basis expansions as our input space,\n\\[\nh(x_i) = (h_1(x_i), h_2(x_i), ..., h_M(x_i)), \\quad i = 1, 2, ..., N\n\\]\nthe inner product of these basis expansions, \\(h(x)\\), will represent the desired kernal, \\(K(x, x')\\), used to move feature space to a higher dimension. The hyperplane will therefore be defined as,\n\\[\n{f}(x) = h(x)^T\\beta + \\beta_0\n\\]\nwhere \\(h(x)\\) is the kernal. It is difficult to have a hyperplane that perfectly classifies, to tune the model some slack is allowed. This is a soft-margin SVM. Slack are observations that are allowed to be missclassified. SVMs maximise the margin with some regularization,\n\\[\n\\frac{1}{2}\\beta^T\\beta + C\\sum_{n=1}^N \\zeta_n\n\\]\nsubject to,\n\\[\ny_i(h(x)^T\\beta + \\beta_0) \\geq 1 - \\zeta_n, \\quad n = 1,2,...,N \\\\\n\\zeta_n \\geq 0, \\quad\\quad n = 1,2,...,N\n\\]\nthis is a convex optimisation problem (Hastie, Tibshirani, and Friedman (2001), Pienaar (2023) ) . For multiclass problems the package provided by Meyer et al. (2023) uses a one-versus-one approach. This will fit all pairwise classifiers and use a voting system to pick the best ones.\nA base model has been used to explore our data, hyperparameter tuning is used to regularize the model, by using cost constraint."
  },
  {
    "objectID": "report.html#pre-processing",
    "href": "report.html#pre-processing",
    "title": "Predict the President",
    "section": "Pre-Processing",
    "text": "Pre-Processing\nAfter obtaining the data, the first line of each speech is removed. This first line is the date the address is held. The data is then tokenized by sentence. Only sentences with over two words remained for the rest of the analysis. The reason this is done is because the sentences with less than 3 words seemed to either either be made up of digits or unfinished words, most of the time.\nFollowing this many different considerations have been made in the Supplementary to find the best representation of the data for classification. Both lexical and character features are considered; the inclusion and exclusion of function words (stop words include function words); using token frequencies and TF-IDF are considered; different sized word bags are considered; the inclusion of case and punctuation are are also considered. Lexical features regard tokens as a sequence of words, where a unigram is a single word, a bigram are two contiguous words, a trigram is three and so forth. Character features are a sequence of contiguous characters also using n-grams to characterise how many characters are present in each feature. These two types of features qualify as two of the most basic markers used for identifying an author’s style, where lexical features are slightly more complex than character features (@Stylometry). Of the many options explored in the Supplementary, only two of the models will be represented here. The first approach will use unigrams of words to represent lexical features, while the second uses character features with a 4 character n-gram, this is how we will distinguish between the two going forward. These are important distinctions at this stage as they determine how the data is organised.\nIn the Supplementary, both a topic-based approach and style-based approach were looked into. It was found that the style-based approach performed better than topic-based approach, in most scenarios. This was illustrated in the improvement of model performance of the models with the inclusion of function words. This idea will be furthered explored below.\n\nLexical Features\nThere is complexity that comes with using lexical features; the sparsity within the bag-of-words of such features was a the reason why capitalization was not deemed appropriate in conjunction with lexical features (@LexVsChar). Capitalization within the transcription of a speech can only indicate the beginning of an idea, sentiment cannot be captured by such. Whereas, in the context of a book, capitalization can be use to convey a strong meaning in a character’s words.\nFirst the speeches are tokenized into sentences and are given sentence ids. Thereafter sentences were cleaned quite strictly. All digits are removed using a regex operation. Contractions are replaced with their long form via regex pattern matching (@textclean). Punctuation is maintained as punctuation is able to characterise syntatic information (@Punc). Punctuation is maintained, however, special unicode characters, astrixes, and other punctuation that is not commonly seen in text are removed, only full stops, question marks, commas, exclamation marks, brackets, apostrophes and curly brackets are maintained. Lemmatization is performed as commonly appearing words appear in different forms for each president. A lemma is a set of words that have the same stem, where each variation of the word is called a word form. Lemmatization is a process that is able to determine that two words have the same root. To lemmatize is to change wordforms into their root. This process of lemmatization is done by morphological parsing, where this separates the morphenes into parts. A morphene is a smaller building block of a word. Lemmatization was done by using a dictionary of common wordforms, if a word is a part of a lemma, then it will be matched to the dictionary and replaced by the morphenes that make it up. After performing this lemmatization, the affixes, additional morphenes that aren’t the stem, are still present in the data. Affixes are often random letters that will appear (@Jurafsky2000SpeechAL). The final step in cleaning is to remove these affixes\n\n\nCharacter Features\nCharacter features are more simplistic than lexical features and are able to capture stylistic nuances. They are able to capture some lexical and contextual information. They do however carry a lot of redundancies resulting in high dimensionality.\nThe data here is cleaned differently. In the Supplementary, it is shown that the feature space that includes both punctuation and capitalization performs the best of all the character spaces. A character n-gram of both 3 and 4 features were assessed, and both are decently sized n-grams. Here the character n-gram of 4 contiguous characters will be used.\nFirst the speeches are tokenized into sentences, and given an id. Where previously the text would have been set to lower case and the punctuation was included, here both the text maintains it’s capitalization and the punctuation remain. To clean these sentences, all digits were removed, followed by the lemmatization of wordforms to stems.\n\n\nImbalanced Classes\nThe data has imbalanced classes present. This means that the distribution of the data across classes is skewed. Motlanthe and de Klerk both only gave one speech. This was Motlanthe’s and de Klerk’s outgoing speeches; every other presidents managed to give more than one speech.\nThere are a number of ways to deal with imbalanced classes, for future work one might look into Stamatatos (2008) to get an idea of what strategies do actually work on text classification tasks. In this report, these outlying classes were removed from the remainder of the analysis."
  },
  {
    "objectID": "report.html#bag-of-words",
    "href": "report.html#bag-of-words",
    "title": "Predict the President",
    "section": "Bag of Words",
    "text": "Bag of Words\nIt is important that we get the data into a format that can be used by the classifiers. A bag of words is one such way to do this. The text is placed into something called a bag of words. This is an unordered set of words that contain the frequency of each word in the document (@Jurafsky2000SpeechAL). For both lexical and character features, the pre-processed sentences need to be tokenized down to their desired level. The lexical features are tokenized as unigrams of words, while including punctuation as individual words; the character features are tokenized as n-grams with 4 characters, with the inclusion of punctuation and capitalization. Both types of features included function words, as this was decided to be a stylometric exercise as per the Supplementary. In both cases, the sentence id was maintained over each word to allocate which sentence it belongs to.\nIn both cases, lexical and character features, the most important features are the most frequent. With this understanding, a crude feature selection, can be conducted for both.\nAfter tokenizing, most frequent features were selected. This was tried as 200, 500, 2000, 4500 and all the tokens for both the lexical features and the character features. After making these selections, a word bag was created for each selection of top features."
  },
  {
    "objectID": "report.html#term-frequency---inverse-document-frequency",
    "href": "report.html#term-frequency---inverse-document-frequency",
    "title": "Predict the President",
    "section": "Term Frequency - Inverse Document Frequency",
    "text": "Term Frequency - Inverse Document Frequency\nUnfortunately, for this data split, even for unigrams, we see very small frequencies even for words that are specific to one journal. This may mean that we will have to look at a different way to obtain an inverse weighted distribution of samples.\n\n\n\n  deKlerk   Mandela     Mbeki Motlanthe Ramaphosa      Zuma \n       97      1665      2419       266      2286      2656 \n\n\n\n  deKlerk   Mandela     Mbeki Motlanthe Ramaphosa      Zuma \n       70        70        70        70        70        70 \n\n\nThe word counts are helpful indicator of commonly occuring words, however, in this dataset, many of the presidents start their speeches similarly. They also have repetitive speech and points of order. It is therefore worthwhile to use the tf-idf weigthed counts instead of the frequencies of each word to find frequently occuring words. The TF-IDF finds the frequently occuring words for a particular document.\n\n\n# A tibble: 6 × 2\n  president total\n  &lt;chr&gt;     &lt;int&gt;\n1 Mandela    1785\n2 Mbeki      1927\n3 Motlanthe  2021\n4 Ramaphosa  1451\n5 Zuma       1508\n6 deKlerk    1527\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFor the bigrams below, we see very useless words come into play that could be used to characterise presidents, it may be useful to consider removing stop words to improve the goodness of this study. However, these minute differences may be just a manner of speaking for some of these presidents and as such are necessary to the text. Like it could be a personality trait specific to that president.\nWe see below the usage of words that are not really useful to our analysis like, “last year”, this appears for 3 different presidents. Maybe remove this in a user-defined stop word dictionary.\n\n\n# A tibble: 6 × 2\n  president total\n  &lt;chr&gt;     &lt;int&gt;\n1 Mandela    1715\n2 Mbeki      1857\n3 Motlanthe  1951\n4 Ramaphosa  1381\n5 Zuma       1438\n6 deKlerk    1457\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nWhat we can see below is that a pentagram is not exactly useful in distiguishing different presidents. We see that there are some commonly occuring phrases. But these happen a total of 3 to 5 times out of over 30 000 entries for some presidents. Therefore it is not necessary to go this far.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nA trigram in this instance is as far as I would recommend looking. Although, it is still not very stable."
  },
  {
    "objectID": "report.html#term-frequency---inverse-document-frequency-tf-idf",
    "href": "report.html#term-frequency---inverse-document-frequency-tf-idf",
    "title": "Predict the President",
    "section": "Term Frequency - Inverse Document Frequency (TF-IDF)",
    "text": "Term Frequency - Inverse Document Frequency (TF-IDF)\nFor comparison, TF-IDF was also considered. Term frequency, TF, is a measure of how frequently a term occurs in a document. TF can be found as,\n\\[\\text{tf(term t in document i)} = \\frac{\\text{Number of times term t appears in document i}}{\\text{Number of terms in document i}}.\\]\nInverse document term frequency downweights terms that are frequent in a collection of documents and upweights terms that are not frequent within a collection of documents. It is calculated as,\n\\[\\text{idf(term t)} = \\text{ln}\\left(\\frac{\\text{Number of documents in corpus}}{\\text{Number of documents containing term t}}\\right).\\]\nTF-IDF is the multiplication of the two metrics (@TextMining, Durbach (2023) ). TF-IDF measures the importance of a word to a document in a collection. In this analysis this has been done by considering one document to be a sentence. Following this intuition, the quantities above can be calculated.\nSince TF-IDF determines what features are important to which document, this can be seen as a topic-based classification, as function words and commonly occurring tokens among the documents will be downweighted resulting in semantic features being strongly weighted. For TF-IDF, all tokens were kept, because some of the highly weighted scores may only appear once in a document and as a result may act as quite a poor classifier out-of-sample, and so it is necessary to leave all possible tokens."
  },
  {
    "objectID": "report.html#train-validation-and-testing-splits",
    "href": "report.html#train-validation-and-testing-splits",
    "title": "Predict the President",
    "section": "Train, Validation and Testing Splits",
    "text": "Train, Validation and Testing Splits\nFor the training, validation and test splits, it would be wise to select sample sizes by sentence and not by word, this will represent each row in the bag of words.\nThe data will have a 60:20:20 split of training, validation and test data. The training set will represent 60% of the total number of sentences available, the validation set will represent 20% and the test set will be the remaining 20%. These splits are done for each word bag and for tf-idf."
  },
  {
    "objectID": "report.html#validation-results",
    "href": "report.html#validation-results",
    "title": "Predict the President",
    "section": "Validation Results",
    "text": "Validation Results\nFor the FFNN, in general we see that loss increase and accuracy increases, which indicates that the model is learning. The in-sample, IS, model accuracy looks to improve as the data set increases for all different types of dataset. With the model built off TFIDF holding the best IS accuracy.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n18/18 - 0s - loss: 2.7783 - accuracy: 0.5988 - 112ms/epoch - 6ms/step\n\n\n\n\n\nBag\nNB\nSVM\nFFNN\n\n\nCount\nLex.n\nChar.n\nTFIDF.n\nLex.s\nChar.s\nTFIDF.s\nLex.f\nChar.f\nTFIDF.f\n\n\n200\n0.583\n0.594\n    \n0.473\n0.523\n    \n0.511\n0.564\n    \n\n\n500\n0.578\n0.563\n    \n0.455\n0.486\n    \n0.457\n0.519\n    \n\n\n2000\n0.714\n0.518\n    \n0.417\n0.433\n    \n0.409\n0.446\n    \n\n\n4500\n0.784\n0.481\n    \n0.613\n0.419\n    \n0.399\n0.431\n    \n\n\nAll\n0.772\n0.823\n0.708\n0.62 \n0.64 \n0.698\n0.401\n0.382\n0.356"
  },
  {
    "objectID": "report.html#hyperparameter-tuning-results",
    "href": "report.html#hyperparameter-tuning-results",
    "title": "Predict the President",
    "section": "Hyperparameter Tuning Results",
    "text": "Hyperparameter Tuning Results"
  },
  {
    "objectID": "report.html#final-model",
    "href": "report.html#final-model",
    "title": "Predict the President",
    "section": "Final Model",
    "text": "Final Model"
  }
]