[
  {
    "objectID": "report.html#lemmatization",
    "href": "report.html#lemmatization",
    "title": "Predict the President",
    "section": "Lemmatization",
    "text": "Lemmatization\nIt may be necessary to perform lemmatization as commonly appearing words may appear in different forms for each president. So changing something like running to run may be necessary in finding commonly occuring words for example.\nUnfortunately, for this data split, even for unigrams, we see very small frequencies even for words that are specific to one journal. This may mean that we will have to look at a different way to obtain an inverse weighted distribution of samples.\n\n\n\n  deKlerk   Mandela     Mbeki Motlanthe Ramaphosa      Zuma \n       97      1665      2419       266      2286      2656 \n\n\n\n  deKlerk   Mandela     Mbeki Motlanthe Ramaphosa      Zuma \n       70        70        70        70        70        70 \n\n\nThe word counts are helpful indicator of commonly occuring words, however, in this dataset, many of the presidents start their speeches similarly. They also have repetitive speech and points of order. It is therefore worthwhile to use the tf-idf weigthed counts instead of the frequencies of each word to find frequently occuring words. The TF-IDF finds the frequently occuring words for a particular document.\n\n\n# A tibble: 6 × 2\n  president total\n  &lt;chr&gt;     &lt;int&gt;\n1 Mandela    1785\n2 Mbeki      1927\n3 Motlanthe  2021\n4 Ramaphosa  1451\n5 Zuma       1508\n6 deKlerk    1527\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFor the bigrams below, we see very useless words come into play that could be used to characterise presidents, it may be useful to consider removing stop words to improve the goodness of this study. However, these minute differences may be just a manner of speaking for some of these presidents and as such are necessary to the text. Like it could be a personality trait specific to that president.\nWe see below the usage of words that are not really useful to our analysis like, “last year”, this appears for 3 different presidents. Maybe remove this in a user-defined stop word dictionary.\n\n\n# A tibble: 6 × 2\n  president total\n  &lt;chr&gt;     &lt;int&gt;\n1 Mandela    1715\n2 Mbeki      1857\n3 Motlanthe  1951\n4 Ramaphosa  1381\n5 Zuma       1438\n6 deKlerk    1457\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nWhat we can see below is that a pentagram is not exactly useful in distiguishing different presidents. We see that there are some commonly occuring phrases. But these happen a total of 3 to 5 times out of over 30 000 entries for some presidents. Therefore it is not necessary to go this far.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nA trigram in this instance is as far as I would recommend looking. Although, it is still not very stable."
  },
  {
    "objectID": "report.html#eda",
    "href": "report.html#eda",
    "title": "Predict the President",
    "section": "EDA",
    "text": "EDA"
  },
  {
    "objectID": "report.html#naive-bayes-classifier",
    "href": "report.html#naive-bayes-classifier",
    "title": "Predict the President",
    "section": "Naive Bayes Classifier",
    "text": "Naive Bayes Classifier\nA multinomial naive Bayes classifier was to classify this text. This is a probabilistic classifier, given the document \\(d\\) and classes \\(c \\in C\\), the classifier will predict the class, \\(\\hat{c}\\), with the maximum posterior probabiliity,\n\\[\\hat{c} = \\underset{c\\in C}{\\arg\\max} P(c|d)\\]\nUsing Bayes rule, the above prediction can be further broken down into,\n\\[\\hat{c} = \\underset{c\\in C}{\\arg\\max} P(c|d) = \\underset{c\\in C}{\\arg\\max} \\frac{P(d|c)P(c)}{P(d)}\\]\nThis can be further simplified by to,\n\\[\\hat{c} = \\underset{c\\in C}{\\arg\\max} P(d|c)P(c) \\]\nas \\(P(d)\\) stays constant for all classes, which allows us to cancel this out. This equation above is made up of the likehood of the document and te prior probability of the class. The documents can subsequently be divided up into features, be it our lexical or character features above. Following this great number of features, \\(\\{f_i : i = 1,2,...,n\\}\\), this classifier makes two simplifying assumptions that would infer it’s naivety, that the tokens are position independent and that the joint likelihood of the features also maintain independence,\n\\[P(d|c) = P(f_1, f_2, ..., f_n|c) = P(f_1|c)P(f_2|c)...P(f_n|c)\\] The final equation of the naive Bayes classifier then becomes,\n\\[c_{NB} = \\underset{c\\in C}{\\arg\\max} P(c) \\prod_{f\\in F} P(f|c).\\]\n\n\n[1] 0.5829621\n\n\n[1] 0.4727171\n\n\n\n\n[1] 0.4808013\n\n\n[1] 0.4190317"
  },
  {
    "objectID": "report.html#feed-forward-neural-networks",
    "href": "report.html#feed-forward-neural-networks",
    "title": "Predict the President",
    "section": "Feed Forward Neural Networks",
    "text": "Feed Forward Neural Networks"
  },
  {
    "objectID": "report.html#support-vector-machines",
    "href": "report.html#support-vector-machines",
    "title": "Predict the President",
    "section": "Support Vector Machines",
    "text": "Support Vector Machines"
  },
  {
    "objectID": "report.html#pre-processing",
    "href": "report.html#pre-processing",
    "title": "Predict the President",
    "section": "Pre-Processing",
    "text": "Pre-Processing\nAfter obtaining the data, the first line of each speech is removed. This first line is the date the address is held. The data is then tokenized by sentence. Only sentences with over two words remained for the rest of the analysis. The reason this is done is because the sentences with less than 3 words seemed to either just be digits or to be unfinished words, most of the time.\nFollowing this many different considerations have been made in the Supplementary to find the best representation of the data for classification. Both lexical and character features are considered; the inclusion and exclusion of function words (stop words include function words); using token frequencies and TF-IDF are considered; different sized word bags are considered; the inclusion of case and punctuation are are also considered. Lexical features regard tokens as a sequence of words, where a unigram is a single word, a bigram are two contiguous words, a trigram is three and so forth. Character features are a sequence of contiguous characters also using n-grams to characterise how many characters are present in each feature. These two types of features qualify as two of the most basic markers used for identifying an author’s style, where lexical features are slighttly more complex than character features (Stamatatos (2009)). Of the many options explored in the Supplementary, only two of the models will be represented here. How data is cleaned and shaped is different between these two approaches. The first approach uses lexical features, while the second uses character features, this is how we will distinguish between the two going forward.\nThese are important distinctions at this stage as they determine how the data is organised.\nIn the Supplementary, both a topic-based approach and style-based approaches were looked into. It was found that the style-based approach performed better than topic-based approach. This was illustrated in how the model performance improved with the inclusion of function words…….\n\nLexical Features\nThere is complexity that comes with using lexical features, and the sparsity within the bag-of-words that such features introduce, allowing features with capitalization was not appropriate (Stamatatos (2008)), capitalization within the transcription of a speech, indicate the beginning of an idea, or the beginning of a sentence, sentiment cannot be captured by such. Whereas, in the context of a book, capitalization can be use to convey a strong meaning in a character’s words.\nHere the sentences were cleaned quite strictly in order to obtain a dataset. All digits are removed using a regex operation. Contractions are replaced with their long form also via the aid of regex pattern matching (Rinker (2018)). Here punctuation is maintained as punctuation is able to characterise syntatic information ((punc?)). Punctuation is maintained, however, special unicode characters, astrixes, and other punctuation that is not commonly seen in text are removed, only full stops, question marks, commas, exclamation marks, brackets, apostrophes and curly brackets are maintained. Lemmatization is done as commonly appearing words appear in different forms for each president. A lemma is a set of words that have the same stem, where each variation of the word is called a word form. Lemmatization is a process that is able to determine that two words have the same root, to lemmatize is to change all these wordforms into their root. This process of lemmatization is done by morphological parsing, where this separates the morphenes into parts. A morphene, is the smaller building blocks of a word. Lemmatization was done by using a dictionary of common word forms, if a word is a part of a lemma, then it will be matched to the dictionary and replaced by the morphenes that make it up. After performing this lemmatization, the affixes, additional morphenes that aren’t the stem, are still present in the data, these are generally quite random letters that will appear (Jurafsky and Martin (2023)). The final step in cleaning is to remove these affixes\n\n\nCharacter Features\nCharacter features are more simplistic than lexical features and are able to capture stylistic nuances. They are able to capture some lexical and contextual information. They do however carry a lot of redundancies resulting in high dimensionality.\nThe data here is cleaned differently. In the Supplementary, it is shown that the feature space that includes both punctuation and capitalization performs the best of all the character spaces. A character n-gram of both 3 and 4 features were assessed, and both are decently sized n-grams. Here the character n-gram of 4 contiguous characters will be used.\nFirst the speeches are tokenized into sentences, where previously, the text would have been set to lower case and the punctuation was included, here both the text maintains it’s capitalization and the punctuation shall remain. To clean these sentences, all digits were removed, followed by the lemmatization of wordforms to stems.\n\n\nImbalanced Classes\nBoth Motlanthe and deKlerk are outliers… To balance the classes"
  },
  {
    "objectID": "report.html#bag-of-words",
    "href": "report.html#bag-of-words",
    "title": "Predict the President",
    "section": "Bag of Words",
    "text": "Bag of Words\nIt is important that we get the data into a format that can be used by the classifiers to classify a sentence. A bag of words in one such way we can do this. What happens here is that the entirety of text is put into something called a bag of words. This is an unordered set of words which contain the frequency of each word in the document (Jurafsky and Martin (2023)). For both cases of lexoical features and character features, the pre-processed sentences need to be tokenized down to their desired level. The lexical features are tokenized as unigrams of words, while including punctuation of individual words; the character features are tokenized as n-grams with 4 characters, with the inclusion of punctuation and capitalization. Both types of features included function words, as this was decided to be a stylometric exercise as per the Supplementary.\nIn both the cases of the lexical and character features, the most important features are the most frequent. With this understanding, a crude feature selection, can be conducted for both the lexical and the character features.\nAfter tokenizing, most frequent features were selected. This was tried as 200, 1000, 2000, 4000, 4500 and all the tokens for both the lexical features and the character features. After making these selections, a word bag was created for each selection of top features."
  },
  {
    "objectID": "report.html#term-frequency---inverse-document-frequency",
    "href": "report.html#term-frequency---inverse-document-frequency",
    "title": "Predict the President",
    "section": "Term Frequency - Inverse Document Frequency",
    "text": "Term Frequency - Inverse Document Frequency\nUnfortunately, for this data split, even for unigrams, we see very small frequencies even for words that are specific to one journal. This may mean that we will have to look at a different way to obtain an inverse weighted distribution of samples.\n\n\n\n  deKlerk   Mandela     Mbeki Motlanthe Ramaphosa      Zuma \n       97      1665      2419       266      2286      2656 \n\n\n\n  deKlerk   Mandela     Mbeki Motlanthe Ramaphosa      Zuma \n       70        70        70        70        70        70 \n\n\nThe word counts are helpful indicator of commonly occuring words, however, in this dataset, many of the presidents start their speeches similarly. They also have repetitive speech and points of order. It is therefore worthwhile to use the tf-idf weigthed counts instead of the frequencies of each word to find frequently occuring words. The TF-IDF finds the frequently occuring words for a particular document.\n\n\n# A tibble: 6 × 2\n  president total\n  &lt;chr&gt;     &lt;int&gt;\n1 Mandela    1785\n2 Mbeki      1927\n3 Motlanthe  2021\n4 Ramaphosa  1451\n5 Zuma       1508\n6 deKlerk    1527\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFor the bigrams below, we see very useless words come into play that could be used to characterise presidents, it may be useful to consider removing stop words to improve the goodness of this study. However, these minute differences may be just a manner of speaking for some of these presidents and as such are necessary to the text. Like it could be a personality trait specific to that president.\nWe see below the usage of words that are not really useful to our analysis like, “last year”, this appears for 3 different presidents. Maybe remove this in a user-defined stop word dictionary.\n\n\n# A tibble: 6 × 2\n  president total\n  &lt;chr&gt;     &lt;int&gt;\n1 Mandela    1715\n2 Mbeki      1857\n3 Motlanthe  1951\n4 Ramaphosa  1381\n5 Zuma       1438\n6 deKlerk    1457\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nWhat we can see below is that a pentagram is not exactly useful in distiguishing different presidents. We see that there are some commonly occuring phrases. But these happen a total of 3 to 5 times out of over 30 000 entries for some presidents. Therefore it is not necessary to go this far.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nA trigram in this instance is as far as I would recommend looking. Although, it is still not very stable."
  },
  {
    "objectID": "report.html#term-frequency---inverse-document-frequency-tf-idf",
    "href": "report.html#term-frequency---inverse-document-frequency-tf-idf",
    "title": "Predict the President",
    "section": "Term Frequency - Inverse Document Frequency (TF-IDF)",
    "text": "Term Frequency - Inverse Document Frequency (TF-IDF)\nFor comparison, TF-IDF was also considered. Term frequency, TF, is a measure of how frequently a term occurs in a document. TF can be found as,\n\\[\\text{tf(term t in document i)} = \\frac{\\text{Number of times term t appears in document i}}{\\text{Number of terms in document i}}.\\]\nInverse document term frequency downweights terms that are frequent in a collection of documents and upweights terms that are not frequent within a collection of documents. It is calculated as,\n\\[\\text{idf(term t)} = \\text{ln}\\left(\\frac{\\text{Number of documents in corpus}}{\\text{Number of documents containing term t}}\\right).\\]\nTF-IDF is the multiplication of the two metrics (Silge and Robinson (2017)). TF-IDF measures the importance of a word to a document in a collection. In this analysis this has been done by considering one document to be a sentence. Following this intuition, the quantities above can be calculated.\nIn the Supplementary, both the lexical feature approach and the character features were looked into to find which would be better performing for TF-IDF. The character features seemed to show a better result and so will be used in this report.\nSince TF-IDF determines what features are important to which document, this can be seen as a topic-based classification, as function words and commonly occurring tokens amongst the documents will be downweighted resulting in semantic features being strongly weighted. For TF-IDF, all tokens were kept, because some of the highly weighted scores may only appear once in a document and as a result may act as quite a poor classifier out-of-sample, and so it is necessary to leave all possible tokens."
  },
  {
    "objectID": "report.html#train-validation-and-testing-splits",
    "href": "report.html#train-validation-and-testing-splits",
    "title": "Predict the President",
    "section": "Train, Validation and Testing Splits",
    "text": "Train, Validation and Testing Splits\nFor the training, validation and test splits, it would be wise to select sample sizes by sentence and not by word, this will represent each row in the bag of words.\nThe data will have a 60:20:20 split of training, validation and test data. The training set will represent 60% of the total number of sentences available, the validation set will represent 20% and the test set will be the remaining 20%.\n\n\n[1] 8983"
  }
]