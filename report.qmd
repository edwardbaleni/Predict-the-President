---
title: "Predict the President"
author: "Edward Baleni, BLNEDW003"
format: html
editor: visual
bibliography: references.bib
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE, fig.align="center", out.width = "65%", fig.pos = "H")
```

```{r Packages}
require(dplyr)
require(stringr)
require(tidytext)
require(ggplot2)
require(tidyr)
require(forcats)

# Lemmatization and Contraction decrementer
  # https://www.rdocumentation.org/packages/qdap/versions/2.4.6/topics/replace_contraction
  # https://github.com/trinker/textstem
require(textstem)
require(qdap)
```

# Abstract

# Introduction and Literature review

Look out for speaker disfluencies, such things may be able to aid in the identification of speech

# Data

```{r Data}
sys.source("sona-first-steps.R" , envir = knitr::knit_global())
sona <- as_tibble(sona)
sona$president <- as.factor(sona$president)
```

# Methodology

The idea here is that each president should be represented equally in their training and validation. It is not neccessary to concern oneself with the equality of representation in the test set. For this reason, it would make sense to extract a sample of the same size for each president, regardless of how many speeches they've done. 

Another thing to try is to simply remove the outliers Mothlale and deKlerk as they each only provided one speech. It is possible that looking into such characters may in fact skew the nature of this report in the predicting ability of the models.

Another method is to look into inversely proportional. This is almost the same idea as having equal representations.

Let's train the data on a sample of 1000 for each president and see how it fairs. Howver, I do know that these 1000 samples cannot feature the opening statements because they all say the openning statements

## Lemmatization

It may be necessary to perform lemmatization as commonly appearing words may appear in different forms for each president. So changing something like running to run may be necessary in finding commonly occuring words for example. 

```{r Cleaning Data}
# Remove numbers
sona$speech = gsub('[0-9]+', '', sona$speech)
sona
sona$speech <- lemmatize_strings(sona$speech)
sona$speech <- replace_contraction(sona$speech)
```


Unfortunately, for this data split, even for unigrams, we see very small frequencies even for words that are specific to one journal. This may mean that we will have to look at a different way to obtain an inverse weighted distribution of samples.
```{r Split Data}
# Need to keep some sentences separate.
sentences <- sona %>%
  unnest_sentences(speech, speech)

table(sentences$president)

# Let's say each president has 70 sentences (that's close to 1000 words each.)
# for each president, I would advise removing their first sentence as it appears in all statements in some way.

train <- sentences %>% 
  group_by(president) %>%
  slice_sample(n = 70) %>% 
  ungroup() %>%
  rename( token = speech )

table(train$president)

test <- sentences %>%
  unnest_sentences(token, speech) %>%
  anti_join(train)
```


```{r Tokenization}
# Unigram
uni <- train %>%
  rename( speech = token) %>%
  unnest_tokens(token, speech) %>%
  #filter(!token %in% stop_words$word) %>%
  count(president, token)

# Bigram
bigram <- train %>% 
    rename( speech = token) %>%
  unnest_tokens(token, speech, token = 'ngrams', n = 2) %>%
  count(president, token)

# Trigram
trigram <- train %>% 
    rename( speech = token) %>%
  unnest_tokens(token, speech, token = 'ngrams', n = 3) %>%
  count(president, token)

# Pentagram
pentagram <- train%>% 
    rename( speech = token) %>%
  unnest_tokens(input = speech, output = token, token = 'ngrams', n = 3) %>%
  count(president, token)
```


The word counts are helpful indicator of commonly occuring words, however, in this dataset, many of the presidents start their speeches similarly. They also have repetitive speech and points of order. It is therefore worthwhile to use the tf-idf weigthed counts instead of the frequencies of each word to find frequently occuring words. The TF-IDF finds the frequently occuring words for a particular document.

```{r Unigram}
uni %>%
  group_by(president) %>%
  summarise(total = sum(n))

uni_tf_idf <- uni %>%
  bind_tf_idf(token, president, n)

uni_tf_idf %>% 
  group_by(president) %>% 
  slice_max(tf_idf, n = 10) %>% 
  ungroup() %>%
  mutate(token = reorder(token, tf_idf, decreasing = F)) %>%
  ggplot(aes(tf_idf, token, fill = president)) +
  geom_col(show.legend = FALSE) +
  labs(x = "tf-idf", y = NULL) +
  facet_wrap(~president, ncol = 2, scales = "free")

uni_tf_idf %>%
  group_by(president) %>%
  slice_max(tf_idf, n= 10)%>%
  ungroup() %>%
  ggplot(aes(n, fct_reorder(token, n), fill = president)) +
  geom_col(show.legend = FALSE) +
  facet_wrap(~president, ncol = 2, scales = "free") +
  labs(x = "Count", y = NULL)
```

For the bigrams below, we see very useless words come into play that could be used to characterise presidents, it may be useful to consider removing stop words to improve the goodness of this study. However, these minute differences may be just a manner of speaking for some of these presidents and as such are necessary to the text. Like it could be a personality trait specific to that president.


We see below the usage of words that are not really useful to our analysis like, "last year", this appears for 3 different presidents. Maybe remove this in a user-defined stop word dictionary.
```{r Bigram}

bigram_tf_idf <- bigram %>%
  bind_tf_idf(token, president, n)

bigram %>%
  group_by(president) %>%
  summarise(total = sum(n))

bigram_tf_idf %>% 
  group_by(president) %>% 
  slice_max(tf_idf, n = 6) %>% 
  ungroup() %>%
  mutate(token = reorder(token, tf_idf, decreasing = F)) %>%
  ggplot(aes(tf_idf, token, fill = president)) +
  geom_col(show.legend = FALSE) +
  labs(x = "tf-idf", y = NULL) +
  facet_wrap(~president, ncol = 2, scales = "free")

bigram_tf_idf %>%
  group_by(president) %>%
  slice_max(tf_idf, n= 15)%>%
  ungroup() %>%
  ggplot(aes(n, fct_reorder(token, n), fill = president)) +
  geom_col(show.legend = FALSE) +
  facet_wrap(~president, ncol = 2, scales = "free") +
  labs(x = "Count", y = NULL)
```

What we can see below is that a pentagram is not exactly useful in distiguishing different presidents. We see that there are some commonly occuring phrases. But these happen a total of 3 to 5 times out of over 30 000 entries for some presidents. Therefore it is not necessary to go this far.
```{r Pentagram}
# Note that this may be useless, should maybe only go as high as a trigram
  # These Pentagrams may not be useful in categorizing much as it will not allow us much data to work with, since every phrase of 5 words will probably not repeat itself.
# Also notice that the first 20 sentences or something is repeated in every speech, so it may not be helpful to look to this extent unless we look at tf-idf

# Note that the more complex an n-gram is the harder it is to utilise for this type of task, and hence it may be better to not go further than a trigram. However, simple way to check when it is too much would be to check for increasing n and eventually the counts will be around 0. Subsequently, it is useless to use N-grams at or close to these distributions of N-grams. It may be useful to look at these quickly with tf-idf.


pent_tf_idf <- pentagram %>%
  bind_tf_idf(token, president, n) %>%
  mutate(president = factor(president, levels = c("deKlerk", "Mandela", "Mbeki", "Motlanthe", "Ramaphosa", "Zuma"))) %>%
  arrange(desc(tf_idf))

pent_tf_idf %>%
  group_by(president) %>%
  slice_max(tf_idf, n= 5)%>%
  ungroup() %>%
  ggplot(aes(tf_idf, fct_reorder(token, tf_idf), fill = president)) +
  geom_col(show.legend = FALSE) +
  facet_wrap(~president, ncol = 2, scales = "free") +
  labs(x = "tf-idf", y = NULL)

pent_tf_idf %>%
  group_by(president) %>%
  slice_max(tf_idf, n= 5)%>%
  ungroup() %>%
  ggplot(aes(n, fct_reorder(token, n), fill = president)) +
  geom_col(show.legend = FALSE) +
  facet_wrap(~president, ncol = 2, scales = "free") +
  labs(x = "Count", y = NULL)




```

A trigram in this instance is as far as I would recommend looking. Although, it is still not very stable.
```{r Trigram}
# Note that this may be useless, should maybe only go as high as a trigram
  # These Pentagrams may not be useful in categorizing much as it will not allow us much data to work with, since every phrase of 5 words will probably not repeat itself.
# Also notice that the first 20 sentences or something is repeated in every speech, so it may not be helpful to look to this extent unless we look at tf-idf

# Note that the more complex an n-gram is the harder it is to utilise for this type of task, and hence it may be better to not go further than a trigram. However, simple way to check when it is too much would be to check for increasing n and eventually the counts will be around 0. Subsequently, it is useless to use N-grams at or close to these distributions of N-grams. It may be useful to look at these quickly with tf-idf.


tri_tf_idf <- trigram %>%
  bind_tf_idf(token, president, n) %>%
  mutate(president = factor(president, levels = c("deKlerk", "Mandela", "Mbeki", "Motlanthe", "Ramaphosa", "Zuma"))) %>%
  arrange(desc(tf_idf))

tri_tf_idf %>%
  group_by(president) %>%
  slice_max(tf_idf, n= 5)%>%
  ungroup() %>%
  ggplot(aes(tf_idf, fct_reorder(token, tf_idf), fill = president)) +
  geom_col(show.legend = FALSE) +
  facet_wrap(~president, ncol = 2, scales = "free") +
  labs(x = "tf-idf", y = NULL)

tri_tf_idf %>%
  group_by(president) %>%
  slice_max(tf_idf, n= 5)%>%
  ungroup() %>%
  ggplot(aes(n, fct_reorder(token, n), fill = president)) +
  geom_col(show.legend = FALSE) +
  facet_wrap(~president, ncol = 2, scales = "free") +
  labs(x = "Count", y = NULL)
```


## EDA
<!-- a  great  variety  ofmeasures, including sentence length, word length, word fre-quencies,  character  frequencies,  and  vocabulary  richness -->

<!-- vocabulary richnessfunctions are attempts to quan-tify the diversity of the vocabulary of a tex -->

## Naive Bayes Classifier

## Feed Forward Neural Networks

## Support Vector Machines

# Results

# Discussion

# Limitations

A limitation that arises in this study is a lack of candidate authors, as well as minimal data provided for some authors. This is an issue that has been detailed in research before ([\@Stylometry]()).

# Conclusion
