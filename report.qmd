---
title: "Predict the President"
author: "Edward Baleni, BLNEDW003"
bibliography: references.bib
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE, warning=FALSE, fig.align="center", out.width = "65%", fig.pos = "H", message = F)
```

```{r Packages, include=FALSE,results='hide'}
require(dplyr)
require(stringr)
require(tidytext)
require(ggplot2)
require(tidyr)
require(forcats)
require(textstem)
require(textclean)
require(cowplot)
require(huxtable)
require(dplyr)
require(e1071)

require(keras) 
require(reticulate) 
require(tensorflow)
require(kerastuneR)
```

# Abstract

# Introduction and Literature Review

South Africa's State of the Nation Address (SONA) is an annual event where the president of the country gives a report on the status of the nation. This status entails a highlighting the key challenges and achievements witnessed in the past year as well as a mentioning of the government's goals and objectives for the foreseable future. A number of presidents have taken office between the years of 1994 and 2023. These presidents being FW De Klerk, Nelson Mandela, Thabo Mbeki, Kgalema Motlanthe, Jacob Zuma and Cyril Ramaphosa. The purpose of this study is to create a text classification task that identifies which president was the source of a given sentence. Such a task is often called authorship attribution ([\@Ngrams]()). In such a task is important that one is able to characterize each author, or speaker, in some way that is able to capture the style or ideas of each president.

Author attribution is a natural language processing (NLP) task ....([\@WordFreq]())....

There are are a number of ways to characterise authors. In this study a comparison between a topic-based text classification and a style-based text classification will be explored. Text-based text classification, attempts to not use functional words in the classification of texts, this is be used to find the general ideas or meaining of texts, as well as being able to identify topics associated with a given text; this provides semantic information. Style-based classification, Stylometry, makes strong use of the function words in classification ([\@Stylometry]()). Function words are used to aid in the syntax of a sentence rather than the meaning. These features that are not consciously used within a text and as such vary between different authors, this lack of control over function words have made them ideal for modelling function word frequencies to create an effective attribution technique ([\@FunctionWords]()). Although, the points addressed within each speech is slightly different, the main topic is that of a political address, therefore it is worthwhile to view this research in the context of a style-based text classification as well as a topic-based text classification.

In this study the classic word bag approach with token frequencies to TF-IDF

Naive Bayes SVM FFNN

In the topics to follow, the data and methodology will be defined. Following this, various models will be used and validated to select the best type of data for the analysis using base models. After this, hyperparameter tuning will be done on the top 3 models to obtain their best performances...

# Data and Methodology

The data used in this study was sourced from [gov.za](https://www.gov.za/state-nation-address). It is a collection of 36 addresses from 6 different presidents within the years 1994 to 2023. The speakers present in this time span were FW De Klerk, Nelson Mandela, Thabo Mbeki, Kgalema Motlanthe, Jacob Zuma and Cyril Ramaphosa. Web-scraping techniques were used to collect this data, courtesy of Ian Durbach.

```{r Data}
set.seed(2023)
sys.source("sona-first-steps.R" , envir = knitr::knit_global())
sona <- as_tibble(sona)
```

## EDA

<!-- a  great  variety  ofmeasures, including sentence length, word length, word fre-quencies,  character  frequencies,  and  vocabulary  richness -->

<!-- vocabulary richnessfunctions are attempts to quan-tify the diversity of the vocabulary of a tex -->

<!-- Perform a quick EDA to explain the data and a quick rationale for removing the outliers -->

## Pre-Processing

After obtaining the data, the first line of each speech is removed. This first line is the date the address is held. The data is then tokenized by sentence. Only sentences with over two words remained for the rest of the analysis. The reason this is done is because the sentences with less than 3 words seemed to either either be made up of digits or unfinished words, most of the time.

```{r Pre-Process Lex}
sentences <- sona %>%
  unnest_sentences(speech, speech, strip_punct = F)

str.ct <- function(x){
  hold <- length(unlist(str_split(x, " ")))
  return(hold)
}

sentences$StringCount <- unlist(lapply(sentences$speech, str.ct))
sentences <- sentences[sentences$StringCount > 2,]
```

Following this many different considerations have been made in the [Supplementary](supplementary.qmd) to find the best representation of the data for classification. Both lexical and character features are considered; the inclusion and exclusion of function words (stop words include function words); using token frequencies and TF-IDF are considered; different sized word bags are considered; the inclusion of case and punctuation are are also considered. Lexical features regard tokens as a sequence of words, where a unigram is a single word, a bigram are two contiguous words, a trigram is three and so forth. Character features are a sequence of contiguous characters also using n-grams to characterise how many characters are present in each feature. These two types of features qualify as two of the most basic markers used for identifying an author's style, where lexical features are slightly more complex than character features ([\@Stylometry]()). Of the many options explored in the [Supplementary](supplementary.qmd), only two of the models will be represented here. The first approach will use unigrams of words to represent lexical features, while the second uses character features with a 4 character n-gram, this is how we will distinguish between the two going forward. These are important distinctions at this stage as they determine how the data is organised.

In the [Supplementary](supplementary.qmd), both a topic-based approach and style-based approach were looked into. It was found that the style-based approach performed better than topic-based approach, in most scenarios. This was illustrated in the improvement of model performance of the models with the inclusion of function words. This idea will be furthered explored below.

### Lexical Features

There is complexity that comes with using lexical features; the sparsity within the bag-of-words of such features was a the reason why capitalization was not deemed appropriate in conjunction with lexical features ([\@LexVsChar]()). Capitalization within the transcription of a speech can only indicate the beginning of an idea, sentiment cannot be captured by such. Whereas, in the context of a book, capitalization can be use to convey a strong meaning in a character's words.

First the speeches are tokenized into sentences and are given sentence ids. Thereafter sentences were cleaned quite strictly. All digits are removed using a regex operation. Contractions are replaced with their long form via regex pattern matching ([\@textclean]()). Punctuation is maintained as punctuation is able to characterise syntatic information ([\@Punc]()). Punctuation is maintained, however, special unicode characters, astrixes, and other punctuation that is not commonly seen in text are removed, only full stops, question marks, commas, exclamation marks, brackets, apostrophes and curly brackets are maintained. Lemmatization is performed as commonly appearing words appear in different forms for each president. A lemma is a set of words that have the same stem, where each variation of the word is called a word form. Lemmatization is a process that is able to determine that two words have the same root. To lemmatize is to change wordforms into their root. This process of lemmatization is done by morphological parsing, where this separates the morphenes into parts. A morphene is a smaller building block of a word. Lemmatization was done by using a dictionary of common wordforms, if a word is a part of a lemma, then it will be matched to the dictionary and replaced by the morphenes that make it up. After performing this lemmatization, the affixes, additional morphenes that aren't the stem, are still present in the data. Affixes are often random letters that will appear ([\@Jurafsky2000SpeechAL]()). The final step in cleaning is to remove these affixes

```{r Lex Clean}
clean1 <- function(x){
  # remove contractions
  x <- textclean::replace_contraction(x)
  # lemmatization
  x <- lemmatize_strings(x)
  # remove single characters and Affixes
  x <- gsub("\\s.\\s", " ", x)
  # remove numbers
  x <- gsub("[0-9]", "", x)
}

cleaned.sentences <- sentences
cleaned.sentences$speech <- unlist(lapply(cleaned.sentences$speech, clean1))
```

### Character Features

Character features are more simplistic than lexical features and are able to capture stylistic nuances. They are able to capture some lexical and contextual information. They do however carry a lot of redundancies resulting in high dimensionality.

The data here is cleaned differently. In the [Supplementary](supplementary.qmd), it is shown that the feature space that includes both punctuation and capitalization performs the best of all the character spaces. A character n-gram of both 3 and 4 features were assessed, and both are decently sized n-grams. Here the character n-gram of 4 contiguous characters will be used.

First the speeches are tokenized into sentences, and given an id. Where previously the text would have been set to lower case and the punctuation was included, here both the text maintains it's capitalization and the punctuation remain. To clean these sentences, all digits were removed, followed by the lemmatization of wordforms to stems.

```{r Pre-Process Chars}
sentences.cap.punc             <- sona %>%
  unnest_sentences(speech, speech, to_lower = F, strip_punct = F)

sentences.cap.punc$StringCount <- unlist(lapply(sentences.cap.punc$speech, str.ct))
sentences.cap.punc             <- sentences.cap.punc[sentences.cap.punc$StringCount > 2,]
```

```{r N-gram with 4 Chars}
clean2 <- function(x){
  # remove numbers
  x <- gsub("\\d+", "", x)
  # lemmatization
  x <- lemmatize_strings(x)
}

sentences.cap.punc$speech <- unlist(lapply(sentences.cap.punc$speech, clean2))
```

### Imbalanced Classes

The data has imbalanced classes present. This means that the distribution of the data across classes is skewed. Motlanthe and de Klerk both only gave one speech. This was Motlanthe's and de Klerk's outgoing speeches; every other presidents managed to give more than one speech.

There are a number of ways to deal with imbalanced classes, for future work one might look into @LexVsChar to get an idea of what strategies do actually work on text classification tasks. In this report, these outlying classes were removed from the remainder of the analysis.

```{r Remove Imabalances Lex}
data_count <- cleaned.sentences %>%
  filter(!president %in% c("deKlerk", "Motlanthe"))
data_count$president <- as.factor(data_count$president)

data_count <- data_count %>%
  mutate(ids_ = row_number())
```

```{r Remove Imbalances Char}
data_count2 <- sentences.cap.punc %>%
  filter(!president %in% c("deKlerk", "Motlanthe"))
data_count2$president <- as.factor(data_count2$president)

data_count2 <- data_count2 %>%
  mutate(ids_ = row_number())
```

## Bag of Words

It is important that we get the data into a format that can be used by the classifiers. A bag of words is one such way to do this. The text is placed into something called a bag of words. This is an unordered set of words that contain the frequency of each word in the document ([\@Jurafsky2000SpeechAL]()). For both lexical and character features, the pre-processed sentences need to be tokenized down to their desired level. The lexical features are tokenized as unigrams of words, while including punctuation as individual words; the character features are tokenized as n-grams with 4 characters, with the inclusion of punctuation and capitalization. Both types of features included function words, as this was decided to be a stylometric exercise as per the [Supplementary](supplementary.qmd). In both cases, the sentence id was maintained over each word to allocate which sentence it belongs to.

In both cases, lexical and character features, the most important features are the most frequent. With this understanding, a crude feature selection, can be conducted for both.

After tokenizing, most frequent features were selected. This was tried as 200, 500, 2000, 4500 and all the tokens for both the lexical features and the character features. After making these selections, a word bag was created for each selection of top features.

```{r Unigram 3}
uni.3 <- data_count %>%
  unnest_tokens(token, speech, token = "words", strip_punct =FALSE, strip_numeric = TRUE)
  
word.bag.uni.200 <- uni.3 %>%
  group_by(token) %>%
  count() %>%
  ungroup() %>%
  top_n(200, wt = n) %>%
  select(-n)

word.bag.uni.500 <- uni.3 %>%
  group_by(token) %>%
  count() %>%
  ungroup() %>%
  top_n(500, wt = n) %>%
  select(-n)

word.bag.uni.2000 <- uni.3 %>%
  group_by(token) %>%
  count() %>%
  ungroup() %>%
  top_n(2000, wt = n) %>%
  select(-n)

word.bag.uni.4500 <- uni.3 %>%
  group_by(token) %>%
  count() %>%
  ungroup() %>%
  top_n(4500, wt = n) %>%
  select(-n)

word.bag.uni.all <-  uni.3 %>%
  group_by(token) %>%
  count() %>%
  ungroup() %>%
  select(-n)

uni.tdf.200 <- uni.3 %>%
  inner_join(word.bag.uni.200) %>%
  group_by(ids_,token) %>%
  count() %>%  
  group_by(ids_) %>%
  mutate(total = sum(n)) %>%
  ungroup()


bag.of.words.uni.200 <- uni.tdf.200 %>% 
  select(ids_, token, n) %>% 
  pivot_wider(names_from = token, values_from = n, values_fill = 0) 


bag.of.words.uni.200 <- data_count %>%
  select(ids_, president) %>%
  rename (Response_President = president) %>%
  right_join(bag.of.words.uni.200, join_by("ids_")) %>%
  select(ids_, Response_President, everything()) 


uni.tdf.500 <- uni.3 %>%
  inner_join(word.bag.uni.500) %>%
  group_by(ids_,token) %>%
  count() %>%  
  group_by(ids_) %>%
  mutate(total = sum(n)) %>%
  ungroup()


bag.of.words.uni.500 <- uni.tdf.500 %>% 
  select(ids_, token, n) %>% 
  pivot_wider(names_from = token, values_from = n, values_fill = 0) 


bag.of.words.uni.500 <- data_count %>%
  select(ids_, president) %>%
  rename (Response_President = president) %>%
  right_join(bag.of.words.uni.500, join_by("ids_")) %>%
  select(ids_, Response_President, everything()) 

uni.tdf.2000 <- uni.3 %>%
  inner_join(word.bag.uni.2000) %>%
  group_by(ids_,token) %>%
  count() %>%  
  group_by(ids_) %>%
  mutate(total = sum(n)) %>%
  ungroup()


bag.of.words.uni.2000 <- uni.tdf.2000 %>% 
  select(ids_, token, n) %>% 
  pivot_wider(names_from = token, values_from = n, values_fill = 0) 


bag.of.words.uni.2000 <- data_count %>%
  select(ids_, president) %>%
  rename (Response_President = president) %>%
  right_join(bag.of.words.uni.2000, join_by("ids_")) %>%
  select(ids_, Response_President, everything()) 

uni.tdf.4500 <- uni.3 %>%
  inner_join(word.bag.uni.4500) %>%
  group_by(ids_,token) %>%
  count() %>%  
  group_by(ids_) %>%
  mutate(total = sum(n)) %>%
  ungroup()


bag.of.words.uni.4500 <- uni.tdf.4500 %>% 
  select(ids_, token, n) %>% 
  pivot_wider(names_from = token, values_from = n, values_fill = 0) 


bag.of.words.uni.4500 <- data_count %>%
  select(ids_, president) %>%
  rename (Response_President = president) %>%
  right_join(bag.of.words.uni.4500, join_by("ids_")) %>%
  select(ids_, Response_President, everything()) 




uni.tdf.all <- uni.3 %>%
  inner_join(word.bag.uni.all) %>%
  group_by(ids_,token) %>%
  count() %>%  
  group_by(ids_) %>%
  mutate(total = sum(n)) %>%
  ungroup()


bag.of.words.uni.all <- uni.tdf.all %>% 
  select(ids_, token, n) %>% 
  pivot_wider(names_from = token, values_from = n, values_fill = 0) 


bag.of.words.uni.all <- data_count %>%
  select(ids_, president) %>%
  rename (Response_President = president) %>%
  right_join(bag.of.words.uni.all, join_by("ids_")) %>%
  select(ids_, Response_President, everything()) 
```

```{r 4 Characters Case}
char <- data_count2 %>%
  unnest_character_shingles(token, speech, n = 4, strip_non_alphanum = F, to_lower = F) 
  
word.bag.char.200 <- char %>%
  group_by(token) %>%
  count() %>%
  ungroup() %>%
  top_n(200, wt = n) %>%
  select(-n)

char.tdf <- char %>%
  inner_join(word.bag.char.200) %>%
  group_by(ids_,token) %>%
  count() %>%  
  group_by(ids_) %>%
  mutate(total = sum(n)) %>%
  ungroup()


bag.of.words.char.200 <- char.tdf %>% 
  select(ids_, token, n) %>% 
  pivot_wider(names_from = token, values_from = n, values_fill = 0) 


bag.of.words.char.200 <- data_count2 %>%
  select(ids_, president) %>%
  rename (Response_President = president) %>%
  right_join(bag.of.words.char.200, join_by("ids_")) %>%
  select(ids_, Response_President, everything()) 

word.bag.char.500 <- char %>%
  group_by(token) %>%
  count() %>%
  ungroup() %>%
  top_n(500, wt = n) %>%
  select(-n)

char.tdf <- char %>%
  inner_join(word.bag.char.500) %>%
  group_by(ids_,token) %>%
  count() %>%  
  group_by(ids_) %>%
  mutate(total = sum(n)) %>%
  ungroup()


bag.of.words.char.500 <- char.tdf %>% 
  select(ids_, token, n) %>% 
  pivot_wider(names_from = token, values_from = n, values_fill = 0) 


bag.of.words.char.500 <- data_count2 %>%
  select(ids_, president) %>%
  rename (Response_President = president) %>%
  right_join(bag.of.words.char.500, join_by("ids_")) %>%
  select(ids_, Response_President, everything()) 

word.bag.char.2000 <- char %>%
  group_by(token) %>%
  count() %>%
  ungroup() %>%
  top_n(2000, wt = n) %>%
  select(-n)

char.tdf <- char %>%
  inner_join(word.bag.char.2000) %>%
  group_by(ids_,token) %>%
  count() %>%  
  group_by(ids_) %>%
  mutate(total = sum(n)) %>%
  ungroup()


bag.of.words.char.2000 <- char.tdf %>% 
  select(ids_, token, n) %>% 
  pivot_wider(names_from = token, values_from = n, values_fill = 0) 


bag.of.words.char.2000 <- data_count2 %>%
  select(ids_, president) %>%
  rename (Response_President = president) %>%
  right_join(bag.of.words.char.2000, join_by("ids_")) %>%
  select(ids_, Response_President, everything()) 

word.bag.char.4500 <- char %>%
  group_by(token) %>%
  count() %>%
  ungroup() %>%
  top_n(4500, wt = n) %>%
  select(-n)

char.tdf <- char %>%
  inner_join(word.bag.char.4500) %>%
  group_by(ids_,token) %>%
  count() %>%  
  group_by(ids_) %>%
  mutate(total = sum(n)) %>%
  ungroup()


bag.of.words.char.4500 <- char.tdf %>% 
  select(ids_, token, n) %>% 
  pivot_wider(names_from = token, values_from = n, values_fill = 0) 


bag.of.words.char.4500 <- data_count2 %>%
  select(ids_, president) %>%
  rename (Response_President = president) %>%
  right_join(bag.of.words.char.4500, join_by("ids_")) %>%
  select(ids_, Response_President, everything()) 

word.bag.char.all <- char %>%
  group_by(token) %>%
  count() %>%
  ungroup() %>%
  select(-n)

char.tdf <- char %>%
  inner_join(word.bag.char.all) %>%
  group_by(ids_,token) %>%
  count() %>%  
  group_by(ids_) %>%
  mutate(total = sum(n)) %>%
  ungroup()


bag.of.words.char.all <- char.tdf %>% 
  select(ids_, token, n) %>% 
  pivot_wider(names_from = token, values_from = n, values_fill = 0) 


bag.of.words.char.all <- data_count2 %>%
  select(ids_, president) %>%
  rename (Response_President = president) %>%
  right_join(bag.of.words.char.all, join_by("ids_")) %>%
  select(ids_, Response_President, everything()) 
```

## Term Frequency - Inverse Document Frequency (TF-IDF)

For comparison, TF-IDF was also considered. Term frequency, TF, is a measure of how frequently a term occurs in a document. TF can be found as,

$$\text{tf(term t in document i)} = \frac{\text{Number of times term t appears in document i}}{\text{Number of terms in document i}}.$$

Inverse document term frequency downweights terms that are frequent in a collection of documents and upweights terms that are not frequent within a collection of documents. It is calculated as,

$$\text{idf(term t)} = \text{ln}\left(\frac{\text{Number of documents in corpus}}{\text{Number of documents containing term t}}\right).$$

TF-IDF is the multiplication of the two metrics ([\@TextMining](), @Durbach2023 ). TF-IDF measures the importance of a word to a document in a collection. In this analysis this has been done by considering one document to be a sentence. Following this intuition, the quantities above can be calculated.

Since TF-IDF determines what features are important to which document, this can be seen as a topic-based classification, as function words and commonly occurring tokens among the documents will be downweighted resulting in semantic features being strongly weighted. For TF-IDF, all tokens were kept, because some of the highly weighted scores may only appear once in a document and as a result may act as quite a poor classifier out-of-sample, and so it is necessary to leave all possible tokens.

```{r TFIDF}
uni_ <- data_count2 %>%
  unnest_character_shingles(token, speech, n = 4, strip_non_alphanum = F, to_lower = F) 

uni_tdf <-  uni_ %>%
  group_by(ids_, token) %>%
  count() %>%
  group_by(ids_) %>%
  mutate(total = sum(n)) %>%
  ungroup()

uni_tdf <- uni_tdf %>%
  bind_tf_idf(token, ids_, n)

tfidf <- uni_tdf %>% 
  select(ids_, token, tf_idf) %>%  # note the change, using tf-idf
  pivot_wider(names_from = token, values_from = tf_idf, values_fill = 0)


tfidf <- data_count %>%
  select(ids_, president) %>%
  rename (Response_President = president) %>%
  right_join(tfidf, join_by("ids_")) %>%
  select(ids_, Response_President, everything()) 
```

## Train, Validation and Testing Splits

For the training, validation and test splits, it would be wise to select sample sizes by sentence and not by word, this will represent each row in the bag of words.

The data will have a 60:20:20 split of training, validation and test data. The training set will represent 60% of the total number of sentences available, the validation set will represent 20% and the test set will be the remaining 20%. These splits are done for each word bag and for tf-idf.

```{r Splits}
id           <- unique(data_count$ids_)
N            <- length(id) 

# Training set
train <- sample(id, size = N * 0.6)

# Validation set
not_train    <- na.omit(ifelse(!id %in% train, id, NA))
NN           <- length(not_train)
val          <- sample(not_train, size = NN * 0.5 )

# Test set
test         <- na.omit(ifelse(!not_train %in% val, not_train, NA))
```

```{r Train Val Test}
# 200
train.set.uni.1    <- bag.of.words.uni.200[bag.of.words.uni.200$ids_ %in% train, -1]
val.set.uni.1      <- bag.of.words.uni.200[bag.of.words.uni.200$ids_ %in% val, -1]
test.set.uni.1     <- bag.of.words.uni.200[bag.of.words.uni.200$ids_ %in% test, -1]

train.set.char.1    <- bag.of.words.char.200[bag.of.words.char.200$ids_ %in% train, -1]
val.set.char.1      <- bag.of.words.char.200[bag.of.words.char.200$ids_ %in% val, -1]
test.set.char.1     <- bag.of.words.char.200[bag.of.words.char.200$ids_ %in% test, -1]

# 500
train.set.uni.2    <- bag.of.words.uni.500[bag.of.words.uni.500$ids_ %in% train, -1]
val.set.uni.2      <- bag.of.words.uni.500[bag.of.words.uni.500$ids_ %in% val, -1]
test.set.uni.2     <- bag.of.words.uni.500[bag.of.words.uni.500$ids_ %in% test, -1]

train.set.char.2    <- bag.of.words.char.500[bag.of.words.char.500$ids_ %in% train, -1]
val.set.char.2      <- bag.of.words.char.500[bag.of.words.char.500$ids_ %in% val, -1]
test.set.char.2     <- bag.of.words.char.500[bag.of.words.char.500$ids_ %in% test, -1]

# 2000
train.set.uni.3    <- bag.of.words.uni.2000[bag.of.words.uni.2000$ids_ %in% train, -1]
val.set.uni.3      <- bag.of.words.uni.2000[bag.of.words.uni.2000$ids_ %in% val, -1]
test.set.uni.3     <- bag.of.words.uni.2000[bag.of.words.uni.2000$ids_ %in% test, -1]

train.set.char.3    <- bag.of.words.char.2000[bag.of.words.char.2000$ids_ %in% train, -1]
val.set.char.3      <- bag.of.words.char.2000[bag.of.words.char.2000$ids_ %in% val, -1]
test.set.char.3     <- bag.of.words.char.2000[bag.of.words.char.2000$ids_ %in% test, -1]

# 4500

train.set.uni.4    <- bag.of.words.uni.4500[bag.of.words.uni.4500$ids_ %in% train, -1]
val.set.uni.4      <- bag.of.words.uni.4500[bag.of.words.uni.4500$ids_ %in% val, -1]
test.set.uni.4     <- bag.of.words.uni.4500[bag.of.words.uni.4500$ids_ %in% test, -1]

train.set.char.4    <- bag.of.words.char.4500[bag.of.words.char.4500$ids_ %in% train, -1]
val.set.char.4      <- bag.of.words.char.4500[bag.of.words.char.4500$ids_ %in% val, -1]
test.set.char.4     <- bag.of.words.char.4500[bag.of.words.char.4500$ids_ %in% test, -1]

# All
train.set.uni.all    <- bag.of.words.uni.all[bag.of.words.uni.all$ids_ %in% train, -1]
val.set.uni.all      <- bag.of.words.uni.all[bag.of.words.uni.all$ids_ %in% val, -1]
test.set.uni.all     <- bag.of.words.uni.all[bag.of.words.uni.all$ids_ %in% test, -1]

train.set.char.all    <- bag.of.words.char.all[bag.of.words.char.all$ids_ %in% train, -1]
val.set.char.all      <- bag.of.words.char.all[bag.of.words.char.all$ids_ %in% val, -1]
test.set.char.all     <- bag.of.words.char.all[bag.of.words.char.all$ids_ %in% test, -1]

train.set.tfidf     <- tfidf[tfidf$ids_ %in% train, -1]
val.set.tfidf     <- tfidf[tfidf$ids_ %in% val, -1]
test.set.tfidf     <- tfidf[tfidf$ids_ %in% test, -1]
```

## Naive Bayes Classifier

A multinomial naive Bayes classifier was used to classify text. This is a probabilistic classifier, given the document $d$ and classes $c \in C$, the classifier will predict the class, $\hat{c}$, with the maximum posterior probability,

$$\hat{c} = \underset{c\in C}{\arg\max} P(c|d)$$

Using Bayes rule, the above prediction can be further broken down into,

$$\hat{c} = \underset{c\in C}{\arg\max} P(c|d) = \underset{c\in C}{\arg\max} \frac{P(d|c)P(c)}{P(d)}$$

This can be further simplified by to,

$$\hat{c} = \underset{c\in C}{\arg\max} P(d|c)P(c) $$

as $P(d)$ stays constant for all classes, which allows us to cancel this out. This equation above is made up of the likelihood of the document and prior probability of the class. The documents can subsequently be divided up into features, be it our lexical or character features above. Following this great number of features, $\{f_i : i = 1,2,...,n\}$, this classifier makes two simplifying assumptions that would infer it's naivety, that the tokens are position independent and that the joint likelihood of the features also maintain independence,

$$P(d|c) = P(f_1, f_2, ..., f_n|c) = P(f_1|c)P(f_2|c)...P(f_n|c)$$ The final equation of the naive Bayes classifier then becomes,

$$\hat{c}_{NB} = \underset{c\in C}{\arg\max} P(c) \prod_{f\in F} P(f|c)$$

([\@Jurafsky2000SpeechAL]()).

This classifier was made with no Laplace smoothing. Laplace smoothing, will be looked into for the final models. This is a simple smoothing algorithm that can be easily implemented in this classifier. It simply adds to the count the number specified before finding probabilities ([\@Jurafsky2000SpeechAL]()).

```{r Naive Bayes Classifier Lexical}

# data.classifier.1.1  <- naiveBayes(train.set.uni.1[,-1], train.set.uni.1[,1])
# p.1.1                <- predict(data.classifier.1.1, val.set.uni.1[,-1])
# t.1.1                <- table((p.1.1), (unlist(val.set.uni.1[,1])))
# 
# data.classifier.2.1  <- naiveBayes(train.set.uni.2[,-1], train.set.uni.2[,1])
# p.2.1                <- predict(data.classifier.2.1, val.set.uni.2[,-1])
# t.2.1                <- table((p.2.1), (unlist(val.set.uni.2[,1])))
# 
# data.classifier.3.1  <- naiveBayes(train.set.uni.3[,-1], train.set.uni.3[,1])
# p.3.1                <- predict(data.classifier.3.1, val.set.uni.3[,-1])
# t.3.1                <- table((p.3.1), (unlist(val.set.uni.3[,1])))
# 
# data.classifier.4.1  <- naiveBayes(train.set.uni.4[,-1], train.set.uni.4[,1])
# p.4.1                <- predict(data.classifier.4.1, val.set.uni.4[,-1])
# t.4.1                <- table((p.4.1), (unlist(val.set.uni.4[,1])))
# 
# data.classifier.all.1  <- naiveBayes(train.set.uni.all[,-1], train.set.uni.all[,1])
# p.all.1                <- predict(data.classifier.all.1, val.set.uni.all[,-1])
# t.all.1                <- table((p.all.1), (unlist(val.set.uni.all[,1])))
# 
# save(p.1.1,
#      t.1.1,
#      p.2.1,
#      t.2.1,
#      p.3.1,
#      t.3.1,
#      t.4.1,
#      p.4.1,
#      p.all.1,
#      t.all.1,
#      data.classifier.1.1, 
#      data.classifier.2.1, 
#      data.classifier.3.1, 
#      data.classifier.4.1, 
#      data.classifier.all.1, file = "naiveLexical.RData")
load("naiveLexical.RData")
```

```{r Naive Bayes Classifier Character}
# data.classifier.1.2  <- naiveBayes(train.set.char.1[,-1], train.set.char.1[,1])
# p.1.2                <- predict(data.classifier.1.2, val.set.char.1[,-1])
# t.1.2                <- table((p.1.2), (unlist(val.set.char.1[,1])))
# 
# data.classifier.2.2  <- naiveBayes(train.set.char.2[,-1], train.set.char.2[,1])
# p.2.2                <- predict(data.classifier.2.2, val.set.char.2[,-1])
# t.2.2                <- table((p.2.2), (unlist(val.set.char.2[,1])))
# 
# data.classifier.3.2  <- naiveBayes(train.set.char.3[,-1], train.set.char.3[,1])
# p.3.2                <- predict(data.classifier.3.2, val.set.char.3[,-1])
# t.3.2                <- table((p.3.2), (unlist(val.set.char.3[,1])))
# 
# data.classifier.4.2  <- naiveBayes(train.set.char.4[,-1], train.set.char.4[,1])
# p.4.2                <- predict(data.classifier.4.2, val.set.char.4[,-1])
# t.4.2                <- table((p.4.2), (unlist(val.set.char.4[,1])))
# 
# data.classifier.all.2  <- naiveBayes(train.set.char.all[,-1], train.set.char.all[,1])
# p.all.2                <- predict(data.classifier.all.2, val.set.char.all[,-1])
# t.all.2                <- table((p.all.2), (unlist(val.set.char.all[,1])))
# 
# save(p.1.2,
#      t.1.2,
#      p.2.2,
#      t.2.2,
#      p.3.2,
#      t.3.2,
#      t.4.2,
#      p.4.2,
#      p.all.2,
#      t.all.2,
#      data.classifier.1.2, 
#      data.classifier.2.2, 
#      data.classifier.3.2, 
#      data.classifier.4.2, 
#      data.classifier.all.2, file = "naiveCharacter.RData")

load("naiveCharacter.RData")
```

```{r Naive Bayes Classifier TFIDF}
# tfidf.classifier  <- naiveBayes(train.set.tfidf[,-1], train.set.tfidf[,1])
# tfidf.p           <- predict(tfidf.classifier, val.set.tfidf[,-1])
# tfidf.t           <- table((tfidf.p), (unlist(val.set.tfidf[,1])))
# 
# save(tfidf.classifier, tfidf.p, tfidf.t, file = "naiveTFIDF.RData")

load("naiveTFIDF.RData")
```

## Support Vector Machines

Support vector machines, SVM, are widely used in the space of text classification ([\@Stylometry]()). SVMs perform linear operations in upto an infinite number of dimensions. If data is not linearly separable, SVMs can be used to linearly separable in a higher dimension. SVMs allow to find an optimal separating hyperplane for separable and non-separable data.

The goal is to evaluate the hyperplane such that it creates the greatest margin between the training points of each class and itself. The margin is the distance between the nearest data point to the plane.

If the feature space is non-linenar we would look to use a set of basis expansions as our input space,

$$
h(x_i) = (h_1(x_i), h_2(x_i), ..., h_M(x_i)), \quad i = 1, 2, ..., N
$$

the inner product of these basis expansions, $h(x)$, will represent the desired kernal, $K(x, x')$, used to move feature space to a higher dimension. The hyperplane will therefore be defined as,

$$
{f}(x) = h(x)^T\beta + \beta_0
$$

where $h(x)$ is the kernal. It is difficult to have a hyperplane that perfectly classifies, to tune the model some slack is allowed. This is a soft-margin SVM. Slack are observations that are allowed to be missclassified. SVMs maximise the margin with some regularization,

$$
\frac{1}{2}\beta^T\beta + C\sum_{n=1}^N \zeta_n
$$

subject to,

$$
y_i(h(x)^T\beta + \beta_0) \geq 1 - \zeta_n, \quad n = 1,2,...,N \\
\zeta_n \geq 0, \quad\quad n = 1,2,...,N
$$

this is a convex optimisation problem (@Hastie, @Et2 ) . For multiclass problems the package provided by @e1071 uses a one-versus-one approach. This will fit all pairwise classifiers and use a voting system to pick the best ones.

A base model has been used to explore our data, hyperparameter tuning is used to regularize the model, by using cost constraint.

```{r SVM Classifier Lexical}

# svm.classifier.1.1  <- svm(train.set.uni.1[,-1], train.set.uni.1[,1], type = "C-classification", scale = T)
# svm.p.1.1                <- predict(svm.classifier.1.1, val.set.uni.1[,-1])
# svm.t.1.1                <- table((svm.p.1.1), (unlist(val.set.uni.1[,1])))
# 
# svm.classifier.2.1  <- svm(train.set.uni.2[,-1], train.set.uni.2[,1],type = "C-classification", scale = T)
# svm.p.2.1                <- predict(svm.classifier.2.1, val.set.uni.2[,-1])
# svm.t.2.1                <- table((svm.p.2.1), (unlist(val.set.uni.2[,1])))
# 
# svm.classifier.3.1  <- svm(train.set.uni.3[,-1], train.set.uni.3[,1],type = "C-classification", scale = T)
# svm.p.3.1                <- predict(svm.classifier.3.1, val.set.uni.3[,-1])
# svm.t.3.1                <- table((svm.p.3.1), (unlist(val.set.uni.3[,1])))
# 
# svm.classifier.4.1  <- svm(train.set.uni.4[,-1], train.set.uni.4[,1],type = "C-classification", scale = T)
# svm.p.4.1                <- predict(svm.classifier.4.1, val.set.uni.4[,-1])
# svm.t.4.1                <- table((svm.p.4.1), (unlist(val.set.uni.4[,1])))
# 
# svm.classifier.all.1  <- svm(train.set.uni.all[,-1], train.set.uni.all[,1],type = "C-classification", scale = T)
# svm.p.all.1                <- predict(svm.classifier.all.1, val.set.uni.all[,-1])
# svm.t.all.1                <- table((svm.p.all.1), (unlist(val.set.uni.all[,1])))
# 
# save(svm.p.1.1,
#      svm.t.1.1,
#      svm.p.2.1,
#      svm.t.2.1,
#      svm.p.3.1,
#      svm.t.3.1,
#      svm.t.4.1,
#      svm.p.4.1,
#      svm.p.all.1,
#      svm.t.all.1,
#      svm.classifier.1.1, 
#      svm.classifier.2.1, 
#      svm.classifier.3.1, 
#      svm.classifier.4.1, 
#      svm.classifier.all.1, file = "svmLexical.RData")

load("svmLexical.RData")

```

```{r SVM Classifier Character}
# svm.classifier.1.2       <- svm(train.set.char.1[,-1], train.set.char.1[,1], type = "C-classification", scale = T)
# svm.p.1.2                <- predict(svm.classifier.1.2, val.set.char.1[,-1])
# svm.t.1.2                <- table((svm.p.1.2), (unlist(val.set.char.1[,1])))
# 
# svm.classifier.2.2       <- svm(train.set.char.2[,-1], train.set.char.2[,1], type = "C-classification", scale = T)
# svm.p.2.2                <- predict(svm.classifier.2.2, val.set.char.2[,-1])
# svm.t.2.2                <- table((svm.p.2.2), (unlist(val.set.char.2[,1])))
# 
# svm.classifier.3.2       <- svm(train.set.char.3[,-1], train.set.char.3[,1], type = "C-classification", scale = T)
# svm.p.3.2                <- predict(svm.classifier.3.2, val.set.char.3[,-1])
# svm.t.3.2                <- table((svm.p.3.2), (unlist(val.set.char.3[,1])))
# 
# svm.classifier.4.2       <- svm(train.set.char.4[,-1], train.set.char.4[,1], type = "C-classification", scale = T)
# svm.p.4.2                <- predict(svm.classifier.4.2, val.set.char.4[,-1])
# svm.t.4.2                <- table((svm.p.4.2), (unlist(val.set.char.4[,1])))
# 
# svm.classifier.all.2       <- svm(train.set.char.all[,-1], train.set.char.all[,1], type = "C-classification", scale = T)
# svm.p.all.2                <- predict(svm.classifier.all.2, val.set.char.all[,-1])
# svm.t.all.2                <- table((svm.p.all.2), (unlist(val.set.char.all[,1])))
# 
# save(svm.p.1.2,
#      svm.t.1.2,
#      svm.p.2.2,
#      svm.t.2.2,
#      svm.p.3.2,
#      svm.t.3.2,
#      svm.t.4.2,
#      svm.p.4.2,
#      svm.p.all.2,
#      svm.t.all.2,
#      svm.classifier.1.2, 
#      svm.classifier.2.2, 
#      svm.classifier.3.2, 
#      svm.classifier.4.2, 
#      svm.classifier.all.2, file = "svmCharacter.RData")

load("svmCharacter.RData")

```

```{r SVM Classifier TFIDF}
# svm.tfidf.classifier  <- svm(train.set.tfidf[,-1], train.set.tfidf[,1], type = "C-classification", scale = F)
# svm.tfidf.p           <- predict(svm.tfidf.classifier, val.set.tfidf[,-1])
# svm.tfidf.t           <- table((svm.tfidf.p), (unlist(val.set.tfidf[,1])))
# 
# save(svm.tfidf.classifier, svm.tfidf.p, svm.tfidf.t, file = "svmTFIDF.RData")

load("svmTFIDF.RData")
```

## Feed Forward Neural Networks

A neural network is a series of non-linear basis functions, this is handled via interconnected nodes over a number of layers. Each layer has a number of nodes and is connected to every other node in the layer before (if any) and after it (if any). The case where there are no layers before the current layer is for the input layer, which is a layer that takes in data. The number of nodes present at the input layer are equal to the number of features in the bag of words. Similarly, the case where there are no layers after the current layer is for the output layer, in this text classification task. Since the task is that of multinomial classification, the number of nodes on this output layer is equivalent to the number of classes available. The layers between the input layer and the output layer are called hidden layers. The output of each layer is passed on as the input to the next layer. The hidden layers are capable of taking a weighted sum of their inputs and applying a non-linear activation function to this to obtain its output ([\@Jurafsky2000SpeechAL](), [\@Hastie]()). Forward and backward passes of the data are used to change the parameters in order to optimize an objective function.

A general way in which to represent this operation is the following,

$$a^l_j(i) = \sigma_l\left( \sum_{k = 1}^{d_l - 1} a_k^{l-1}(i)w_{kj}^l + b_j^l \right), \quad l = 1,2,...,L; j = 1,2,...d_l $$

where $l$ represents the number of layers, $d_l$ the number of nodes in layer $l$, $\sigma_l(.)$ denotes the activation function, $w_{kj}^l$ denotes the weight linking the $k^{th}$ node in layer $l-1$ to the $j^{th}$ node in layer $l$, $b_j^l$ denotes the $j^{th}$ bias in layer $l$ ([\@Et]()).

The activation for the hidden layers has been chosen as the reLu activation function and the output layer uses a soft-max activation function. The classification task uses accuracy rate to evaluate model performance. Like the other models this is just a measure of correctly classified predictions.

To find the best data in terms of FFNN, the bag of words at different word bags was assessed against a default model with 32 nodes, an input layer with relu activation function, a dropout rate of 0.5, an output layer with a softmax activation function and performacne calculated via accuracy. Later on, hyperparameter tuning to find a better model will happen and used to test the final data.

```{r Data for FFNN}

# Lexical
y.train.1.1 <- as.integer(unlist(train.set.uni.1[,1])) ; x.train.1.1 <- as.matrix(train.set.uni.1[,-1])
y.val.1.1   <- as.integer(unlist(val.set.uni.1[,1]))   ; x.val.1.1   <- as.matrix(val.set.uni.1[,-1]);  
y.train.1.1 <- to_categorical(y.train.1.1)             ; y.val.1.1   <- to_categorical(y.val.1.1)

y.train.1.2 <- as.integer(unlist(train.set.uni.2[,1])) ; x.train.1.2 <- as.matrix(train.set.uni.2[,-1])
y.val.1.2   <- as.integer(unlist(val.set.uni.2[,1]))   ; x.val.1.2   <- as.matrix(val.set.uni.2[,-1]);  
y.train.1.2 <- to_categorical(y.train.1.2)             ; y.val.1.2   <- to_categorical(y.val.1.2)

y.train.1.3 <- as.integer(unlist(train.set.uni.3[,1])) ; x.train.1.3 <- as.matrix(train.set.uni.3[,-1])
y.val.1.3   <- as.integer(unlist(val.set.uni.3[,1]))   ; x.val.1.3   <- as.matrix(val.set.uni.3[,-1]);  
y.train.1.3 <- to_categorical(y.train.1.3)             ; y.val.1.3   <- to_categorical(y.val.1.3)

y.train.1.4 <- as.integer(unlist(train.set.uni.4[,1])) ; x.train.1.4 <- as.matrix(train.set.uni.4[,-1])
y.val.1.4   <- as.integer(unlist(val.set.uni.4[,1]))   ; x.val.1.4   <- as.matrix(val.set.uni.4[,-1]);  
y.train.1.4 <- to_categorical(y.train.1.4)             ; y.val.1.4   <- to_categorical(y.val.1.4)

y.train.1.all <- as.integer(unlist(train.set.uni.all[,1])) ; x.train.1.all <- as.matrix(train.set.uni.all[,-1])
y.val.1.all   <- as.integer(unlist(val.set.uni.all[,1]))   ; x.val.1.all   <- as.matrix(val.set.uni.all[,-1]);  
y.train.1.all <- to_categorical(y.train.1.all)             ; y.val.1.all   <- to_categorical(y.val.1.all)

# Character
y.train.2.1 <- as.integer(unlist(train.set.char.1[,1])) ; x.train.2.1 <- as.matrix(train.set.char.1[,-1])
y.val.2.1   <- as.integer(unlist(val.set.char.1[,1]))   ; x.val.2.1   <- as.matrix(val.set.char.1[,-1]);  
y.train.2.1 <- to_categorical(y.train.2.1)             ; y.val.2.1   <- to_categorical(y.val.2.1)

y.train.2.2 <- as.integer(unlist(train.set.char.2[,1])) ; x.train.2.2 <- as.matrix(train.set.char.2[,-1])
y.val.2.2   <- as.integer(unlist(val.set.char.2[,1]))   ; x.val.2.2   <- as.matrix(val.set.char.2[,-1]);  
y.train.2.2 <- to_categorical(y.train.2.2)             ; y.val.2.2   <- to_categorical(y.val.2.2)

y.train.2.3 <- as.integer(unlist(train.set.char.3[,1])) ; x.train.2.3 <- as.matrix(train.set.char.3[,-1])
y.val.2.3   <- as.integer(unlist(val.set.char.3[,1]))   ; x.val.2.3   <- as.matrix(val.set.char.3[,-1]);  
y.train.2.3 <- to_categorical(y.train.2.3)             ; y.val.2.3   <- to_categorical(y.val.2.3)

y.train.2.4 <- as.integer(unlist(train.set.char.4[,1])) ; x.train.2.4 <- as.matrix(train.set.char.4[,-1])
y.val.2.4   <- as.integer(unlist(val.set.char.4[,1]))   ; x.val.2.4   <- as.matrix(val.set.char.4[,-1]);  
y.train.2.4 <- to_categorical(y.train.2.4)             ; y.val.2.4   <- to_categorical(y.val.2.4)

y.train.2.all <- as.integer(unlist(train.set.char.all[,1])) ; x.train.2.all <- as.matrix(train.set.char.all[,-1])
y.val.2.all   <- as.integer(unlist(val.set.char.all[,1]))   ; x.val.2.all   <- as.matrix(val.set.char.all[,-1]);  
y.train.2.all <- to_categorical(y.train.2.all)             ; y.val.2.all   <- to_categorical(y.val.2.all)

y.train.tfidf <- as.integer(unlist(train.set.tfidf[,1])) ; x.train.tfidf <- as.matrix(train.set.tfidf[,-1])
y.val.tfidf   <- as.integer(unlist(val.set.tfidf[,1]))   ; x.val.tfidf   <- as.matrix(val.set.tfidf[,-1]);  
y.train.tfidf <- to_categorical(y.train.tfidf)             ; y.val.tfidf   <- to_categorical(y.val.tfidf)

```

```{r FFNN Lexical}
model.1.1 <- keras_model_sequential() %>%
   layer_dense(units = 32, activation = 'relu', input_shape = c(ncol(x.train.1.1))) %>% 
   layer_dropout(rate = 0.5) %>%
   layer_dense(units = 5, activation = 'softmax')

model.1.1 %>% compile(
  loss = "categorical_crossentropy",
  optimizer = optimizer_adam(learning_rate = 0.01),
  metrics = "accuracy"
)
model.1.2 <- keras_model_sequential() %>%
   layer_dense(units = 32, activation = 'relu', input_shape = c(ncol(x.train.1.2))) %>% 
   layer_dropout(rate = 0.5) %>%
   layer_dense(units = 5, activation = 'softmax')

model.1.2 %>% compile(
  loss = "categorical_crossentropy",
  optimizer = optimizer_adam(learning_rate = 0.01),
  metrics = "accuracy"
)

model.1.3 <- keras_model_sequential() %>%
   layer_dense(units = 32, activation = 'relu', input_shape = c(ncol(x.train.1.3))) %>% 
   layer_dropout(rate = 0.5) %>%
   layer_dense(units = 5, activation = 'softmax')

model.1.3 %>% compile(
  loss = "categorical_crossentropy",
  optimizer = optimizer_adam(learning_rate = 0.01),
  metrics = "accuracy"
)

model.1.4 <- keras_model_sequential() %>%
   layer_dense(units = 32, activation = 'relu', input_shape = c(ncol(x.train.1.4))) %>% 
   layer_dropout(rate = 0.5) %>%
   layer_dense(units = 5, activation = 'softmax')

model.1.4 %>% compile(
  loss = "categorical_crossentropy",
  optimizer = optimizer_adam(learning_rate = 0.01),
  metrics = "accuracy"
)

model.1.all <- keras_model_sequential() %>%
   layer_dense(units = 32, activation = 'relu', input_shape = c(ncol(x.train.1.all))) %>% 
   layer_dropout(rate = 0.5) %>%
   layer_dense(units = 5, activation = 'softmax')

model.1.all %>% compile(
  loss = "categorical_crossentropy",
  optimizer = optimizer_adam(learning_rate = 0.01),
  metrics = "accuracy"
)

```

```{r FFNN Character}
model.2.1 <- keras_model_sequential() %>%
   layer_dense(units = 32, activation = 'relu', input_shape = c(ncol(x.train.2.1))) %>% 
   layer_dropout(rate = 0.5) %>%
   layer_dense(units = 5, activation = 'softmax')

model.2.1 %>% compile(
  loss = "categorical_crossentropy",
  optimizer = optimizer_adam(learning_rate = 0.01),
  metrics = "accuracy"
)
model.2.2 <- keras_model_sequential() %>%
   layer_dense(units = 32, activation = 'relu', input_shape = c(ncol(x.train.2.2))) %>% 
   layer_dropout(rate = 0.5) %>%
   layer_dense(units = 5, activation = 'softmax')

model.2.2 %>% compile(
  loss = "categorical_crossentropy",
  optimizer = optimizer_adam(learning_rate = 0.01),
  metrics = "accuracy"
)

model.2.3 <- keras_model_sequential() %>%
   layer_dense(units = 32, activation = 'relu', input_shape = c(ncol(x.train.2.3))) %>% 
   layer_dropout(rate = 0.5) %>%
   layer_dense(units = 5, activation = 'softmax')

model.2.3 %>% compile(
  loss = "categorical_crossentropy",
  optimizer = optimizer_adam(learning_rate = 0.01),
  metrics = "accuracy"
)

model.2.4 <- keras_model_sequential() %>%
   layer_dense(units = 32, activation = 'relu', input_shape = c(ncol(x.train.2.4))) %>% 
   layer_dropout(rate = 0.5) %>%
   layer_dense(units = 5, activation = 'softmax')

model.2.4 %>% compile(
  loss = "categorical_crossentropy",
  optimizer = optimizer_adam(learning_rate = 0.01),
  metrics = "accuracy"
)

model.2.all <- keras_model_sequential() %>%
   layer_dense(units = 32, activation = 'relu', input_shape = c(ncol(x.train.2.all))) %>% 
   layer_dropout(rate = 0.5) %>%
   layer_dense(units = 5, activation = 'softmax')

model.2.all %>% compile(
  loss = "categorical_crossentropy",
  optimizer = optimizer_adam(learning_rate = 0.01),
  metrics = "accuracy"
)


```

```{r FFNN TFIDF}
model.tfidf <- keras_model_sequential() %>%
   layer_dense(units = 32, activation = 'relu', input_shape = c(ncol(x.train.tfidf))) %>% 
   layer_dropout(rate = 0.5) %>%
   layer_dense(units = 5, activation = 'softmax')

model.tfidf %>% compile(
  loss = "categorical_crossentropy",
  optimizer = optimizer_adam(learning_rate = 0.01),
  metrics = "accuracy"
)
```

# Results

## Validation Results

For the FFNN, in general we see that loss increase and accuracy increases, which indicates that the model is learning. The in-sample, IS, model accuracy looks to improve as the data set increases for all different types of dataset. With the model built off TFIDF holding the best IS accuracy.

```{r FFNN Lexical Fit, fig.cap=""}
history.1.1 <- model.1.1 %>% fit(
  x.train.1.1, y.train.1.1,
  epochs = 50, batch_size = 100,verbose = 0)

history.1.2 <- model.1.2 %>% fit(
  x.train.1.2, y.train.1.2,
  epochs = 50, batch_size = 100,verbose = 0)

history.1.3 <- model.1.3 %>% fit(
  x.train.1.3, y.train.1.3,
  epochs = 50, batch_size = 100,verbose = 0)

history.1.4 <- model.1.4 %>% fit(
  x.train.1.4, y.train.1.4,
  epochs = 50, batch_size = 100,verbose = 0)

history.1.all <- model.1.all %>% fit(
  x.train.1.all, y.train.1.all,
  epochs = 50, batch_size = 100,verbose = 0)

# save(history.1.1, history.1.2, history.1.3, history.1.4, history.1.all, file = "FFLexical.RData")
# 
# load("FFLexical.RData")

plot_grid(plot(history.1.1), plot(history.1.2), plot(history.1.3), plot(history.1.4), plot(history.1.all), labels = c("200", "500", "2000", "4500", "All"), label_size = 7, label_colour = "red")
```
A similar observation can be made in the below figure. 

```{r FFNN Character Fit, fig.cap=""}
history.2.1 <- model.2.1 %>% fit(
  x.train.2.1, y.train.2.1,
  epochs = 50, batch_size = 100,verbose = 0)

history.2.2 <- model.2.2 %>% fit(
  x.train.2.2, y.train.2.2,
  epochs = 50, batch_size = 100,verbose = 0)

history.2.3 <- model.2.3 %>% fit(
  x.train.2.3, y.train.2.3,
  epochs = 50, batch_size = 100,verbose = 0)

history.2.4 <- model.2.4 %>% fit(
  x.train.2.4, y.train.2.4,
  epochs = 50, batch_size = 100,verbose = 0)

history.2.all <- model.2.all %>% fit(
  x.train.2.all, y.train.2.all,
  epochs = 50, batch_size = 100,verbose = 0)

# save(history.2.1, history.2.2, history.2.3, history.2.4, history.2.all, file = "FFChar.RData")
# 
# load("FFChar.RData")

plot_grid(plot(history.2.1), plot(history.2.2), plot(history.2.3), plot(history.2.4), plot(history.2.all), labels = c("200", "500", "2000", "4500", "All"), label_size = 7, label_colour = "red")
```
This figure as mentioned above seems to have the best IS accuracy.

```{r FFNN TFIDF Fit, fig.cap="", out.width="50%"}
history.tfidf <- model.tfidf %>% fit(
  x.train.tfidf, y.train.tfidf,
  epochs = 50, batch_size = 100,verbose = 0)

# save(history.tfidf, file = "FFtfidf.RData")
# 
# load("FFTFIDF.RData")

plot(history.tfidf)
```
Table 1 depicts the results of our analysis of different features over differently sized word bags. It is an analysis, that is capable of depicting how many words are necessary for our models and which models we can look to take forward with which data. We see that for both the naive Bayes classifier and the SVM the missclassification is lowest when using character features at a bag size of 4500 features. The FFNN that the best classification comes when tfidf is considered above all else. In order to find the model that performs best, both these two well performing feature sets will undergo hyperparameter tuning for NB, SVM and FFNN.

```{r Validation Results}
# Lexical naive bayes
nL <- c(1 - sum(diag(t.1.1))/ sum(t.1.1),
       1 - sum(diag(t.2.1))/ sum(t.2.1),
       1 - sum(diag(t.3.1))/ sum(t.3.1),
       1 - sum(diag(t.4.1))/ sum(t.4.1),
       1 - sum(diag(t.all.1))/ sum(t.all.1))

# Character Naive Bayes
nC <- c(1 - sum(diag(t.1.2))/ sum(t.1.2),
        1 - sum(diag(t.2.2))/ sum(t.2.2),
        1 - sum(diag(t.3.2))/ sum(t.3.2),
        1 - sum(diag(t.4.2))/ sum(t.4.2),
        1 - sum(diag(t.all.2))/ sum(t.all.2))

# TFIDF naive bayes
nT <- c(NA,NA, NA, NA,1 - sum(diag(tfidf.t))/ sum(tfidf.t))

# Lexical SVM
sL <- c(1 - sum(diag(svm.t.1.1))/ sum(svm.t.1.1),
        1 - sum(diag(svm.t.2.1))/ sum(svm.t.2.1),
        1 - sum(diag(svm.t.3.1))/ sum(svm.t.3.1),
        1 - sum(diag(svm.t.4.1))/ sum(svm.t.4.1),
        1 - sum(diag(svm.t.all.1))/ sum(svm.t.all.1))

# Character SVM
sC <- c(1 - sum(diag(svm.t.1.2))/ sum(svm.t.1.2),
        1 - sum(diag(svm.t.2.2))/ sum(svm.t.2.2),
        1 - sum(diag(svm.t.3.2))/ sum(svm.t.3.2),
        1 - sum(diag(svm.t.4.2))/ sum(svm.t.4.2),
        1 - sum(diag(svm.t.all.2))/ sum(svm.t.all.2))

# TFIDF SVM
sT <- c(NA,NA,NA,NA,1 - sum(diag(svm.tfidf.t))/ sum(svm.tfidf.t))

# Lexical FFNN
fL <- c(1 - (model.1.1 %>% evaluate(x.val.1.1, y.val.1.1, batch_size=100, verbose = 0))[2],
        1 - (model.1.2 %>% evaluate(x.val.1.2, y.val.1.2, batch_size=100, verbose = 0))[2],
        1 - (model.1.3 %>% evaluate(x.val.1.3, y.val.1.3, batch_size=100, verbose = 0))[2],
        1 - (model.1.4 %>% evaluate(x.val.1.4, y.val.1.4, batch_size=100, verbose = 0))[2],
        1 - (model.1.all %>% evaluate(x.val.1.all, y.val.1.all, batch_size=100, verbose = 2))[2])

# Character FFNN
fC <- c(1 - (model.2.1 %>% evaluate(x.val.2.1, y.val.2.1, batch_size=100, verbose = 0))[2],
        1 - (model.2.2 %>% evaluate(x.val.2.2, y.val.2.2, batch_size=100, verbose = 0))[2],
        1 - (model.2.3 %>% evaluate(x.val.2.3, y.val.2.3, batch_size=100, verbose = 0))[2],
        1 - (model.2.4 %>% evaluate(x.val.2.4, y.val.2.4, batch_size=100, verbose = 0))[2],
        1 - (model.2.all %>% evaluate(x.val.2.all, y.val.2.all, batch_size=100, verbose = 0))[2])

# TFIDF FFNN
  # Best model is TFIDF with characters
fT <- c(NA,NA,NA,NA, 1 - (model.tfidf %>% evaluate(x.val.tfidf, y.val.tfidf, batch_size=100, verbose = 0))[2])

validatedResults <- data.frame(Count = c(200, 500, 2000, 4500, "All"), Lex.n = nL, Char.n = nC, TFIDF.n = nT, Lex.s = sL, Char.s = sC, TFIDF.s = sT, Lex.f = fL, Char.f = fC, TFIDF.f = fT)

#save(validatedResults, file = "validatedResults.RData")

as_hux(validatedResults) %>% 
  insert_row("","NB", "", "", "SVM", "", "", "FFNN", fill = "") %>% 
  merge_cells(1, 2:4) %>% 
  merge_cells(1, 5:7) %>%
  merge_cells(1, 8:10) %>%
  set_header_rows(1:2, TRUE) %>% 
  set_header_cols(1, TRUE) %>% 
  style_headers(bold = TRUE) %>%
  set_all_borders()%>% set_caption("Table 1: Validation results depicting performance of differently specified data in each model for different word bags and TFIDF")
```
## Hyperparameter Tuning Results

```{r Laplace Smoothing}
n_hyper <- 0:4
tune.n.1 <- list()
tune.n.2 <- list()
for (i in 1:5) {
  char.classifier <- naiveBayes(train.set.char.4[,-1], train.set.char.4[,1], laplace = n_hyper[i])
  char.p          <- predict(char.classifier, val.set.char.4[,-1])
  char.t          <- table((char.p), (unlist(val.set.char.4[,1]))) 
  tune.n.1[[i]]   <- 1 - sum(diag(char.t))/ sum(char.t)
  
  classifier      <- naiveBayes(train.set.tfidf[,-1], train.set.tfidf[,1], laplace = n_hyper[i])
  p               <- predict(classifier, val.set.tfidf[,-1])
  t               <- table((p), (unlist(val.set.tfidf[,1])))
  tune.n.2[[i]]   <- 1 - sum(diag(t))/ sum(t)
}


```

```{r Cost Complexity}
svm.classifier.4.2       <- svm(train.set.char.4[,-1], train.set.char.4[,1], type = "C-classification", scale = T)
svm.p.4.2                <- predict(svm.classifier.4.2, val.set.char.4[,-1])
svm.t.4.2                <- table((svm.p.4.2), (unlist(val.set.char.4[,1])))

# svm.tfidf.classifier  <- svm(train.set.tfidf[,-1], train.set.tfidf[,1], type = "C-classification", scale = F)
# svm.tfidf.p           <- predict(svm.tfidf.classifier, val.set.tfidf[,-1])
# svm.tfidf.t           <- table((svm.tfidf.p), (unlist(val.set.tfidf[,1])))
```

```{r}
# https://cran.r-project.org/web/packages/kerastuneR/vignettes/Introduction.html
build_model = function(hp) {
  model = keras_model_sequential()
  model %>% layer_dense(units=hp$Int('units',
                                      min_value=32,
                                      max_value=512,
                                      step=32),
                                      input_shape = ncol(x.train.1.4),
                                      activation='relu') %>% 
  layer_dense(units=5, activation='softmax') %>% 
  compile(
    optimizer= tf$keras$optimizers$Adam(
      hp$Choice('learning_rate',
                values=c(1e-2, 1e-3, 1e-4))),
    loss='binary_crossentropy',
    metrics='accuracy') 
  return(model)
}
```


```{r FFNN Hyperparameter tuning}
model.2.4 <- keras_model_sequential() %>%
   layer_dense(units = 32, activation = 'relu', input_shape = c(ncol(x.train.2.4))) %>% 
   layer_dropout(rate = 0.5) %>%
   layer_dense(units = 5, activation = 'softmax')

model.2.4 %>% compile(
  loss = "categorical_crossentropy",
  optimizer = optimizer_adam(learning_rate = 0.01),
  metrics = "accuracy"
)

model.tfidf <- keras_model_sequential() %>%
   layer_dense(units = 32, activation = 'relu', input_shape = c(ncol(x.train.tfidf))) %>% 
   layer_dropout(rate = 0.5) %>%
   layer_dense(units = 5, activation = 'softmax')

model.tfidf %>% compile(
  loss = "categorical_crossentropy",
  optimizer = optimizer_adam(learning_rate = 0.01),
  metrics = "accuracy"
)
```


```{r FFNN Hyperparameter tuning}
tf_tuner <-  import("keras")

tuner <- kerastuneR::Hyperband(
    build_model,
    objective = 'accuracy',
    max_trials = 5,
    executions_per_trial = 3,
    project_name = 'helloworld')
```



<!-- Perform hyperparameter tuning for svm and ffnn on final model -->

## Final Model

<!-- Use test set on final models -->

# Discussion

# Limitations

A limitation that arises in this study is a lack of candidate authors, as well as minimal data provided for some authors. This is an issue that has been detailed in research before ([\@Stylometry]()).

Perform a recuesive feature elimination (@WordFreq) using the rfe function in the package provided by, @Caret .

# Conclusion

# Bibliography

<!-- It will fill up automatically -->
