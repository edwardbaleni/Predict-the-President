---
title: "Supplementary"
author: "Edward Baleni"
format: html
editor: visual
bibliography: references.bib
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE, fig.align="center", out.width = "65%", fig.pos = "H")
```

```{r Packages}
require(dplyr)
require(stringr)
require(tidytext)
require(ggplot2)
require(tidyr)
require(forcats)
require(textstem)
require(qdap)

library(keras) 
library(reticulate) 
library(tensorflow)
```

```{r Data}
set.seed(2023)
sys.source("sona-first-steps.R" , envir = knitr::knit_global())
sona <- as_tibble(sona)
sona$president <- as.factor(sona$president)
```

```{r Cleaning Data}
sentences <- sona %>%
  unnest_sentences(speech, speech) #%>%
  #mutate(ids = row_number())

clean <- function(x){
  # remove numbers
  x <- gsub("\\d+", "", x)
  # remove contractions
  x <- gsub("/'s", "", x)
  # remove special characters
  x <- gsub("[^[:alnum:] ]", "", x)
  # lemmatization
  x <- lemmatize_strings(x)
  # remove single characters
  x <- gsub("\\s.\\s", " ", x)
}

sentences$speech <- unlist(lapply(sentences$speech, clean))
```

```{r Remove first sentence}
# Need to remove first sentence for each president, to analyse count well
# Since first sentence is similar for each president
hold <- c()
for (i in unique(sentences$filename)){
  hold <- c(hold, which( sentences$filename %in% i)[1])
}
data <- sentences[-hold,]
sentences <- data
```

# Data Handling Options

## Equally represented training sets

There are various ways in which one might go about selecting the training and test split for the models. One option considered was in having equal representation of each president in the training split.

The idea here is that each president should be represented equally in their training and validation. It is not necessary to concern oneself with the equality of representation in the test set. For this reason, it would make sense to extract a sample of the same size for each president, regardless of how many speeches they've done.

There are two approaches examined here. The first being undersampling, the following being oversampling. Undersampling, is removing data at random from classes that are over-represented until the classes are all balanced. The issue with such a method is that important information may in fact be lost in the process. Oversampling on the other hand will result in the overestimation of the underrepresented classes.

([\@Imbalance]())

Whether we use an undersampling approach or an oversampling approach the idea is that we have a balanced dataset, that is capable of equally representing each class.

### Undersampling Approach

Undersampling has been done by taking a random sample without replacement from each president. Each speech was tokenized into sentences. Since deKlerk had a total of 92 sentences, each training class had to be somewhere below this threshold. 70 training sentences were considered as this represented 80% of deKlerks total number of sentences. 70 sentences were therefore collected from the each presidents' domain. With the understanding that presidents like Mandela, Mbeki and Zuma all had over 1500 sentences to contribute, this undersampling may not be beneficial at all in that it ultimately may lose way too much information.

```{r Undersample Split}
# Need to keep some sentences separate.

table(sentences$president)

# Let's say each president has 70 sentences (that's close to 1000 words each.)
# for each president, I would advise removing their first sentence as it appears in all statements in some way.

under <- sentences %>% 
  group_by(president) %>%
  slice_sample(n = 70) %>% 
  ungroup() %>%
  rename( token = speech )

# Here we have an undersample
table(under$president)
```

Via an exploration of tokenization by words, we see very small frequencies even for words that are important to a document. This level of improtance has been calculated using tf-idf (term frequency - inverse document frequency). The only president who benefits from this undersampling is deKlerk. This can be seen in how they continue to maintain higher term counts for words commonly used in their speech. The majority of presidents suffer from this undersampling as they have counts mostly between 0 and 6, for commonly used words in their speeches.

```{r Exploration of Under-Split}
# Unigram
uni <- under %>%
  rename( speech = token) %>%
  unnest_tokens(token, speech) %>%
  #filter(!token %in% stop_words$word) %>%
  count(president, token)

uni %>%
  group_by(president) %>%
  summarise(total = sum(n))

uni_tf_idf <- uni %>%
  bind_tf_idf(token, president, n)

uni_tf_idf %>% 
  group_by(president) %>% 
  slice_max(tf_idf, n = 5) %>% 
  ungroup() %>%
  mutate(token = reorder(token, tf_idf, decreasing = F)) %>%
  ggplot(aes(tf_idf, token, fill = president)) +
  geom_col(show.legend = FALSE) +
  labs(x = "tf-idf", y = NULL) +
  facet_wrap(~president, ncol = 2, scales = "free")

uni_tf_idf %>%
  group_by(president) %>%
  slice_max(tf_idf, n= 5)%>%
  ungroup() %>%
  ggplot(aes(tf, fct_reorder(token, n), fill = president)) +
  geom_col(show.legend = FALSE) +
  facet_wrap(~president, ncol = 2, scales = "free") +
  labs(x = "Term Frequency", y = NULL)

uni_tf_idf %>%
  group_by(president) %>%
  slice_max(tf_idf, n= 5)%>%
  ungroup() %>%
  ggplot(aes(n, fct_reorder(token, n), fill = president)) +
  geom_col(show.legend = FALSE) +
  facet_wrap(~president, ncol = 2, scales = "free") +
  labs(x = "Count", y = NULL)
```

### Oversampling Approach

<!-- use bootstrapping of sample of 70 sentences -->

Another option is to look into oversampling ([\@Imbalance]()). [\@Imbalance]() has suggested that oversampling may in fact be the correct way to go about correctly representing a class that is not well represented in a dataset, and that it could generally yield better results than undersampling.

Let's train the data on a sample of 1200 sentences for each president and see how it fairs. This would mean having to bootstrap for presidents like deKlerk and Mothlanthe. However, this should be done with the understanding that these two characters may have overestimated validation results as there could be instance of training examples in the validation.

Howeveer, one needs to understand that with oversample, it is necesssary that we ensure that the samples that are resampled are not repeated in the training, validation and testing samples.

However, we need these to be together to obtain statistics like tf-idf, frequency and frequency count.

```{r Oversample Split}
table(sentences$president)

over1 <- sentences %>%
  filter(!president %in% c("deKlerk", "Motlanthe")) %>%
  group_by(president) %>%
  sample_n(size = 1200) %>%
  ungroup()

over2 <- as.data.frame(sentences) %>%
  filter(president %in% c("deKlerk", "Motlanthe")) %>%
  group_by(president) %>%
  slice_sample( prop = 0.6 ) %>%
  sample_n(size = 1200, replace = T) %>%
  ungroup()

over.train <- bind_rows(over1, over2) %>%
  mutate(train = T)

over.test <- sentences %>%
  anti_join(over.train) %>%
  mutate(train = F)

over <- bind_rows(over.train, over.test) %>%
  mutate(ids = row_number())

table(over$president)
```

In the short exploration of the training samples below it would seem that oversampling is indeed a better strategy than undersampling. We can attempt to build off this. We may even be able to look into bigram and trigrams for this sized dataset. However, it should be understood that bigrams and trigrams would be grossly disproportianate under this bootstrapping as both deKlerk and Motlanthe will have a lot more common phrases than any other president, not because they are trying to get a point across but because some of their sentences are resampled.

```{r Exploration of Over-Split}
# Unigram
uni <- over %>%
  unnest_tokens(token, speech) %>%
  filter(!token %in% stop_words$word) %>%
  count(president, token)

uni %>%
  group_by(president) %>%
  summarise(total = sum(n))

uni_tf_idf <- uni %>%
  bind_tf_idf(token, president, n) %>%
  mutate( ids = row_number())

# Plots
uni_tf_idf %>% 
  group_by(president) %>% 
  slice_max(n, n = 15) %>% 
  ungroup() %>%
  mutate(token = reorder(token, n, decreasing = F)) %>%
  ggplot(aes(n, token, fill = president)) +
  geom_col(show.legend = FALSE) +
  labs(x = "tf-idf", y = NULL) +
  facet_wrap(~president, ncol = 2, scales = "free")

uni_tf_idf %>%
  group_by(president) %>%
  slice_max(tf_idf, n= 15)%>%
  ungroup() %>%
  ggplot(aes(tf_idf, fct_reorder(token, tf_idf), fill = president)) +
  geom_col(show.legend = FALSE) +
  facet_wrap(~president, ncol = 2, scales = "free") +
  labs(x = "Count", y = NULL)
```

```{r}
# Bigram
bigram <- over.train %>% 
  unnest_tokens(token, speech, token = 'ngrams', n = 2) %>%
  count(president, token)

bigram_tf_idf <- bigram %>%
  bind_tf_idf(token, president, n)

bigram %>%
  group_by(president) %>%
  summarise(total = sum(n))

bigram_tf_idf %>% 
  group_by(president) %>% 
  slice_max(tf_idf, n = 6) %>% 
  ungroup() %>%
  mutate(token = reorder(token, tf_idf, decreasing = F)) %>%
  ggplot(aes(tf_idf, token, fill = president)) +
  geom_col(show.legend = FALSE) +
  labs(x = "tf-idf", y = NULL) +
  facet_wrap(~president, ncol = 2, scales = "free")

bigram_tf_idf %>%
  group_by(president) %>%
  slice_max(tf_idf, n= 15)%>%
  ungroup() %>%
  ggplot(aes(n, fct_reorder(token, n), fill = president)) +
  geom_col(show.legend = FALSE) +
  facet_wrap(~president, ncol = 2, scales = "free") +
  labs(x = "Count", y = NULL)
```

Looking below, it would seem that anything more than a bigram for this oversampled training set would not be in our favour. Characters like deKlerk and Motlanthe have many resampled observations, this would indeed skew their repeated observations for trigrams and beyond tremendously, which could ultimately result in some bias in their direction when modelling or may increase their overestimation. For this reason, it is probably preferred to look into tokenization for individual words as this would be the least influential in terms of over-estimation and bias in-sample.

```{r}
# Trigram
trigram <- over.train %>% 
  unnest_tokens(token, speech, token = 'ngrams', n = 3) %>%
  count(president, token)


tri_tf_idf <- trigram %>%
  bind_tf_idf(token, president, n) %>%
  mutate(president = factor(president, levels = c("deKlerk", "Mandela", "Mbeki", "Motlanthe", "Ramaphosa", "Zuma"))) %>%
  arrange(desc(tf_idf))

tri_tf_idf %>%
  group_by(president) %>%
  slice_max(tf_idf, n= 10)%>%
  ungroup() %>%
  ggplot(aes(tf_idf, fct_reorder(token, tf_idf), fill = president)) +
  geom_col(show.legend = FALSE) +
  facet_wrap(~president, ncol = 2, scales = "free") +
  labs(x = "tf-idf", y = NULL)

tri_tf_idf %>%
  group_by(president) %>%
  slice_max(tf_idf, n= 10)%>%
  ungroup() %>%
  ggplot(aes(n, fct_reorder(token, n), fill = president)) +
  geom_col(show.legend = FALSE) +
  facet_wrap(~president, ncol = 2, scales = "free") +
  labs(x = "Count", y = NULL)
```
