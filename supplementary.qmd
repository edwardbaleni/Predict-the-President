---
title: "Supplementary"
author: "Edward Baleni"
format: html
editor: visual
bibliography: references.bib
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE, fig.align="center", out.width = "65%", fig.pos = "H")
```

```{r Packages}
require(dplyr)
require(stringr)
require(tidytext)
require(ggplot2)
require(tidyr)
require(forcats)
require(textstem)
require(qdap)

library(e1071)

library(keras) 
library(reticulate) 
library(tensorflow)

#set.seed(321)
```

```{r Data}
set.seed(2023)
sys.source("sona-first-steps.R" , envir = knitr::knit_global())
sona <- as_tibble(sona)
#sona$president <- as.factor(sona$president)
```

```{r Cleaning Data}
sentences <- sona %>%
  unnest_sentences(speech, speech) #%>%
  #mutate(ids = row_number())

clean <- function(x){
  # remove numbers
  x <- gsub("\\d+", "", x)
  # remove contractions
  x <- gsub("/'s", "", x)
  # remove special characters
  x <- gsub("[^[:alnum:] ]", "", x)
  # lemmatization
  x <- lemmatize_strings(x)
  # remove single characters
  x <- gsub("\\s.\\s", " ", x)
}

sentences$speech <- unlist(lapply(sentences$speech, clean))
```

# Supplementary

This supplementary serves as grounds on which most of the final decisions were based. It includes finding preferred stylometic features and the handling of imbalanced classes.

## Stylometric Features

The first task is to select the stylometric features. This is deciding how one may choose to represent text. This can either be done by tokenizing by words (lexical features), a number of characters (character features), or in other more sophisticated ways like syntatic features or semantic features. In this study, only lexical features and character features are explored, while the other two require more domain knowledge to properly quantify ([\@Stylometry]()). 

The goal of this assignment is that of author attribution, we are essentially trying to decide which president said what sentence. Although the some of the points addressed within the speech are slightly different, the main topic is that of a political address, therefore it may be worthwhile to view this research as a style-based text classification rather than a topic-based text classification. This will have significant effect on the study and the effect will be demonstrated here.

One of the issues that still need to be addressed is the handling of imbalanced classes, at this stage, the imbalanced classes will be removed and will be looked at again later on.

Another point to address is the useage of token count or TF-IDF weightings, as observations for the study. This initial exploration will be done with frequency, as this is a very stable way in conducting author attribution ([\@WordFreq]()). 

For each experiment, a Naive Bayes classifier and a Support Vector Machine will be used to inform decisions.

When looking into the count data, it sometimes does help to select a the most frequent words for the analysis. This will be examined later, at this stage, 150

### Lexical Features

```{r Remove Imabalances}
data <- sentences %>%
  filter(!president %in% c("deKlerk", "Motlanthe"))
data$president <- as.factor(data$president)

data_count <- data

data_count <- data_count %>%
  mutate(ids_ = row_number())
```

When it comes to lexical features, these can be classed into N-grams. A collection of N contiguous words. The greater N is, the more complex the representation of the feature. It often doesn't show better results and the greater the N the higher the dimensionality of the problem, which will ultimately increase sparsity in the dataset, making it more difficult to classify ([\@Ngrams](), [\@WordFreq]()). To that extent, only unigrams and bigrams will be explored. 

```{r Unigram}
uni <- data_count %>%
  unnest_tokens(token, speech, token = "words") %>%
  filter(!token %in% stop_words$word)
  
word.bag.uni <- uni %>%
  group_by(token) %>%
  count() %>%
  ungroup() %>%
  top_n(200, wt = n) %>%
  select(-n)

uni.tdf <- uni %>%
  inner_join(word.bag.uni) %>%
  group_by(ids_,token) %>%
  count() %>%  
  group_by(ids_) %>%
  mutate(total = sum(n)) %>%
  ungroup()


bag.of.words.uni <- uni.tdf %>% 
  select(ids_, token, n) %>% 
  pivot_wider(names_from = token, values_from = n, values_fill = 0) 


bag.of.words.uni <- data_count %>%
  select(ids_, president) %>%
  rename (Response_President = president) %>%
  right_join(bag.of.words.uni, join_by("ids_")) %>%
  select(ids_, Response_President, everything()) 
```


```{r Bigram}
bi <- data_count %>%
  unnest_tokens(token, speech, token = "ngrams", n = 2) %>%
  filter(!token %in% stop_words$word)
  
word.bag.bi <- bi %>%
  group_by(token) %>%
  count() %>%
  ungroup() %>%
  top_n(200, wt = n) %>%
  select(-n)

bi.tdf <- bi %>%
  inner_join(word.bag.bi) %>%
  group_by(ids_,token) %>%
  count() %>%  
  group_by(ids_) %>%
  mutate(total = sum(n)) %>%
  ungroup()


bag.of.words.bi <- bi.tdf %>% 
  select(ids_, token, n) %>% 
  pivot_wider(names_from = token, values_from = n, values_fill = 0) 


bag.of.words.bi <- data_count %>%
  select(ids_, president) %>%
  rename (Response_President = president) %>%
  right_join(bag.of.words.bi, join_by("ids_")) %>%
  select(ids_, Response_President, everything()) 
```

For the training, validation and test splits, it would be wise to select sample sizes by sentence and not by word, this will represent each row in the bag of words.
```{r Splits}
id           <- unique(data_count$ids_)
N            <- length(id) 

# Training set
train <- sample(id, size = N * 0.6)

# Validation set
not_train    <- na.omit(ifelse(!id %in% train, id, NA))
NN           <- length(not_train)
val          <- sample(not_train, size = NN * 0.5 )

# Test set
test         <- na.omit(ifelse(!not_train %in% val, not_train, NA))

length(test) + length(val) + length(train)
```


```{r Unigram Test}
train.set    <- bag.of.words.uni[bag.of.words.uni$ids_ %in% train, -1]
val.set      <- bag.of.words.uni[bag.of.words.uni$ids_ %in% val, -1]
test.set     <- bag.of.words.uni[bag.of.words.uni$ids_ %in% test, -1]

# Bayes classifier
data_classifier <- naiveBayes(train.set[,-1], train.set[,1])
p1 <- predict(data_classifier, val.set[,-1])
tt <- table((p1), (unlist(val.set[,1])))
1 - sum(diag(tt))/ sum(tt)

# SVM
classifier = svm(train.set[,-1], train.set[,1], type = "C-classification", scale = T)
p1 <- predict(classifier, val.set[,-1])
tt <- table((p1), (unlist(val.set[,1])))
1 - sum(diag(tt))/ sum(tt)
```

```{r Bigram Test}
train.set    <- bag.of.words.bi[bag.of.words.bi$ids_ %in% train, -1]
val.set      <- bag.of.words.bi[bag.of.words.bi$ids_ %in% val, -1]
test.set     <- bag.of.words.bi[bag.of.words.bi$ids_ %in% test, -1]


# Bayes Classifier
data_classifier <- naiveBayes(train.set[,-1], train.set[,1])
p1 <- predict(data_classifier, val.set[,-1])
tt <- table((p1), (unlist(val.set[,1])))
1 - sum(diag(tt))/ sum(tt)

# SVM
classifier = svm(train.set[,-1], train.set[,1], type = "C-classification", scale = T)
p1 <- predict(classifier, val.set[,-1])
tt <- table((p1), (unlist(val.set[,1])))
1 - sum(diag(tt))/ sum(tt)
```

What we see above is that for a word bag of 200 most frequent content words that the bigram does indeed perform worse. Content words are the opposite of functional words, where functional words are used to very abundandtly in style-based classification. These functional words are often removed in topic-based classification ([\@Stylometry]()) (there are more papers)..

Although the missclassification error here is already quite poor, we can use this as a platform on which to launch the rest of the analysis. It is essential that we observe whether allowing functional words will improve the statement made earlier, that this is more directed towards style-based classification.

It should also be noted that the unigram performining better than the bigram could be due to the size of the word bag.

First let's look into whether using functional words will be useful to the analysis.



```{r Unigram}
data_count <- data

data_count <- data_count %>%
  mutate(ids_ = row_number())

uni <- data_count %>%
  unnest_tokens(token, speech, token = "words")
  
word.bag.uni <- uni %>%
  group_by(token) %>%
  count() %>%
  ungroup() %>%
  top_n(200, wt = n) %>%
  select(-n)

uni.tdf <- uni %>%
  inner_join(word.bag.uni) %>%
  group_by(ids_,token) %>%
  count() %>%  
  group_by(ids_) %>%
  mutate(total = sum(n)) %>%
  ungroup()


bag.of.words.uni <- uni.tdf %>% 
  select(ids_, token, n) %>% 
  pivot_wider(names_from = token, values_from = n, values_fill = 0) 


bag.of.words.uni <- data_count %>%
  select(ids_, president) %>%
  rename (Response_President = president) %>%
  right_join(bag.of.words.uni, join_by("ids_")) %>%
  select(ids_, Response_President, everything()) 
```


```{r Bigram}
bi <- data_count %>%
  unnest_tokens(token, speech, token = "ngrams", n = 2)
  
word.bag.bi <- bi %>%
  group_by(token) %>%
  count() %>%
  ungroup() %>%
  top_n(200, wt = n) %>%
  select(-n)

bi.tdf <- bi %>%
  inner_join(word.bag.bi) %>%
  group_by(ids_,token) %>%
  count() %>%  
  group_by(ids_) %>%
  mutate(total = sum(n)) %>%
  ungroup()


bag.of.words.bi <- bi.tdf %>% 
  select(ids_, token, n) %>% 
  pivot_wider(names_from = token, values_from = n, values_fill = 0) 


bag.of.words.bi <- data_count %>%
  select(ids_, president) %>%
  rename (Response_President = president) %>%
  right_join(bag.of.words.bi, join_by("ids_")) %>%
  select(ids_, Response_President, everything()) 
```


```{r Unigram Test}
train.set    <- bag.of.words.uni[bag.of.words.uni$ids_ %in% train, -1]
val.set      <- bag.of.words.uni[bag.of.words.uni$ids_ %in% val, -1]
test.set     <- bag.of.words.uni[bag.of.words.uni$ids_ %in% test, -1]

# Bayes classifier
data_classifier <- naiveBayes(train.set[,-1], train.set[,1])
p1 <- predict(data_classifier, val.set[,-1])
tt <- table((p1), (unlist(val.set[,1])))
1 - sum(diag(tt))/ sum(tt)

# SVM
classifier = svm(train.set[,-1], train.set[,1], type = "C-classification", scale = T)
p1 <- predict(classifier, val.set[,-1])
tt <- table((p1), (unlist(val.set[,1])))
1 - sum(diag(tt))/ sum(tt)
```

```{r Bigram Test}
train.set    <- bag.of.words.bi[bag.of.words.bi$ids_ %in% train, -1]
val.set      <- bag.of.words.bi[bag.of.words.bi$ids_ %in% val, -1]
test.set     <- bag.of.words.bi[bag.of.words.bi$ids_ %in% test, -1]


# Bayes Classifier
data_classifier <- naiveBayes(train.set[,-1], train.set[,1])
p1 <- predict(data_classifier, val.set[,-1])
tt <- table((p1), (unlist(val.set[,1])))
1 - sum(diag(tt))/ sum(tt)

# SVM
classifier = svm(train.set[,-1], train.set[,1], type = "C-classification", scale = T)
p1 <- predict(classifier, val.set[,-1])
tt <- table((p1), (unlist(val.set[,1])))
1 - sum(diag(tt))/ sum(tt)
```

Here we see that this is indeed more of style-based classification than a topic-based classification, this is shown in how the missclassification in general does decrease in both cases of of the bigram and unigram. As mentioned above, this is so as each of the SONA speeches, although done by different presidents, in general speaks on very similar topics and ideas, therefore, their styles are more important than their topics.

In this second set of classifications we see that the unigram actually outperforms the bigram, whilst also having a missclassification that is lower than when stop words were removed.

At this stage, I would suggest using unigrams and not using stop words for the analysis.

However, since this is now a style-based classification task, using character features has in previous works been shown to capture style, lexical information and some contextual informatial. It is also able use punctuation and capitalization to infer stylistic approaches.




```{r 3 Characters}
uni <- data_count %>%
  unnest_character_shingles(token, speech, n = 3) 
  
word.bag.uni <- uni %>%
  group_by(token) %>%
  count() %>%
  ungroup() %>%
  top_n(200, wt = n) %>%
  select(-n)

uni.tdf <- uni %>%
  inner_join(word.bag.uni) %>%
  group_by(ids_,token) %>%
  count() %>%  
  group_by(ids_) %>%
  mutate(total = sum(n)) %>%
  ungroup()


bag.of.words.uni <- uni.tdf %>% 
  select(ids_, token, n) %>% 
  pivot_wider(names_from = token, values_from = n, values_fill = 0) 


bag.of.words.uni <- data_count %>%
  select(ids_, president) %>%
  rename (Response_President = president) %>%
  right_join(bag.of.words.uni, join_by("ids_")) %>%
  select(ids_, Response_President, everything()) 
```


```{r 4 Characters}
bi <- data_count %>%
  unnest_character_shingles(token, speech, n = 4) 
  
word.bag.bi <- bi %>%
  group_by(token) %>%
  count() %>%
  ungroup() %>%
  top_n(200, wt = n) %>%
  select(-n)

bi.tdf <- bi %>%
  inner_join(word.bag.bi) %>%
  group_by(ids_,token) %>%
  count() %>%  
  group_by(ids_) %>%
  mutate(total = sum(n)) %>%
  ungroup()


bag.of.words.bi <- bi.tdf %>% 
  select(ids_, token, n) %>% 
  pivot_wider(names_from = token, values_from = n, values_fill = 0) 


bag.of.words.bi <- data_count %>%
  select(ids_, president) %>%
  rename (Response_President = president) %>%
  right_join(bag.of.words.bi, join_by("ids_")) %>%
  select(ids_, Response_President, everything()) 
```


```{r 3 Char Test}
train.set    <- bag.of.words.uni[bag.of.words.uni$ids_ %in% train, -1]
val.set      <- bag.of.words.uni[bag.of.words.uni$ids_ %in% val, -1]
test.set     <- bag.of.words.uni[bag.of.words.uni$ids_ %in% test, -1]

# Bayes classifier
data_classifier <- naiveBayes(train.set[,-1], train.set[,1])
p1 <- predict(data_classifier, val.set[,-1])
tt <- table((p1), (unlist(val.set[,1])))
1 - sum(diag(tt))/ sum(tt)

# SVM
classifier = svm(train.set[,-1], train.set[,1], type = "C-classification", scale = T)
p1 <- predict(classifier, val.set[,-1])
tt <- table((p1), (unlist(val.set[,1])))
1 - sum(diag(tt))/ sum(tt)
```

```{r 4 Char Test}
train.set    <- bag.of.words.bi[bag.of.words.bi$ids_ %in% train, -1]
val.set      <- bag.of.words.bi[bag.of.words.bi$ids_ %in% val, -1]
test.set     <- bag.of.words.bi[bag.of.words.bi$ids_ %in% test, -1]


# Bayes Classifier
data_classifier <- naiveBayes(train.set[,-1], train.set[,1])
p1 <- predict(data_classifier, val.set[,-1])
tt <- table((p1), (unlist(val.set[,1])))
1 - sum(diag(tt))/ sum(tt)

# SVM
classifier = svm(train.set[,-1], train.set[,1], type = "C-classification", scale = T)
p1 <- predict(classifier, val.set[,-1])
tt <- table((p1), (unlist(val.set[,1])))
1 - sum(diag(tt))/ sum(tt)
```



At this stage we can see that the classification is not that much different from a unigram, however, points of which to consider, is that when using character features certain items become necessary and deserve exploration: the number of characters, the size of the word bag, the inclusion of punctuation.

The number of characters have been chosen to be either 3 or 4, which is consistent with the study ([\@Stylometry]()). The size of the bag and the inclusion of punctuation are two points which need to be further explored.




```{r 3 Characters Bigger Bag}
uni <- data_count %>%
  unnest_character_shingles(token, speech, n = 3) 
  
word.bag.uni <- uni %>%
  group_by(token) %>%
  count() %>%
  ungroup() %>%
  top_n(2500, wt = n) %>%
  select(-n)

uni.tdf <- uni %>%
  inner_join(word.bag.uni) %>%
  group_by(ids_,token) %>%
  count() %>%  
  group_by(ids_) %>%
  mutate(total = sum(n)) %>%
  ungroup()


bag.of.words.uni <- uni.tdf %>% 
  select(ids_, token, n) %>% 
  pivot_wider(names_from = token, values_from = n, values_fill = 0) 


bag.of.words.uni <- data_count %>%
  select(ids_, president) %>%
  rename (Response_President = president) %>%
  right_join(bag.of.words.uni, join_by("ids_")) %>%
  select(ids_, Response_President, everything()) 
```


```{r 4 Characters Bigger Bag}
bi <- data_count %>%
  unnest_character_shingles(token, speech, n = 4) 
  
word.bag.bi <- bi %>%
  group_by(token) %>%
  count() %>%
  ungroup() %>%
  top_n(2500, wt = n) %>%
  select(-n)

bi.tdf <- bi %>%
  inner_join(word.bag.bi) %>%
  group_by(ids_,token) %>%
  count() %>%  
  group_by(ids_) %>%
  mutate(total = sum(n)) %>%
  ungroup()


bag.of.words.bi <- bi.tdf %>% 
  select(ids_, token, n) %>% 
  pivot_wider(names_from = token, values_from = n, values_fill = 0) 


bag.of.words.bi <- data_count %>%
  select(ids_, president) %>%
  rename (Response_President = president) %>%
  right_join(bag.of.words.bi, join_by("ids_")) %>%
  select(ids_, Response_President, everything()) 
```


```{r 3 Char Test Bigger Bag}
train.set    <- bag.of.words.uni[bag.of.words.uni$ids_ %in% train, -1]
val.set      <- bag.of.words.uni[bag.of.words.uni$ids_ %in% val, -1]
test.set     <- bag.of.words.uni[bag.of.words.uni$ids_ %in% test, -1]

# Bayes classifier
data_classifier <- naiveBayes(train.set[,-1], train.set[,1])
p1 <- predict(data_classifier, val.set[,-1])
tt <- table((p1), (unlist(val.set[,1])))
1 - sum(diag(tt))/ sum(tt)

# SVM
classifier = svm(train.set[,-1], train.set[,1], type = "C-classification", scale = T)
p1 <- predict(classifier, val.set[,-1])
tt <- table((p1), (unlist(val.set[,1])))
1 - sum(diag(tt))/ sum(tt)
```

```{r 4 Char Test Bigger Bag}
train.set    <- bag.of.words.bi[bag.of.words.bi$ids_ %in% train, -1]
val.set      <- bag.of.words.bi[bag.of.words.bi$ids_ %in% val, -1]
test.set     <- bag.of.words.bi[bag.of.words.bi$ids_ %in% test, -1]


# Bayes Classifier
data_classifier <- naiveBayes(train.set[,-1], train.set[,1])
p1 <- predict(data_classifier, val.set[,-1])
tt <- table((p1), (unlist(val.set[,1])))
1 - sum(diag(tt))/ sum(tt)

# SVM
classifier = svm(train.set[,-1], train.set[,1], type = "C-classification", scale = T)
p1 <- predict(classifier, val.set[,-1])
tt <- table((p1), (unlist(val.set[,1])))
1 - sum(diag(tt))/ sum(tt)
```



The missclassification has decreased tremendously from when the word bag contained 200 words.



Now below we look at the inclusion of punctuation



```{r Exluding Punctuation}
sentences <- sona %>%
  unnest_sentences(speech, speech) #%>%
  #mutate(ids = row_number())

clean <- function(x){
  # remove numbers
  x <- gsub("\\d+", "", x)
  # lemmatization
  x <- lemmatize_strings(x)
}

sentences$speech <- unlist(lapply(sentences$speech, clean))

data <- sentences %>%
  filter(!president %in% c("deKlerk", "Motlanthe"))
data$president <- as.factor(data$president)

data_count <- data

data_count <- data_count %>%
  mutate(ids_ = row_number())
```




```{r 4 Characters Bigger Bag Punc}
bi <- data_count %>%
  unnest_character_shingles(token, speech, n = 4, strip_non_alphanum = F) 
  
word.bag.bi <- bi %>%
  group_by(token) %>%
  count() %>%
  ungroup() %>%
  top_n(4500, wt = n) %>%
  select(-n)

bi.tdf <- bi %>%
  inner_join(word.bag.bi) %>%
  group_by(ids_,token) %>%
  count() %>%  
  group_by(ids_) %>%
  mutate(total = sum(n)) %>%
  ungroup()


bag.of.words.bi <- bi.tdf %>% 
  select(ids_, token, n) %>% 
  pivot_wider(names_from = token, values_from = n, values_fill = 0) 


bag.of.words.bi <- data_count %>%
  select(ids_, president) %>%
  rename (Response_President = president) %>%
  right_join(bag.of.words.bi, join_by("ids_")) %>%
  select(ids_, Response_President, everything()) 
```

<!-- For nchar = 4, when bag size = 4500, naive bayes = 0.47 and svm = 0.58 -->
A word bag of 2000, 2500, 3000, 4000, 4500 and 5000 were looked at, these tests will not be shown, but can be replicated. The best performing model came out to be with a bag size of 4500 of the configurations tried. Unfortunately, character sequences such as: "..." and "..j" were created when looking at a number of characters of 3, this can not be run by these models. So for simplicity, only when the number of characters equal to 4 was observed here. 

```{r 4 Char Test Bigger Bag No Punc}
train.set    <- bag.of.words.bi[bag.of.words.bi$ids_ %in% train, -1]
val.set      <- bag.of.words.bi[bag.of.words.bi$ids_ %in% val, -1]
test.set     <- bag.of.words.bi[bag.of.words.bi$ids_ %in% test, -1]


# Bayes Classifier
data_classifier <- naiveBayes(train.set[,-1], train.set[,1])
p1 <- predict(data_classifier, val.set[,-1])
tt <- table((p1), (unlist(val.set[,1])))
1 - sum(diag(tt))/ sum(tt)

# SVM
classifier = svm(train.set[,-1], train.set[,1], type = "C-classification", scale = T)
p1 <- predict(classifier, val.set[,-1])
tt <- table((p1), (unlist(val.set[,1])))
1 - sum(diag(tt))/ sum(tt)
```
<!-- With spaces: naive = 0.463; svm = 0.418 -->
<!-- Without spaces: naive = 0.47 svm = 0.42 -->

We clearly see here that there is an improvement when punctuation is included.

Need to check if case matters too. But I don't think it will matter



# Data Handling Options

## Equally represented training sets

There are various ways in which one might go about selecting the training and test split for the models. One option considered was in having equal representation of each president in the training split.

The idea here is that each president should be represented equally in their training and validation. It is not necessary to concern oneself with the equality of representation in the test set. For this reason, it would make sense to extract a sample of the same size for each president, regardless of how many speeches they've done.

There are two approaches examined here. The first being undersampling, the following being oversampling. Undersampling, is removing data at random from classes that are over-represented until the classes are all balanced. The issue with such a method is that important information may in fact be lost in the process. Oversampling on the other hand will result in the overestimation of the underrepresented classes.

([\@Imbalance]())

Whether we use an undersampling approach or an oversampling approach the idea is that we have a balanced dataset, that is capable of equally representing each class.

### Undersampling Approach

Undersampling has been done by taking a random sample without replacement from each president. Each speech was tokenized into sentences. Since deKlerk had a total of 92 sentences, each training class had to be somewhere below this threshold. 70 training sentences were considered as this represented 80% of deKlerks total number of sentences. 70 sentences were therefore collected from the each presidents' domain. With the understanding that presidents like Mandela, Mbeki and Zuma all had over 1500 sentences to contribute, this undersampling may not be beneficial at all in that it ultimately may lose way too much information.

```{r Undersample Split}
# Need to keep some sentences separate.

table(sentences$president)

# Let's say each president has 70 sentences (that's close to 1000 words each.)
# for each president, I would advise removing their first sentence as it appears in all statements in some way.

under <- sentences %>% 
  group_by(president) %>%
  slice_sample(n = 70) %>% 
  ungroup() %>%
  rename( token = speech )

# Here we have an undersample
table(under$president)
```

Via an exploration of tokenization by words, we see very small frequencies even for words that are important to a document. This level of improtance has been calculated using tf-idf (term frequency - inverse document frequency). The only president who benefits from this undersampling is deKlerk. This can be seen in how they continue to maintain higher term counts for words commonly used in their speech. The majority of presidents suffer from this undersampling as they have counts mostly between 0 and 6, for commonly used words in their speeches.

```{r Exploration of Under-Split}
# Unigram
uni <- under %>%
  rename( speech = token) %>%
  unnest_tokens(token, speech) %>%
  #filter(!token %in% stop_words$word) %>%
  count(president, token)

uni %>%
  group_by(president) %>%
  summarise(total = sum(n))

uni_tf_idf <- uni %>%
  bind_tf_idf(token, president, n)

uni_tf_idf %>% 
  group_by(president) %>% 
  slice_max(tf_idf, n = 5) %>% 
  ungroup() %>%
  mutate(token = reorder(token, tf_idf, decreasing = F)) %>%
  ggplot(aes(tf_idf, token, fill = president)) +
  geom_col(show.legend = FALSE) +
  labs(x = "tf-idf", y = NULL) +
  facet_wrap(~president, ncol = 2, scales = "free")

uni_tf_idf %>%
  group_by(president) %>%
  slice_max(tf_idf, n= 5)%>%
  ungroup() %>%
  ggplot(aes(tf, fct_reorder(token, n), fill = president)) +
  geom_col(show.legend = FALSE) +
  facet_wrap(~president, ncol = 2, scales = "free") +
  labs(x = "Term Frequency", y = NULL)

uni_tf_idf %>%
  group_by(president) %>%
  slice_max(tf_idf, n= 5)%>%
  ungroup() %>%
  ggplot(aes(n, fct_reorder(token, n), fill = president)) +
  geom_col(show.legend = FALSE) +
  facet_wrap(~president, ncol = 2, scales = "free") +
  labs(x = "Count", y = NULL)
```

### Oversampling Approach

<!-- use bootstrapping of sample of 70 sentences -->

Another option is to look into oversampling ([\@Imbalance]()). [\@Imbalance]() has suggested that oversampling may in fact be the correct way to go about correctly representing a class that is not well represented in a dataset, and that it could generally yield better results than undersampling.

Let's train the data on a sample of 1200 sentences for each president and see how it fairs. This would mean having to bootstrap for presidents like deKlerk and Mothlanthe. However, this should be done with the understanding that these two characters may have overestimated validation results as there could be instance of training examples in the validation.

Howeveer, one needs to understand that with oversample, it is necesssary that we ensure that the samples that are resampled are not repeated in the training, validation and testing samples.

However, we need these to be together to obtain statistics like tf-idf, frequency and frequency count.

```{r Oversample Split}
table(sentences$president)

over1 <- sentences %>%
  filter(!president %in% c("deKlerk", "Motlanthe")) %>%
  group_by(president) %>%
  sample_n(size = 1200) %>%
  ungroup()

over2 <- as.data.frame(sentences) %>%
  filter(president %in% c("deKlerk", "Motlanthe")) %>%
  group_by(president) %>%
  slice_sample( prop = 0.6 ) %>%
  sample_n(size = 1200, replace = T) %>%
  ungroup()

over.train <- bind_rows(over1, over2) %>%
  mutate(train = T)

over.test <- sentences %>%
  anti_join(over.train) %>%
  mutate(train = F)

over <- bind_rows(over.train, over.test) %>%
  mutate(ids = row_number())

table(over$president)
```

In the short exploration of the training samples below it would seem that oversampling is indeed a better strategy than undersampling. We can attempt to build off this. We may even be able to look into bigram and trigrams for this sized dataset. However, it should be understood that bigrams and trigrams would be grossly disproportianate under this bootstrapping as both deKlerk and Motlanthe will have a lot more common phrases than any other president, not because they are trying to get a point across but because some of their sentences are resampled.

```{r Exploration of Over-Split}
# Unigram
uni <- over %>%
  unnest_tokens(token, speech) %>%
  filter(!token %in% stop_words$word) %>%
  count(president, token)

uni %>%
  group_by(president) %>%
  summarise(total = sum(n))

uni_tf_idf <- uni %>%
  bind_tf_idf(token, president, n) %>%
  mutate( ids = row_number())

# Plots
uni_tf_idf %>% 
  group_by(president) %>% 
  slice_max(n, n = 15) %>% 
  ungroup() %>%
  mutate(token = reorder(token, n, decreasing = F)) %>%
  ggplot(aes(n, token, fill = president)) +
  geom_col(show.legend = FALSE) +
  labs(x = "tf-idf", y = NULL) +
  facet_wrap(~president, ncol = 2, scales = "free")

uni_tf_idf %>%
  group_by(president) %>%
  slice_max(tf_idf, n= 15)%>%
  ungroup() %>%
  ggplot(aes(tf_idf, fct_reorder(token, tf_idf), fill = president)) +
  geom_col(show.legend = FALSE) +
  facet_wrap(~president, ncol = 2, scales = "free") +
  labs(x = "Count", y = NULL)
```

```{r}
# Bigram
bigram <- over.train %>% 
  unnest_tokens(token, speech, token = 'ngrams', n = 2) %>%
  count(president, token)

bigram_tf_idf <- bigram %>%
  bind_tf_idf(token, president, n)

bigram %>%
  group_by(president) %>%
  summarise(total = sum(n))

bigram_tf_idf %>% 
  group_by(president) %>% 
  slice_max(tf_idf, n = 6) %>% 
  ungroup() %>%
  mutate(token = reorder(token, tf_idf, decreasing = F)) %>%
  ggplot(aes(tf_idf, token, fill = president)) +
  geom_col(show.legend = FALSE) +
  labs(x = "tf-idf", y = NULL) +
  facet_wrap(~president, ncol = 2, scales = "free")

bigram_tf_idf %>%
  group_by(president) %>%
  slice_max(tf_idf, n= 15)%>%
  ungroup() %>%
  ggplot(aes(n, fct_reorder(token, n), fill = president)) +
  geom_col(show.legend = FALSE) +
  facet_wrap(~president, ncol = 2, scales = "free") +
  labs(x = "Count", y = NULL)
```

Looking below, it would seem that anything more than a bigram for this oversampled training set would not be in our favour. Characters like deKlerk and Motlanthe have many resampled observations, this would indeed skew their repeated observations for trigrams and beyond tremendously, which could ultimately result in some bias in their direction when modelling or may increase their overestimation. For this reason, it is probably preferred to look into tokenization for individual words as this would be the least influential in terms of over-estimation and bias in-sample.

```{r}
# Trigram
trigram <- over.train %>% 
  unnest_tokens(token, speech, token = 'ngrams', n = 3) %>%
  count(president, token)


tri_tf_idf <- trigram %>%
  bind_tf_idf(token, president, n) %>%
  mutate(president = factor(president, levels = c("deKlerk", "Mandela", "Mbeki", "Motlanthe", "Ramaphosa", "Zuma"))) %>%
  arrange(desc(tf_idf))

tri_tf_idf %>%
  group_by(president) %>%
  slice_max(tf_idf, n= 10)%>%
  ungroup() %>%
  ggplot(aes(tf_idf, fct_reorder(token, tf_idf), fill = president)) +
  geom_col(show.legend = FALSE) +
  facet_wrap(~president, ncol = 2, scales = "free") +
  labs(x = "tf-idf", y = NULL)

tri_tf_idf %>%
  group_by(president) %>%
  slice_max(tf_idf, n= 10)%>%
  ungroup() %>%
  ggplot(aes(n, fct_reorder(token, n), fill = president)) +
  geom_col(show.legend = FALSE) +
  facet_wrap(~president, ncol = 2, scales = "free") +
  labs(x = "Count", y = NULL)
```
