---
title: "Supplementary"
author: "Edward Baleni"
format: html
editor: visual
bibliography: references.bib
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE, fig.align="center", out.width = "65%", fig.pos = "H")
```

```{r Packages}
require(dplyr)
require(stringr)
require(tidytext)
require(ggplot2)
require(tidyr)
require(forcats)
require(textstem)
require(qdap)

library(e1071)

library(keras) 
library(reticulate) 
library(tensorflow)

#set.seed(321)
```

```{r Data}
set.seed(2023)
sys.source("sona-first-steps.R" , envir = knitr::knit_global())
sona <- as_tibble(sona)
#sona$president <- as.factor(sona$president)
```

```{r Cleaning Data}
sentences <- sona %>%
  unnest_sentences(speech, speech) #%>%
  #mutate(ids = row_number())

clean <- function(x){
  # remove numbers
  x <- gsub("\\d+", "", x)
  # remove contractions
  x <- gsub("/'s", "", x)
  # remove special characters
  x <- gsub("[^[:alnum:] ]", "", x)
  # lemmatization
  x <- lemmatize_strings(x)
  # remove single characters
  x <- gsub("\\s.\\s", " ", x)
}

sentences$speech <- unlist(lapply(sentences$speech, clean))
```

# Supplementary

This supplementary serves as grounds on which most of the final decisions were based. It includes finding preferred stylometic features and the handling of imbalanced classes.

## Stylometric Features

The first task is to select the stylometric features. This is deciding how one may choose to represent text. This can either be done by tokenizing by words (lexical features), a number of characters (character features), or in other more sophisticated ways like syntatic features or semantic features. In this study, only lexical features and character features are explored, while the other two require more domain knowledge to properly quantify ([\@Stylometry]()). 

The goal of this assignment is that of author attribution, we are essentially trying to decide which president said what sentence. Although the some of the points addressed within the speech are slightly different, the main topic is that of a political address, therefore it may be worthwhile to view this research as a style-based text classification rather than a topic-based text classification. This will have significant effect on the study and the effect will be demonstrated here.

One of the issues that still need to be addressed is the handling of imbalanced classes, at this stage, the imbalanced classes will be removed and will be looked at again later on.

Another point to address is the useage of token count or TF-IDF weightings, as observations for the study. This initial exploration will be done with frequency, as this is a very stable way in conducting author attribution ([\@WordFreq]()). 

For each experiment, a Naive Bayes classifier and a Support Vector Machine will be used to inform decisions.

When looking into the count data, it sometimes does help to select a the most frequent words for the analysis. This will be examined later, at this stage, 150

### Lexical Features

```{r Remove Imabalances}
data <- sentences %>%
  filter(!president %in% c("deKlerk", "Motlanthe"))
data$president <- as.factor(data$president)

data_count <- data

data_count <- data_count %>%
  mutate(ids_ = row_number())
```

When it comes to lexical features, these can be classed into N-grams. A collection of N contiguous words. The greater N is, the more complex the representation of the feature. It often doesn't show better results and the greater the N the higher the dimensionality of the problem, which will ultimately increase sparsity in the dataset, making it more difficult to classify ([\@Ngrams](), [\@WordFreq]()). To that extent, only unigrams and bigrams will be explored. 

```{r Unigram}
uni <- data_count %>%
  unnest_tokens(token, speech, token = "words") %>%
  filter(!token %in% stop_words$word)
  
word.bag.uni <- uni %>%
  group_by(token) %>%
  count() %>%
  ungroup() %>%
  top_n(200, wt = n) %>%
  select(-n)

uni.tdf <- uni %>%
  inner_join(word.bag.uni) %>%
  group_by(ids_,token) %>%
  count() %>%  
  group_by(ids_) %>%
  mutate(total = sum(n)) %>%
  ungroup()


bag.of.words.uni <- uni.tdf %>% 
  select(ids_, token, n) %>% 
  pivot_wider(names_from = token, values_from = n, values_fill = 0) 


bag.of.words.uni <- data_count %>%
  select(ids_, president) %>%
  rename (Response_President = president) %>%
  right_join(bag.of.words.uni, join_by("ids_")) %>%
  select(ids_, Response_President, everything()) 
```


```{r Bigram}
bi <- data_count %>%
  unnest_tokens(token, speech, token = "ngrams", n = 2) %>%
  filter(!token %in% stop_words$word)
  
word.bag.bi <- bi %>%
  group_by(token) %>%
  count() %>%
  ungroup() %>%
  top_n(200, wt = n) %>%
  select(-n)

bi.tdf <- bi %>%
  inner_join(word.bag.bi) %>%
  group_by(ids_,token) %>%
  count() %>%  
  group_by(ids_) %>%
  mutate(total = sum(n)) %>%
  ungroup()


bag.of.words.bi <- bi.tdf %>% 
  select(ids_, token, n) %>% 
  pivot_wider(names_from = token, values_from = n, values_fill = 0) 


bag.of.words.bi <- data_count %>%
  select(ids_, president) %>%
  rename (Response_President = president) %>%
  right_join(bag.of.words.bi, join_by("ids_")) %>%
  select(ids_, Response_President, everything()) 
```

For the training, validation and test splits, it would be wise to select sample sizes by sentence and not by word, this will represent each row in the bag of words.
```{r Splits}
id           <- unique(data_count$ids_)
N            <- length(id) 

# Training set
train <- sample(id, size = N * 0.6)

# Validation set
not_train    <- na.omit(ifelse(!id %in% train, id, NA))
NN           <- length(not_train)
val          <- sample(not_train, size = NN * 0.5 )

# Test set
test         <- na.omit(ifelse(!not_train %in% val, not_train, NA))

length(test) + length(val) + length(train)
```


```{r Unigram Test}
train.set    <- bag.of.words.uni[bag.of.words.uni$ids_ %in% train, -1]
val.set      <- bag.of.words.uni[bag.of.words.uni$ids_ %in% val, -1]
test.set     <- bag.of.words.uni[bag.of.words.uni$ids_ %in% test, -1]

# Bayes classifier
data_classifier <- naiveBayes(train.set[,-1], train.set[,1])
p1 <- predict(data_classifier, val.set[,-1])
tt <- table((p1), (unlist(val.set[,1])))
1 - sum(diag(tt))/ sum(tt)

# SVM
classifier = svm(train.set[,-1], train.set[,1], type = "C-classification", scale = T)
p1 <- predict(classifier, val.set[,-1])
tt <- table((p1), (unlist(val.set[,1])))
1 - sum(diag(tt))/ sum(tt)
```

```{r Bigram Test}
train.set    <- bag.of.words.bi[bag.of.words.bi$ids_ %in% train, -1]
val.set      <- bag.of.words.bi[bag.of.words.bi$ids_ %in% val, -1]
test.set     <- bag.of.words.bi[bag.of.words.bi$ids_ %in% test, -1]


# Bayes Classifier
data_classifier <- naiveBayes(train.set[,-1], train.set[,1])
p1 <- predict(data_classifier, val.set[,-1])
tt <- table((p1), (unlist(val.set[,1])))
1 - sum(diag(tt))/ sum(tt)

# SVM
classifier = svm(train.set[,-1], train.set[,1], type = "C-classification", scale = T)
p1 <- predict(classifier, val.set[,-1])
tt <- table((p1), (unlist(val.set[,1])))
1 - sum(diag(tt))/ sum(tt)
```

What we see above is that for a word bag of 200 most frequent content words that the bigram does indeed perform worse. Content words are the opposite of functional words, where functional words are used to very abundandtly in style-based classification. These functional words are often removed in topic-based classification ([\@Stylometry]()) (there are more papers)..

Although the missclassification error here is already quite poor, we can use this as a platform on which to launch the rest of the analysis. It is essential that we observe whether allowing functional words will improve the statement made earlier, that this is more directed towards style-based classification.

It should also be noted that the unigram performining better than the bigram could be due to the size of the word bag.

First let's look into whether using functional words will be useful to the analysis.



```{r Unigram}
data_count <- data

data_count <- data_count %>%
  mutate(ids_ = row_number())

uni <- data_count %>%
  unnest_tokens(token, speech, token = "words")
  
word.bag.uni <- uni %>%
  group_by(token) %>%
  count() %>%
  ungroup() %>%
  top_n(200, wt = n) %>%
  select(-n)

uni.tdf <- uni %>%
  inner_join(word.bag.uni) %>%
  group_by(ids_,token) %>%
  count() %>%  
  group_by(ids_) %>%
  mutate(total = sum(n)) %>%
  ungroup()


bag.of.words.uni <- uni.tdf %>% 
  select(ids_, token, n) %>% 
  pivot_wider(names_from = token, values_from = n, values_fill = 0) 


bag.of.words.uni <- data_count %>%
  select(ids_, president) %>%
  rename (Response_President = president) %>%
  right_join(bag.of.words.uni, join_by("ids_")) %>%
  select(ids_, Response_President, everything()) 
```


```{r Bigram}
bi <- data_count %>%
  unnest_tokens(token, speech, token = "ngrams", n = 2)
  
word.bag.bi <- bi %>%
  group_by(token) %>%
  count() %>%
  ungroup() %>%
  top_n(200, wt = n) %>%
  select(-n)

bi.tdf <- bi %>%
  inner_join(word.bag.bi) %>%
  group_by(ids_,token) %>%
  count() %>%  
  group_by(ids_) %>%
  mutate(total = sum(n)) %>%
  ungroup()


bag.of.words.bi <- bi.tdf %>% 
  select(ids_, token, n) %>% 
  pivot_wider(names_from = token, values_from = n, values_fill = 0) 


bag.of.words.bi <- data_count %>%
  select(ids_, president) %>%
  rename (Response_President = president) %>%
  right_join(bag.of.words.bi, join_by("ids_")) %>%
  select(ids_, Response_President, everything()) 
```


```{r Unigram Test}
train.set    <- bag.of.words.uni[bag.of.words.uni$ids_ %in% train, -1]
val.set      <- bag.of.words.uni[bag.of.words.uni$ids_ %in% val, -1]
test.set     <- bag.of.words.uni[bag.of.words.uni$ids_ %in% test, -1]

# Bayes classifier
data_classifier <- naiveBayes(train.set[,-1], train.set[,1])
p1 <- predict(data_classifier, val.set[,-1])
tt <- table((p1), (unlist(val.set[,1])))
1 - sum(diag(tt))/ sum(tt)

# SVM
classifier = svm(train.set[,-1], train.set[,1], type = "C-classification", scale = T)
p1 <- predict(classifier, val.set[,-1])
tt <- table((p1), (unlist(val.set[,1])))
1 - sum(diag(tt))/ sum(tt)
```

```{r Bigram Test}
train.set    <- bag.of.words.bi[bag.of.words.bi$ids_ %in% train, -1]
val.set      <- bag.of.words.bi[bag.of.words.bi$ids_ %in% val, -1]
test.set     <- bag.of.words.bi[bag.of.words.bi$ids_ %in% test, -1]


# Bayes Classifier
data_classifier <- naiveBayes(train.set[,-1], train.set[,1])
p1 <- predict(data_classifier, val.set[,-1])
tt <- table((p1), (unlist(val.set[,1])))
1 - sum(diag(tt))/ sum(tt)

# SVM
classifier = svm(train.set[,-1], train.set[,1], type = "C-classification", scale = T)
p1 <- predict(classifier, val.set[,-1])
tt <- table((p1), (unlist(val.set[,1])))
1 - sum(diag(tt))/ sum(tt)
```

Here we see that this is indeed more of style-based classification than a topic-based classification, this is shown in how the missclassification in general does decrease in both cases of of the bigram and unigram. As mentioned above, this is so as each of the SONA speeches, although done by different presidents, in general speaks on very similar topics and ideas, therefore, their styles are more important than their topics.

In this second set of classifications we see that the unigram actually outperforms the bigram, whilst also having a missclassification that is lower than when stop words were removed.

At this stage, I would suggest using unigrams and not using stop words for the analysis.

However, since this is now a style-based classification task, using character features has in previous works been shown to capture style, lexical information and some contextual information. It is also able use punctuation and capitalization to infer stylistic approaches.

### Character Features


```{r 3 Characters}
uni <- data_count %>%
  unnest_character_shingles(token, speech, n = 3) 
  
word.bag.uni <- uni %>%
  group_by(token) %>%
  count() %>%
  ungroup() %>%
  top_n(200, wt = n) %>%
  select(-n)

uni.tdf <- uni %>%
  inner_join(word.bag.uni) %>%
  group_by(ids_,token) %>%
  count() %>%  
  group_by(ids_) %>%
  mutate(total = sum(n)) %>%
  ungroup()


bag.of.words.uni <- uni.tdf %>% 
  select(ids_, token, n) %>% 
  pivot_wider(names_from = token, values_from = n, values_fill = 0) 


bag.of.words.uni <- data_count %>%
  select(ids_, president) %>%
  rename (Response_President = president) %>%
  right_join(bag.of.words.uni, join_by("ids_")) %>%
  select(ids_, Response_President, everything()) 
```


```{r 4 Characters}
bi <- data_count %>%
  unnest_character_shingles(token, speech, n = 4) 
  
word.bag.bi <- bi %>%
  group_by(token) %>%
  count() %>%
  ungroup() %>%
  top_n(200, wt = n) %>%
  select(-n)

bi.tdf <- bi %>%
  inner_join(word.bag.bi) %>%
  group_by(ids_,token) %>%
  count() %>%  
  group_by(ids_) %>%
  mutate(total = sum(n)) %>%
  ungroup()


bag.of.words.bi <- bi.tdf %>% 
  select(ids_, token, n) %>% 
  pivot_wider(names_from = token, values_from = n, values_fill = 0) 


bag.of.words.bi <- data_count %>%
  select(ids_, president) %>%
  rename (Response_President = president) %>%
  right_join(bag.of.words.bi, join_by("ids_")) %>%
  select(ids_, Response_President, everything()) 
```


```{r 3 Char Test}
train.set    <- bag.of.words.uni[bag.of.words.uni$ids_ %in% train, -1]
val.set      <- bag.of.words.uni[bag.of.words.uni$ids_ %in% val, -1]
test.set     <- bag.of.words.uni[bag.of.words.uni$ids_ %in% test, -1]

# Bayes classifier
data_classifier <- naiveBayes(train.set[,-1], train.set[,1])
p1 <- predict(data_classifier, val.set[,-1])
tt <- table((p1), (unlist(val.set[,1])))
1 - sum(diag(tt))/ sum(tt)

# SVM
classifier = svm(train.set[,-1], train.set[,1], type = "C-classification", scale = T)
p1 <- predict(classifier, val.set[,-1])
tt <- table((p1), (unlist(val.set[,1])))
1 - sum(diag(tt))/ sum(tt)
```

```{r 4 Char Test}
train.set    <- bag.of.words.bi[bag.of.words.bi$ids_ %in% train, -1]
val.set      <- bag.of.words.bi[bag.of.words.bi$ids_ %in% val, -1]
test.set     <- bag.of.words.bi[bag.of.words.bi$ids_ %in% test, -1]


# Bayes Classifier
data_classifier <- naiveBayes(train.set[,-1], train.set[,1])
p1 <- predict(data_classifier, val.set[,-1])
tt <- table((p1), (unlist(val.set[,1])))
1 - sum(diag(tt))/ sum(tt)

# SVM
classifier = svm(train.set[,-1], train.set[,1], type = "C-classification", scale = T)
p1 <- predict(classifier, val.set[,-1])
tt <- table((p1), (unlist(val.set[,1])))
1 - sum(diag(tt))/ sum(tt)
```



At this stage we can see that the classification is not that much different from a unigram, however, points of which to consider, is that when using character features certain items become necessary and deserve exploration: the number of characters, the size of the word bag, the inclusion of punctuation.

The number of characters have been chosen to be either 3 or 4, which is consistent with the study ([\@Stylometry]()). The size of the bag and the inclusion of punctuation are two points which need to be further explored.




```{r 3 Characters Bigger Bag}
uni <- data_count %>%
  unnest_character_shingles(token, speech, n = 3) 
  
word.bag.uni <- uni %>%
  group_by(token) %>%
  count() %>%
  ungroup() %>%
  top_n(2500, wt = n) %>%
  select(-n)

uni.tdf <- uni %>%
  inner_join(word.bag.uni) %>%
  group_by(ids_,token) %>%
  count() %>%  
  group_by(ids_) %>%
  mutate(total = sum(n)) %>%
  ungroup()


bag.of.words.uni <- uni.tdf %>% 
  select(ids_, token, n) %>% 
  pivot_wider(names_from = token, values_from = n, values_fill = 0) 


bag.of.words.uni <- data_count %>%
  select(ids_, president) %>%
  rename (Response_President = president) %>%
  right_join(bag.of.words.uni, join_by("ids_")) %>%
  select(ids_, Response_President, everything()) 
```


```{r 4 Characters Bigger Bag}
bi <- data_count %>%
  unnest_character_shingles(token, speech, n = 4) 
  
word.bag.bi <- bi %>%
  group_by(token) %>%
  count() %>%
  ungroup() %>%
  top_n(2500, wt = n) %>%
  select(-n)

bi.tdf <- bi %>%
  inner_join(word.bag.bi) %>%
  group_by(ids_,token) %>%
  count() %>%  
  group_by(ids_) %>%
  mutate(total = sum(n)) %>%
  ungroup()


bag.of.words.bi <- bi.tdf %>% 
  select(ids_, token, n) %>% 
  pivot_wider(names_from = token, values_from = n, values_fill = 0) 


bag.of.words.bi <- data_count %>%
  select(ids_, president) %>%
  rename (Response_President = president) %>%
  right_join(bag.of.words.bi, join_by("ids_")) %>%
  select(ids_, Response_President, everything()) 
```


```{r 3 Char Test Bigger Bag}
train.set    <- bag.of.words.uni[bag.of.words.uni$ids_ %in% train, -1]
val.set      <- bag.of.words.uni[bag.of.words.uni$ids_ %in% val, -1]
test.set     <- bag.of.words.uni[bag.of.words.uni$ids_ %in% test, -1]

# Bayes classifier
data_classifier <- naiveBayes(train.set[,-1], train.set[,1])
p1 <- predict(data_classifier, val.set[,-1])
tt <- table((p1), (unlist(val.set[,1])))
1 - sum(diag(tt))/ sum(tt)

# SVM
classifier = svm(train.set[,-1], train.set[,1], type = "C-classification", scale = T)
p1 <- predict(classifier, val.set[,-1])
tt <- table((p1), (unlist(val.set[,1])))
1 - sum(diag(tt))/ sum(tt)
```

```{r 4 Char Test Bigger Bag}
train.set    <- bag.of.words.bi[bag.of.words.bi$ids_ %in% train, -1]
val.set      <- bag.of.words.bi[bag.of.words.bi$ids_ %in% val, -1]
test.set     <- bag.of.words.bi[bag.of.words.bi$ids_ %in% test, -1]


# Bayes Classifier
data_classifier <- naiveBayes(train.set[,-1], train.set[,1])
p1 <- predict(data_classifier, val.set[,-1])
tt <- table((p1), (unlist(val.set[,1])))
1 - sum(diag(tt))/ sum(tt)

# SVM
classifier = svm(train.set[,-1], train.set[,1], type = "C-classification", scale = T)
p1 <- predict(classifier, val.set[,-1])
tt <- table((p1), (unlist(val.set[,1])))
1 - sum(diag(tt))/ sum(tt)
```



The missclassification has decreased tremendously from when the word bag contained 200 words.



Now below we look at the inclusion of punctuation



```{r Including Punctuation}
sentences <- sona %>%
  unnest_sentences(speech, speech) #%>%
  #mutate(ids = row_number())

clean <- function(x){
  # remove numbers
  x <- gsub("\\d+", "", x)
  # lemmatization
  x <- lemmatize_strings(x)
}

sentences$speech <- unlist(lapply(sentences$speech, clean))

data <- sentences %>%
  filter(!president %in% c("deKlerk", "Motlanthe"))
data$president <- as.factor(data$president)

data_count <- data

data_count <- data_count %>%
  mutate(ids_ = row_number())
```




```{r 4 Characters Bigger Bag Punc}
bi <- data_count %>%
  unnest_character_shingles(token, speech, n = 4, strip_non_alphanum = F) 
  
word.bag.bi <- bi %>%
  group_by(token) %>%
  count() %>%
  ungroup() %>%
  top_n(4500, wt = n) %>%
  select(-n)

bi.tdf <- bi %>%
  inner_join(word.bag.bi) %>%
  group_by(ids_,token) %>%
  count() %>%  
  group_by(ids_) %>%
  mutate(total = sum(n)) %>%
  ungroup()


bag.of.words.bi <- bi.tdf %>% 
  select(ids_, token, n) %>% 
  pivot_wider(names_from = token, values_from = n, values_fill = 0) 


bag.of.words.bi <- data_count %>%
  select(ids_, president) %>%
  rename (Response_President = president) %>%
  right_join(bag.of.words.bi, join_by("ids_")) %>%
  select(ids_, Response_President, everything()) 
```

<!-- For nchar = 4, when bag size = 4500, naive bayes = 0.47 and svm = 0.58 -->
A word bag of 2000, 2500, 3000, 4000, 4500 and 5000 were looked at, these tests will not be shown, but can be replicated. The best performing model came out to be with a bag size of 4500 of the configurations tried. Unfortunately, character sequences such as: "..." and "..j" were created when looking at a number of characters of 3, this can not be run by these models. So for simplicity, only when the number of characters equal to 4 was observed here. 

```{r 4 Char Test Bigger Bag No Punc}
train.set    <- bag.of.words.bi[bag.of.words.bi$ids_ %in% train, -1]
val.set      <- bag.of.words.bi[bag.of.words.bi$ids_ %in% val, -1]
test.set     <- bag.of.words.bi[bag.of.words.bi$ids_ %in% test, -1]


# Bayes Classifier
data_classifier <- naiveBayes(train.set[,-1], train.set[,1])
p1 <- predict(data_classifier, val.set[,-1])
tt <- table((p1), (unlist(val.set[,1])))
1 - sum(diag(tt))/ sum(tt)

# SVM
classifier = svm(train.set[,-1], train.set[,1], type = "C-classification", scale = T)
p1 <- predict(classifier, val.set[,-1])
tt <- table((p1), (unlist(val.set[,1])))
1 - sum(diag(tt))/ sum(tt)
```
<!-- With spaces: naive = 0.463; svm = 0.418 -->
<!-- Without spaces: naive = 0.47 svm = 0.42 -->

We clearly see here that there is an improvement when punctuation is included.

Need to check if case matters too. But I don't think it will matter


```{r Including Case}
sentences <- sona %>%
  unnest_sentences(speech, speech, to_lower = F)

clean <- function(x){
  # remove numbers
  x <- gsub("\\d+", "", x)
  # lemmatization
  x <- lemmatize_strings(x)
}

sentences$speech <- unlist(lapply(sentences$speech, clean))

data <- sentences %>%
  filter(!president %in% c("deKlerk", "Motlanthe"))
data$president <- as.factor(data$president)

data_count <- data

data_count <- data_count %>%
  mutate(ids_ = row_number())
```


```{r 4 Characters Case}
bi <- data_count %>%
  unnest_character_shingles(token, speech, n = 4, strip_non_alphanum = F, to_lower = F) 
  
word.bag.bi <- bi %>%
  group_by(token) %>%
  count() %>%
  ungroup() %>%
  top_n(4500, wt = n) %>%
  select(-n)

bi.tdf <- bi %>%
  inner_join(word.bag.bi) %>%
  group_by(ids_,token) %>%
  count() %>%  
  group_by(ids_) %>%
  mutate(total = sum(n)) %>%
  ungroup()


bag.of.words.bi <- bi.tdf %>% 
  select(ids_, token, n) %>% 
  pivot_wider(names_from = token, values_from = n, values_fill = 0) 


bag.of.words.bi <- data_count %>%
  select(ids_, president) %>%
  rename (Response_President = president) %>%
  right_join(bag.of.words.bi, join_by("ids_")) %>%
  select(ids_, Response_President, everything()) 
```

```{r 4 Char Test With Case} 
train.set    <- bag.of.words.bi[bag.of.words.bi$ids_ %in% train, -1]
val.set      <- bag.of.words.bi[bag.of.words.bi$ids_ %in% val, -1]
test.set     <- bag.of.words.bi[bag.of.words.bi$ids_ %in% test, -1]


# Bayes Classifier
data_classifier <- naiveBayes(train.set[,-1], train.set[,1])
p1 <- predict(data_classifier, val.set[,-1])
tt <- table((p1), (unlist(val.set[,1])))
1 - sum(diag(tt))/ sum(tt)

# SVM
classifier = svm(train.set[,-1], train.set[,1], type = "C-classification", scale = T)
p1 <- predict(classifier, val.set[,-1])
tt <- table((p1), (unlist(val.set[,1])))
1 - sum(diag(tt))/ sum(tt)
```

We see here that if the capitalization is maintained we actually get the best performing svm, and a decently performing naive bayes classifier. With a missclassification of 0.4 for the SVM. 




The next step we can look into to improve the the In-sample error, is to look at feature extraction. This would also be helpful in deciding the size of the word bag that is necessary. Until now, the word bag has been decided by trial and error, and by the assumption that a more simplistic representation, i.e using characters, not removing punctuation, not removing capitalization, would require a larger word bad to attain more contextual information.










```{r FFNN}
y_train <- as.integer(unlist(train.set[,1])) # - 1
x_train <- as.matrix(train.set[,-1])

y_test <- as.integer(unlist(val.set[,1])) # - 1 
x_test <- as.matrix(val.set[,-1])


 y_train <- to_categorical(y_train)
# y_test_original <- y_test
 y_test <- to_categorical(y_test)

model <- keras_model_sequential() %>%
   layer_dense(units = 32, activation = 'relu', input_shape = c(4538)) %>% 
   layer_dropout(rate = 0.5) %>%
   layer_dense(units = 5, activation = 'softmax')

summary(model)

model %>% compile(
  loss = "categorical_crossentropy",
  optimizer = optimizer_adam(learning_rate = 0.01),
  metrics = "categorical_accuracy"
)

history <- model %>% fit(
  x_train, y_train, 
  epochs = 50, batch_size = 100,verbose = 0) 

plot(history)

results <- model %>% evaluate(x_test, y_test, batch_size=100, verbose = 2)
results
```






### Feature Selection


## Topic-Based Classification

As previously mentioned, it may be more beneficial to look at stlye-based classification over a text-based classification, as the topics within these documents are mostly similar with some differences. However, to further prove this, TF-IDF will be explored as this measurement is perfect for the exploratioon of document topic. However, this will be briefly explored here to see whether it is still a viable option.

### TF-IDF

<!-- https://www.researchgate.net/publication/270761963_Improving_Native_Language_Identification_with_TF-IDF_Weighting -->


TF-IDF is able to take on both complex word tokens as well as character tokens. The best models above will be compared here. 
TF-IDF, cannot be divided up into the top n words by TF-IDF in this case, as these types of words only appear once or twice within each document.

```{r}
sentences <- sona %>%
  unnest_sentences(speech, speech)

clean <- function(x){
  # remove numbers
  x <- gsub("\\d+", "", x)
  # remove contractions
  x <- gsub("/'s", "", x)
  # remove special characters
  x <- gsub("[^[:alnum:] ]", "", x)
  # lemmatization
  x <- lemmatize_strings(x)
  # remove single characters
  x <- gsub("\\s.\\s", " ", x)
}

sentences$speech <- unlist(lapply(sentences$speech, clean))

data <- sentences %>%
  filter(!president %in% c("deKlerk", "Motlanthe"))
data$president <- as.factor(data$president)

data_count <- data

data_count <- data_count %>%
  mutate(ids_ = row_number())
```


```{r}
uni_ <- data_count %>%
  unnest_tokens(token, speech) %>%
  filter(!token %in% stop_words$word)

uni_tdf <-  uni_ %>%
  group_by(ids_, token) %>%
  count() %>%
  group_by(ids_) %>%
  mutate(total = sum(n)) %>%
  ungroup()

uni_tdf <- uni_tdf %>%
  bind_tf_idf(token, ids_, n)

# # Calculate the mean TF-IDF score for each term across all presidents
# term_mean_tfidf <- tweets_tdf %>%
#   group_by(token) %>%
#   summarize(mean_tfidf = mean(tf_idf, na.rm = TRUE)) %>%
#   arrange(desc(mean_tfidf))
# 
# # Select the top 200 terms
# top_terms <- term_mean_tfidf$token[1:200]
# 
# # Filter your dataframe to keep only these top terms
# tweets_tdf_top200 <- tweets_tdf %>%
#   filter(token %in% top_terms)

# 
# 
# # Maybe pick out the top 200 on tf_idf alone
# word_bag2 <- tweets_tdf %>%
#   top_n(n = 200, wt = n)
# #
# 
#tweets_tdf <- tweets_tdf %>%
#  semi_join(word_bag)

tfidf <- uni_tdf %>% 
  select(ids_, token, tf_idf) %>%  # note the change, using tf-idf
  pivot_wider(names_from = token, values_from = tf_idf, values_fill = 0)


tfidf <- data_count %>%
  select(ids_, president) %>%
  rename (Response_President = president) %>%
  right_join(tfidf, join_by("ids_")) %>%
  select(ids_, Response_President, everything()) 


# #Converting the frequency of word to count
# convert_counts <- function(x) {
#         x <- ifelse(x > 0, 1, 0)
#         x <- factor(x, levels = c(0, 1), labels = c("No", "Yes"))
#         return(x)
# }
# 
# #Appending count function to Train and Test Dataset
# data_train <- apply(tfidf[,-c(1,2)], MARGIN = 2, convert_counts)
# 
# data_train <- model.matrix(~. -1, data=data.frame(data_train))
# 
# 
# tfidf <-  data.frame(tfidf[,c(1,2)], data_train)
```

```{r}
train.set    <- tfidf[tfidf$ids_ %in% train, -1]
val.set      <- tfidf[tfidf$ids_ %in% val, -1]
test.set     <- tfidf[tfidf$ids_ %in% test, -1]


# Bayes Classifier
data_classifier <- naiveBayes(train.set[,-1], train.set[,1])
p1 <- predict(data_classifier, val.set[,-1])
tt <- table((p1), (unlist(val.set[,1])))
1 - sum(diag(tt))/ sum(tt)

# SVM
classifier = svm(train.set[,-1], train.set[,1], type = "C-classification", scale = F)
p1 <- predict(classifier, val.set[,-1])
tt <- table((p1), (unlist(val.set[,1])))
1 - sum(diag(tt))/ sum(tt)
```



```{r TFIDF Including Case}
sentences <- sona %>%
  unnest_sentences(speech, speech, to_lower = F)

clean <- function(x){
  # remove numbers
  x <- gsub("\\d+", "", x)
  # lemmatization
  x <- lemmatize_strings(x)
}

sentences$speech <- unlist(lapply(sentences$speech, clean))

data <- sentences %>%
  filter(!president %in% c("deKlerk", "Motlanthe"))
data$president <- as.factor(data$president)

data_count <- data

data_count <- data_count %>%
  mutate(ids_ = row_number())
```


```{r}
uni_ <- data_count %>%
  unnest_character_shingles(token, speech, n = 4, strip_non_alphanum = F, to_lower = F) 

uni_tdf <-  uni_ %>%
  group_by(ids_, token) %>%
  count() %>%
  group_by(ids_) %>%
  mutate(total = sum(n)) %>%
  ungroup()

uni_tdf <- uni_tdf %>%
  bind_tf_idf(token, ids_, n)

# # Calculate the mean TF-IDF score for each term across all presidents
# term_mean_tfidf <- tweets_tdf %>%
#   group_by(token) %>%
#   summarize(mean_tfidf = mean(tf_idf, na.rm = TRUE)) %>%
#   arrange(desc(mean_tfidf))
# 
# # Select the top 200 terms
# top_terms <- term_mean_tfidf$token[1:200]
# 
# # Filter your dataframe to keep only these top terms
# tweets_tdf_top200 <- tweets_tdf %>%
#   filter(token %in% top_terms)

# 
# 
# # Maybe pick out the top 200 on tf_idf alone
# word_bag2 <- tweets_tdf %>%
#   top_n(n = 200, wt = n)
# #
# 
#tweets_tdf <- tweets_tdf %>%
#  semi_join(word_bag)

tfidf <- uni_tdf %>% 
  select(ids_, token, tf_idf) %>%  # note the change, using tf-idf
  pivot_wider(names_from = token, values_from = tf_idf, values_fill = 0)


tfidf <- data_count %>%
  select(ids_, president) %>%
  rename (Response_President = president) %>%
  right_join(tfidf, join_by("ids_")) %>%
  select(ids_, Response_President, everything()) 
```



```{r}
train.set    <- tfidf[tfidf$ids_ %in% train, -1]
val.set      <- tfidf[tfidf$ids_ %in% val, -1]
test.set     <- tfidf[tfidf$ids_ %in% test, -1]


# Bayes Classifier
data_classifier <- naiveBayes(train.set[,-1], train.set[,1])
p1 <- predict(data_classifier, val.set[,-1])
tt <- table((p1), (unlist(val.set[,1])))
1 - sum(diag(tt))/ sum(tt)

# SVM
classifier = svm(train.set[,-1], train.set[,1], type = "C-classification", scale = F)
p1 <- predict(classifier, val.set[,-1])
tt <- table((p1), (unlist(val.set[,1])))
1 - sum(diag(tt))/ sum(tt)
```







