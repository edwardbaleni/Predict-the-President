---
title: "Supplementary"
author: "Edward Baleni"
bibliography: references.bib
---

## Equally represented training sets

<!-- https://www.analyticsvidhya.com/blog/2021/06/5-techniques-to-handle-imbalanced-data-for-a-classification-problem/ -->
<!-- https://machinelearningmastery.com/what-is-imbalanced-classification/ -->
<!-- https://machinelearningmastery.com/tactics-to-combat-imbalanced-classes-in-your-machine-learning-dataset/ -->
<!-- https://developers.google.com/machine-learning/data-prep/construct/sampling-splitting/imbalanced-data -->


```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE, fig.align="center", out.width = "65%", fig.pos = "H")
```

```{r Packages}
require(dplyr)
require(stringr)
require(tidytext)
require(ggplot2)
require(tidyr)
require(forcats)
require(textstem)
require(qdap)
```

```{r Data}
set.seed(2023)
sys.source("sona-first-steps.R" , envir = knitr::knit_global())
sona <- as_tibble(sona)
sona$president <- as.factor(sona$president)
```

```{r Cleaning Data}
sentences <- sona %>%
  unnest_sentences(speech, speech) #%>%
  #mutate(ids = row_number())

clean <- function(x){
  x <- gsub("\\d+", "", x)
  # remove contractions
  x <- gsub("/'s", "", x)
  # remove special characters
  x <- gsub("[^[:alnum:] ]", "", x)
  # lemmatization
  x <- lemmatize_strings(x)
  # remove single characters
  x <- gsub("\\s.\\s", " ", x)
}

# remove numbers
sentences$speech <- unlist(lapply(sentences$speech, clean))
```

There are various ways in which one might go about selecting the training and test split for the models. One option considered was in having equal representation of each president in the training split.

The idea here is that each president should be represented equally in their training and validation. It is not necessary to concern oneself with the equality of representation in the test set. For this reason, it would make sense to extract a sample of the same size for each president, regardless of how many speeches they've done.

There are two approaches examined here. The first being undersampling, the following being oversampling. Undersampling, is removing data at random from classes that are over-represented until the classes are all balanced. The issue with such a method is that important information may in fact be lost in the process. Oversampling on the other hand will result in the overestimation of the underrepresented classes.

([\@Imbalance]())

Whether we use an undersampling approach or an oversampling approach the idea is that we have a balanced dataset, that is capable of equally representing each class.

### Undersampling Approach

Undersampling has been done by taking a random sample without replacement from each president. Each speech was tokenized into sentences. Since deKlerk had a total of 92 sentences, each training class had to be somewhere below this threshold. 70 training sentences were considered as this represented 80% of deKlerks total number of sentences. 70 sentences were therefore collected from the each presidents' domain. With the understanding that presidents like Mandela, Mbeki and Zuma all had over 1500 sentences to contribute, this undersampling may not be beneficial at all in that it ultimately may lose way too much information.

```{r Undersample Split}
# Need to keep some sentences separate.

table(sentences$president)

# Let's say each president has 70 sentences (that's close to 1000 words each.)
# for each president, I would advise removing their first sentence as it appears in all statements in some way.

under <- sentences %>% 
  group_by(president) %>%
  slice_sample(n = 70) %>% 
  ungroup() %>%
  rename( token = speech )

# Here we have an undersample
table(under$president)
```

Via an exploration of tokenization by words, we see very small frequencies even for words that are important to a document. This level of improtance has been calculated using tf-idf (term frequency - inverse document frequency). The only president who benefits from this undersampling is deKlerk. This can be seen in how they continue to maintain higher term counts for words commonly used in their speech. The majority of presidents suffer from this undersampling as they have counts mostly between 0 and 6, for commonly used words in their speeches.

```{r Exploration of Under-Split}
# Unigram
uni <- under %>%
  rename( speech = token) %>%
  unnest_tokens(token, speech) %>%
  #filter(!token %in% stop_words$word) %>%
  count(president, token)

uni %>%
  group_by(president) %>%
  summarise(total = sum(n))

uni_tf_idf <- uni %>%
  bind_tf_idf(token, president, n)

uni_tf_idf %>% 
  group_by(president) %>% 
  slice_max(tf_idf, n = 5) %>% 
  ungroup() %>%
  mutate(token = reorder(token, tf_idf, decreasing = F)) %>%
  ggplot(aes(tf_idf, token, fill = president)) +
  geom_col(show.legend = FALSE) +
  labs(x = "tf-idf", y = NULL) +
  facet_wrap(~president, ncol = 2, scales = "free")

uni_tf_idf %>%
  group_by(president) %>%
  slice_max(tf_idf, n= 5)%>%
  ungroup() %>%
  ggplot(aes(tf, fct_reorder(token, n), fill = president)) +
  geom_col(show.legend = FALSE) +
  facet_wrap(~president, ncol = 2, scales = "free") +
  labs(x = "Term Frequency", y = NULL)

uni_tf_idf %>%
  group_by(president) %>%
  slice_max(tf_idf, n= 5)%>%
  ungroup() %>%
  ggplot(aes(n, fct_reorder(token, n), fill = president)) +
  geom_col(show.legend = FALSE) +
  facet_wrap(~president, ncol = 2, scales = "free") +
  labs(x = "Count", y = NULL)
```

### Oversampling Approach

<!-- use bootstrapping of sample of 70 sentences -->

Another option is to look into oversampling ([\@Imbalance]()). [\@Imbalance]() has suggested that oversampling may in fact be the correct way to go about correctly representing a class that is not well represented in a dataset, and that it could generally yield better results than undersampling.

Let's train the data on a sample of 1200 sentences for each president and see how it fairs. This would mean having to bootstrap for presidents like deKlerk and Mothlanthe. However, this should be done with the understanding that these two characters may have overestimated validation results as there could be instance of training examples in the validation.

```{r Oversample Split}
table(sentences$president)

# Let's say each president has 70 sentences (that's close to 1000 words each.)
# for each president, I would advise removing their first sentence as it appears in all statements in some way.

# train <- sentences %>%
#   filter(!president %in% c("deKlerk", "Motlanthe")) %>%
#   group_by(president) %>%
#   sample_n(size = 1200) %>%
#   ungroup() %>%
#   rename( token = speech )
# 
# train2 <- as.data.frame(sentences) %>%
#   filter(president %in% c("deKlerk", "Motlanthe")) %>%
#   group_by(president) %>%
#   slice_sample( prop = 0.8 ) %>%
#   sample_n(size = 1200, replace = T) %>%
#   ungroup() %>%
#   rename( token = speech )

# train <- as.data.frame(sentences) %>%
#   group_by(president) %>%
#   slice_sample( prop = 0.8 ) %>%
#   sample_n(size = 1200, replace = T) %>%
#   ungroup() %>%
#   rename( token = speech )


# train <- bind_rows(train1, train2) %>%
#   mutate(ids = row_number())

# table(train$president)
# 
# test <- sentences %>%
#   unnest_sentences(token, speech) %>%
#   anti_join(train, join_by(token)) %>%
#   mutate(ids = row_number())

over1 <- sentences %>%
  filter(president %in% c("deKlerk", "Motlanthe")) %>%
  group_by(president) %>%
  sample_n(size = 1600, replace = T) %>%
  ungroup()

over2 <- sentences %>%
  filter(!president %in% c("deKlerk", "Motlanthe"))

over <- bind_rows(over1, over2) %>%
  mutate(ids = row_number())

table(over$president)
```

In the short exploration of the training samples below it would seem that oversampling is indeed a better strategy than undersampling. We can attempt to build off this. We may even be able to look into bigram and trigrams for this sized dataset.


```{r Exploration of Over-Split}
# Unigram
uni <- over %>%
  unnest_tokens(token, speech) %>%
  #filter(!token %in% stop_words$word) %>%
  count(president, token)

uni %>%
  group_by(president) %>%
  summarise(total = sum(n))

uni_tf_idf <- uni %>%
  bind_tf_idf(token, president, n) %>%
  mutate( ids = row_number())

# Plots
uni_tf_idf %>% 
  group_by(president) %>% 
  slice_max(tf_idf, n = 15) %>% 
  ungroup() %>%
  mutate(token = reorder(token, tf_idf, decreasing = F)) %>%
  ggplot(aes(tf_idf, token, fill = president)) +
  geom_col(show.legend = FALSE) +
  labs(x = "tf-idf", y = NULL) +
  facet_wrap(~president, ncol = 2, scales = "free")

uni_tf_idf %>%
  group_by(president) %>%
  slice_max(tf_idf, n= 15)%>%
  ungroup() %>%
  ggplot(aes(tf, fct_reorder(token, n), fill = president)) +
  geom_col(show.legend = FALSE) +
  facet_wrap(~president, ncol = 2, scales = "free") +
  labs(x = "Term Frequency", y = NULL)

uni_tf_idf %>%
  group_by(president) %>%
  slice_max(tf_idf, n= 15)%>%
  ungroup() %>%
  ggplot(aes(n, fct_reorder(token, n), fill = president)) +
  geom_col(show.legend = FALSE) +
  facet_wrap(~president, ncol = 2, scales = "free") +
  labs(x = "Count", y = NULL)
```


```{r}
# Bigram
bigram <- over %>% 
  unnest_tokens(token, speech, token = 'ngrams', n = 2) %>%
  count(president, token)

bigram_tf_idf <- bigram %>%
  bind_tf_idf(token, president, n)

bigram %>%
  group_by(president) %>%
  summarise(total = sum(n))

bigram_tf_idf %>% 
  group_by(president) %>% 
  slice_max(tf_idf, n = 6) %>% 
  ungroup() %>%
  mutate(token = reorder(token, tf_idf, decreasing = F)) %>%
  ggplot(aes(tf_idf, token, fill = president)) +
  geom_col(show.legend = FALSE) +
  labs(x = "tf-idf", y = NULL) +
  facet_wrap(~president, ncol = 2, scales = "free")

bigram_tf_idf %>%
  group_by(president) %>%
  slice_max(tf_idf, n= 15)%>%
  ungroup() %>%
  ggplot(aes(n, fct_reorder(token, n), fill = president)) +
  geom_col(show.legend = FALSE) +
  facet_wrap(~president, ncol = 2, scales = "free") +
  labs(x = "Count", y = NULL)
```

Looking below, it would seem that anything more than a bigram for this oversampled training set would not be in our favour. Characters like deKlerk and Motlanthe have many resampled observations, this would indeed skew their repeated observations for trigrams and beyond tremendously, which could ultimately result in some bias in their direction when modelling or may increase their overestimation. For this reason, it is probably preferred to look into tokenization for individual words as this would be the least influential in terms of over-estimation and bias in-sample.

```{r}
# Trigram
trigram <- over %>% 
  unnest_tokens(token, speech, token = 'ngrams', n = 3) %>%
  count(president, token)


tri_tf_idf <- trigram %>%
  bind_tf_idf(token, president, n) %>%
  mutate(president = factor(president, levels = c("deKlerk", "Mandela", "Mbeki", "Motlanthe", "Ramaphosa", "Zuma"))) %>%
  arrange(desc(tf_idf))

tri_tf_idf %>%
  group_by(president) %>%
  slice_max(tf_idf, n= 10)%>%
  ungroup() %>%
  ggplot(aes(tf_idf, fct_reorder(token, tf_idf), fill = president)) +
  geom_col(show.legend = FALSE) +
  facet_wrap(~president, ncol = 2, scales = "free") +
  labs(x = "tf-idf", y = NULL)

tri_tf_idf %>%
  group_by(president) %>%
  slice_max(tf_idf, n= 10)%>%
  ungroup() %>%
  ggplot(aes(n, fct_reorder(token, n), fill = president)) +
  geom_col(show.legend = FALSE) +
  facet_wrap(~president, ncol = 2, scales = "free") +
  labs(x = "Count", y = NULL)
```


```{r Quick analysis to compare uni and bi using random forests}
require(naivebayes)
tt <- sentences %>%
  unnest_tokens(token, speech) %>%
  count(president, token) %>%
  ungroup() %>%
  bind_tf_idf(token, president, n)
#  top_n(200, wt = tf_idf)
  

trainer <- uni_tf_idf %>%
  group_by(president)%>%
  top_n(200, wt = tf_idf) %>%
  ungroup %>%
  select(ids, token, tf_idf) %>%
  pivot_wider(names_from = token, values_from = tf_idf, values_fill = 0)

trainer <- uni_tf_idf %>%
  select(ids, president) %>%
  rename (Response_President = president) %>%
  left_join(trainer, by = "ids")%>%
  select(-ids)

model <- naive_bayes(Response_President ~., data = trainer, usekernal = T)
model <- rpart(Response_President ~ ., method="class", data = trainer[samp,], control =rpart.control(minsplit =1, cp=0))


plot(model)
text(model, use.n = TRUE, all = TRUE, cex=.8)
val <- trainer[-samp,]

p1 <- predict(model, trainer[,-1])
(tab1 <- table(trainer$Response_President, p1))
```

```{r}

```





```{r}

train <- over %>%
  slice_sample(prop = 0.8)
Data_train <- train
test <- over %>%
  anti_join(train)
Data_test <- test

library(tm)

#Creating Corpus
  # What this does is essentially save a list of lists of all the information
suppressWarnings(Data_test_corpus <- Corpus(VectorSource(Data_test$speech)))
suppressWarnings(Data_train_corpus <- Corpus(VectorSource(Data_train$speech)))

#Corpus Cleaning
suppressWarnings(Data_test_corpus_clean <- tm_map(Data_test_corpus, tolower))
suppressWarnings(Data_train_corpus_clean <- tm_map(Data_train_corpus, tolower))

suppressWarnings(Data_test_corpus_clean <- tm_map(Data_test_corpus_clean, removeNumbers))
suppressWarnings(Data_train_corpus_clean <- tm_map(Data_train_corpus_clean, removeNumbers))

suppressWarnings(Data_test_corpus_clean <- tm_map(Data_test_corpus_clean, removeWords, stopwords()))
suppressWarnings(Data_train_corpus_clean <- tm_map(Data_train_corpus_clean, removeWords, stopwords()))

suppressWarnings(Data_test_corpus_clean <- tm_map(Data_test_corpus_clean, removePunctuation))
suppressWarnings(Data_train_corpus_clean <- tm_map(Data_train_corpus_clean, removePunctuation))

suppressWarnings(Data_test_corpus_clean <- tm_map(Data_test_corpus_clean, stripWhitespace))
suppressWarnings(Data_train_corpus_clean <- tm_map(Data_train_corpus_clean, stripWhitespace))

suppressWarnings(inspect(Data_train_corpus_clean[1]))

# Here all we've done is change train and test to another data type and removed labelling
```


```{r , warning=FALSE}
#Word Cloud of Testing Dataset
library(wordcloud)
  # Wordcloud does tokenization of corpus already.
  # It also removes stop words automatically
wordcloud(train$speech, max.words = 100, colors = c("grey80", "darkgoldenrod1", "tomato"), min.freq = 10, random.order = FALSE)
```

```{r}

# Create a document term matrix
# test_dtm <- DocumentTermMatrix(Data_test_corpus_clean, 
#                                control = list(weighting =
#                                                 function(x)
#                                                   weightTfIdf(x, normalize = T),
#                                               stopwords = TRUE))
# 
# #train_dtm2 <- DocumentTermMatrix(Corpus(VectorSource(train$token)))
# #tdformat <- TermDocumentMatrix()
# #train_dtm <- DocumentTermMatrix(Data_train_corpus_clean, control = list(weightTfIdf(m, normalize = TRUE)))
# 
# train_dtm <- DocumentTermMatrix(Data_train_corpus_clean,
#                                 control = list(weighting =
#                                                  function(x)
#                                                    weightTfIdf(x, normalize = T),
#                                                stopwords = TRUE))

cntrl=list(weighting = weightTfIdf)#,
             #removeNumbers=TRUE,
             #removeStopwords=TRUE,
             #removePunctuation=TRUE,
             #stopwords = T)

train_dtm <- DocumentTermMatrix(Data_train_corpus_clean,
                                control = cntrl)
#train_dtm <- removeSparseTerms(train_dtm,  0.95)

test_dtm <- DocumentTermMatrix(Data_test_corpus_clean,
                               control = cntrl)
#test_dtm <- removeSparseTerms(test_dtm, 0.9)

inspect(test_dtm)
inspect(train_dtm)
```

```{r}
#### Preparing Training and Testing Datasets #####
### Creating Indicator features for frequent words ###             But why does this work?????????????????

FreqWords <- findFreqTerms(train_dtm, 5)

#Saving List using Dictionary() Function
Dictionary <- function(x) {
        if( is.character(x) ) {
                return (x)
        }
        stop('x is not a character vector')
}

data_dict <- Dictionary(findFreqTerms(train_dtm, 5))

#Appending Document Term Matrix to Train and Test Dataset
data_train <- DocumentTermMatrix(Data_train_corpus_clean, list(data_dict))
data_test <- DocumentTermMatrix(Data_test_corpus_clean, list(data_dict))

#Converting the frequency of word to count
# convert_counts <- function(x) {
#         x <- ifelse(x > 0, 1, 0)
#         x <- factor(x, levels = c(0, 1), labels = c("No", "Yes"))
#         return(x)
# }
# 
# #Appending count function to Train and Test Dataset
# data_train <- apply(data_train, MARGIN = 2, convert_counts)
# data_test <- apply(data_test, MARGIN = 2, convert_counts)
```

```{r}
data_train <- as.matrix(train_dtm)
rowTotals <- slam::row_sums(data_train)
data_train <- data_train[rowTotals > 0, ]
samp <- sample(1:nrow(data_train), size = 6000)

data_test <- as.matrix(test_dtm)
rowTotals <- slam::row_sums(data_test)
data_test <- data_test[rowTotals > 0, ]

library(e1071)
data_classifier <- naiveBayes(data_train[samp,], Data_train$president[samp], laplace = 0)

library(gmodels)
data_val_pred <- predict(data_classifier, data_train[-samp,] )
CrossTable(data_val_pred, unlist(Data_train$president[-samp]),
          prop.chisq = FALSE, prop.t = FALSE,
          dnn = c('predicted', 'actual'))
#data_test_pred <- predict(data_classifier, data_test)
#CrossTable(data_test_pred, Data_test$president,
#           prop.chisq = FALSE, prop.t = FALSE,
#           dnn = c('predicted', 'actual'))


(tab1 <- table(data_val_pred, unlist(Data_train$president[-samp])))

sum(diag(tab1))/1200  # When findFreqTerms(x) then this is 0.72
```


```{r}
(202 +140 + 102 + 257 + 137 + 136) / 1200 # Normalization when calculating tfidf (Insample)
(207 + 124 + 98 + 168 + 126 + 130) / 1200 # No normalization when calculating tfidf (Insample)


# No laplace
(15 + 475 + 512 + 14 + 662 + 799) / 4800

# Laplace = 1
(11 + 504 + 539 + 11 + 684 + 881) / 4800

# Laplace = 2
(15 + 486 + 473 + 11 + 662 + 884) / 4800

# Laplace = 3
(15 + 478 + 412 + 10 + 645 + 892) / 4800
# Starts to dip from here on
```


```{r XGB}
require(caret)
require(xgboost)
# library(doParallel)
# library(parallel)
xgb_grid <- expand.grid(nrounds = 500,                   #B - number of trees
                        max_depth = 6,               #d - interaction depth
                        eta = 0.01,                           #lambda - learning rate
                        gamma = 0.1,                        #mindev
                        colsample_bytree = 1,                #proportion random features per tree
                        min_child_weight = 6, #also controls tree depth
                        subsample = 1               #bootstrap proportion
  )
  
  # # Parallelization
 # no_cores <- 10
 # cl <- makePSOCKcluster(no_cores)
 # registerDoParallel(cl)
  
  # Set Control
xgb_cntrl <-  trainControl(method = 'cv', number = 5, verboseIter = T)#, allowParallel = T)
dat <- data.frame(president = Data_train$president, data_train)[samp[1:1000],]

a <- apply(data_train, 2, as.factor)

xgb <- train(president ~.,
             data = dat,
             method = 'xgbTree',
             trControl = xgb_cntrl,
             verbose = TRUE,
             tuneGrid = xgb_grid)

 # stopCluster(cl)
 # registerDoSEQ()


  
xgb$results





rf_grid <- expand.grid(.mtry = 20,
                         .min.node.size = 5,
                         .splitrule = "gini")
# Set Control
rf_cntrl <- trainControl(method = 'cv', number = 5, verboseIter = T)

rf <- train(president ~.,
            data = dat,
            method = 'ranger',
            trControl = rf_cntrl,
            tuneGrid = rf_grid,
            num.trees = 250,
            importance = "impurity")
      
```


```{r Test}
data_test_pred <- predict(data_classifier, data_test)
CrossTable(data_test_pred, Data_test$president,
          prop.chisq = FALSE, prop.t = FALSE,
          dnn = c('predicted', 'actual'))

(tab1 <- table(data_test_pred, Data_test$president))
sum(diag(tab1))/4800
```




## Proportional Sampling

This would entail obtaining an 80% sample from each presidents total number of speeches. Again the problem with this approach is the underrepresentation of statements made by presidents deKlerk and Motlanthe. Over this I would suggest removing these two presidents from the entire study, as they would be rare cases.

This option would also disallow us from using Naive Bayes Classifier in this task.

## Removal of Outliers

The final option is to just remove deKlerk and Motlanhle.

```{r Remove Outliers}
# Need to keep some sentences separate.
sentences <- sona %>%
  unnest_sentences(speech, speech)

table(sentences$president)

# Let's say each president has 70 sentences (that's close to 1000 words each.)
# for each president, I would advise removing their first sentence as it appears in all statements in some way.

# train <- sentences %>%
#   filter(!president %in% c("deKlerk", "Motlanthe")) %>%
#   group_by(president) %>%
#   sample_n(size = 1200) %>%
#   ungroup() %>%
#   rename( token = speech )
# 
# train2 <- as.data.frame(sentences) %>%
#   filter(president %in% c("deKlerk", "Motlanthe")) %>%
#   group_by(president) %>%
#   slice_sample( prop = 0.8 ) %>%
#   sample_n(size = 1200, replace = T) %>%
#   ungroup() %>%
#   rename( token = speech )

# train <- bind_rows(train1, train2) %>%
#   mutate(ids = row_number())

table(train$president)

test <- sentences %>%
  unnest_sentences(token, speech) %>%
  anti_join(train, join_by(token)) %>%
  mutate(ids = row_number())

```


```{r}
Data_train <- train
Data_test <- test

library(tm)

#Creating Corpus
  # What this does is essentially save a list of lists of all the information
suppressWarnings(Data_test_corpus <- Corpus(VectorSource(Data_test$token)))
suppressWarnings(Data_train_corpus <- Corpus(VectorSource(Data_train$token)))

#Corpus Cleaning
suppressWarnings(Data_test_corpus_clean <- tm_map(Data_test_corpus, tolower))
suppressWarnings(Data_train_corpus_clean <- tm_map(Data_train_corpus, tolower))

suppressWarnings(Data_test_corpus_clean <- tm_map(Data_test_corpus_clean, removeNumbers))
suppressWarnings(Data_train_corpus_clean <- tm_map(Data_train_corpus_clean, removeNumbers))

suppressWarnings(Data_test_corpus_clean <- tm_map(Data_test_corpus_clean, removeWords, stopwords()))
suppressWarnings(Data_train_corpus_clean <- tm_map(Data_train_corpus_clean, removeWords, stopwords()))

suppressWarnings(Data_test_corpus_clean <- tm_map(Data_test_corpus_clean, removePunctuation))
suppressWarnings(Data_train_corpus_clean <- tm_map(Data_train_corpus_clean, removePunctuation))

suppressWarnings(Data_test_corpus_clean <- tm_map(Data_test_corpus_clean, stripWhitespace))
suppressWarnings(Data_train_corpus_clean <- tm_map(Data_train_corpus_clean, stripWhitespace))

suppressWarnings(inspect(Data_train_corpus_clean[1]))

# Here all we've done is change train and test to another data type and removed labelling
```

```{r}
cntrl=list(weighting = weightTfIdf)#,
             #removeNumbers=TRUE,
             #removeStopwords=TRUE,
             #removePunctuation=TRUE,
             #stopwords = T)

train_dtm <- DocumentTermMatrix(Data_train_corpus_clean,
                                control = cntrl)
train_dtm <- removeSparseTerms(train_dtm,  0.95)

test_dtm <- DocumentTermMatrix(Data_test_corpus_clean,
                               control = cntrl)
test_dtm <- removeSparseTerms(test_dtm, 0.97)

inspect(test_dtm)
inspect(train_dtm)
```

The results of modelling with exclusion of the other two are actually still not great. Much worse when the code block below is not commented out even. We get insample of 0.6 missclassification if the below is uncommented and get insample of 0.2 missclassification if the oversampled one is uncommented.

```{r}
#### Preparing Training and Testing Datasets #####
### Creating Indicator features for frequent words ###             But why does this work?????????????????

# FreqWords <- findFreqTerms(train_dtm, 5)
# 
# #Saving List using Dictionary() Function
# Dictionary <- function(x) {
#         if( is.character(x) ) {
#                 return (x)
#         }
#         stop('x is not a character vector')
# }
# 
# data_dict <- Dictionary(findFreqTerms(train_dtm, 5))
# 
# #Appending Document Term Matrix to Train and Test Dataset
# data_train <- DocumentTermMatrix(Data_train_corpus_clean, list(data_dict))
# data_test <- DocumentTermMatrix(Data_test_corpus_clean, list(data_dict))
# 
# #Converting the frequency of word to count
# convert_counts <- function(x) {
#         x <- ifelse(x > 0, 1, 0)
#         x <- factor(x, levels = c(0, 1), labels = c("No", "Yes"))
#         return(x)
# }
# 
# #Appending count function to Train and Test Dataset
# data_train <- apply(data_train, MARGIN = 2, convert_counts)
# data_test <- apply(data_test, MARGIN = 2, convert_counts)
```

```{r}
data_train <- as.matrix(train_dtm)

samp <- sample(1:nrow(data_train), size = 6000)

data_test <- as.matrix(test_dtm)

library(e1071)
data_classifier <- naiveBayes(data_train[samp,], Data_train$president[samp], laplace = 0)

library(gmodels)
data_val_pred <- predict(data_classifier, data_train[-samp,] )
CrossTable(data_val_pred, Data_train$president[-samp],
          prop.chisq = FALSE, prop.t = FALSE,
          dnn = c('predicted', 'actual'))
#data_test_pred <- predict(data_classifier, data_test)
#CrossTable(data_test_pred, Data_test$president,
#           prop.chisq = FALSE, prop.t = FALSE,
#           dnn = c('predicted', 'actual'))


(tab1 <- table(data_val_pred, Data_train$president[-samp]))
sum(diag(tab1))/1200  # When findFreqTerms(x) then this is 0.72
```




######### Something extra
```{r}
tt <- over %>%
  unnest_tokens(word, speech) %>%
  select(president, word, ids)

word_bag <- tt %>%
  group_by(word) %>%
  count() %>%
  ungroup() %>%
  #top_n(200, wt = n) %>%
  select(-n)

tweets_tdf <- tt %>%
  #inner_join(word_bag) %>%
  group_by(ids,word) %>%
  count() %>%  
  group_by(ids) %>%
  mutate(total = sum(n)) %>%
  ungroup()

tweets_tdf <- tweets_tdf %>% #remove the old ones we worked out
  bind_tf_idf(word, ids, n) %>%
  group_by(word) %>%
  top_n(200, tf_idf)


tfidf <- tweets_tdf %>% 
  select(ids, word, tf_idf) %>%  # note the change, using tf-idf
  pivot_wider(names_from = word, values_from = tf_idf, values_fill = 0)# %>%  
#  left_join(sentences %>% select(ids, president))

tfidf <- over %>%
  select(ids, president) %>%
  rename (Response_President = president) %>%
  left_join(tfidf, by = "ids")%>%
  select(-ids)


train <- tfidf %>%
  group_by(Response_President) %>%
  slice_sample( prop = 0.8 ) %>%
  ungroup()


# train <- bind_rows(train1, train2) %>%
#   mutate(ids = row_number())

table(train$president)

model <- naive_bayes(Response_President ~., data = train, usekernal = T)

p1 <- predict(model, train[,-1])
(tab1 <- table(train$Response_President, p1))

plot(model)
```