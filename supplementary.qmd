---
title: "Supplementary"
author: "Edward Baleni"
format: html
editor: visual
bibliography: references.bib
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE, fig.align="center", out.width = "65%", fig.pos = "H")
```

```{r Packages, include=FALSE,results='hide'}
require(dplyr)
require(stringr)
require(tidytext)
require(ggplot2)
require(tidyr)
require(forcats)
require(textstem)
require(textclean)
require(e1071)
require(keras) 
require(reticulate) 
require(tensorflow)
```

# Supplementary

This supplementary serves as grounds on which most of the final decisions were based. It includes finding preferred stylometic features and the handling of imbalanced classes.

```{r Data}
set.seed(2023)
sys.source("sona-first-steps.R" , envir = knitr::knit_global())
sona <- as_tibble(sona)
```

```{r Pre-Process}
sentences <- sona %>%
  unnest_sentences(speech, speech)

str.ct <- function(x){
  hold <- length(unlist(str_split(x, " ")))
  return(hold)
}

sentences$StringCount <- unlist(lapply(sentences$speech, str.ct))
sentences <- sentences[sentences$StringCount > 2,]
```

```{r Cleanest Data}
clean1 <- function(x){
  # remove numbers
  x <- gsub("\\d+", "", x)
  # remove contractions
  x <- replace_contraction(x)
  # remove special characters
  x <- gsub("[^[:alnum:] ]", "", x)
  # lemmatization
  x <- lemmatize_strings(x)
  # remove single characters
  x <- gsub("\\s.\\s", " ", x)
}
cleaned.sentences        <- sentences 
cleaned.sentences$speech <- unlist(lapply(cleaned.sentences$speech, clean1))
```

## Stylometric Features

The first task is to select the stylometric features. This is deciding how one may choose to represent text. This can either be done by tokenizing by words (lexical features), a number of characters (character features), or in other more sophisticated ways like syntatic features or semantic features. In this study, only lexical features and character features are explored, while the other two require more domain knowledge to properly quantify ([\@Stylometry]()).

Another point to address is the usage of token count or TF-IDF weightings, as observations for the study. This initial exploration will be done with frequency, as this is a very stable way in conducting author attribution ([\@WordFreq]()).

For each experiment, a Naive Bayes classifier and a Support Vector Machine will be used to inform decisions, and at the end a Feed-forward neural network will be explored.

### Lexical Features

```{r Remove Imabalances 1}
data_count <- cleaned.sentences %>%
  filter(!president %in% c("deKlerk", "Motlanthe"))
data_count$president <- as.factor(data_count$president)

data_count <- data_count %>%
  mutate(ids_ = row_number())
```

When it comes to lexical features, these can be classed into N-grams. A collection of N contiguous words. The greater N is, the more complex the representation of the feature. It often doesn't show better results and the greater the N the higher the dimensionality of the problem, which will ultimately increase sparsity in the dataset, making it more difficult to classify ([\@Ngrams](), [\@WordFreq]()). To that extent, only unigrams and bigrams will be explored. Often it would seem that unigrams are sufficient amongst the three. It is however, interesting to observe the behaviour of n-grams @Tiffani2020OptimizationON.

```{r Unigram 1}
uni.1 <- data_count %>%
  unnest_tokens(token, speech, token = "words") %>%
  filter(!token %in% stop_words$word)
  
word.bag.uni.1 <- uni.1 %>%
  group_by(token) %>%
  count() %>%
  ungroup() %>%
  top_n(200, wt = n) %>%
  select(-n)

uni.tdf.1 <- uni.1 %>%
  inner_join(word.bag.uni.1) %>%
  group_by(ids_,token) %>%
  count() %>%  
  group_by(ids_) %>%
  mutate(total = sum(n)) %>%
  ungroup()


bag.of.words.uni.1 <- uni.tdf.1 %>% 
  select(ids_, token, n) %>% 
  pivot_wider(names_from = token, values_from = n, values_fill = 0) 


bag.of.words.uni.1 <- data_count %>%
  select(ids_, president) %>%
  rename (Response_President = president) %>%
  right_join(bag.of.words.uni.1, join_by("ids_")) %>%
  select(ids_, Response_President, everything()) 
```

```{r Bigram 1}
bigram <- data_count %>% 
  unnest_tokens(word, speech, token = 'ngrams', n = 2)

bigrams_separated <- bigram %>%
  separate(word, c("word1", "word2"), sep = " ")

bigrams_filtered <- bigrams_separated %>%
  filter(!word1 %in% stop_words$word) %>%
  filter(!word2 %in% stop_words$word)

bi <- bigrams_filtered %>%
  unite(token, word1, word2, sep = ' ')
  
word.bag.bi <- bi %>%
  group_by(token) %>%
  count() %>%
  ungroup() %>%
  top_n(200, wt = n) %>%
  select(-n)

bi.tdf <- bi %>%
  inner_join(word.bag.bi) %>%
  group_by(ids_,token) %>%
  count() %>%  
  group_by(ids_) %>%
  mutate(total = sum(n)) %>%
  ungroup()


bag.of.words.bi <- bi.tdf %>% 
  select(ids_, token, n) %>% 
  pivot_wider(names_from = token, values_from = n, values_fill = 0) 


bag.of.words.bi <- data_count %>%
  select(ids_, president) %>%
  rename (Response_President = president) %>%
  right_join(bag.of.words.bi, join_by("ids_")) %>%
  select(ids_, Response_President, everything()) 
```

For the training, validation and test splits, it would be wise to select sample sizes by sentence and not by word, this will represent each row in the bag of words.

```{r Splits}
id           <- unique(data_count$ids_)
N            <- length(id) 

# Training set
train <- sample(id, size = N * 0.6)

# Validation set
not_train    <- na.omit(ifelse(!id %in% train, id, NA))
NN           <- length(not_train)
val          <- sample(not_train, size = NN * 0.5 )

# Test set
test         <- na.omit(ifelse(!not_train %in% val, not_train, NA))

length(test) + length(val) + length(train)
```

```{r Unigram Test 1}
train.set.1    <- bag.of.words.uni.1[bag.of.words.uni.1$ids_ %in% train, -1]
val.set.1      <- bag.of.words.uni.1[bag.of.words.uni.1$ids_ %in% val, -1]
test.set.1     <- bag.of.words.uni.1[bag.of.words.uni.1$ids_ %in% test, -1]

# Bayes classifier
data_classifier.1 <- naiveBayes(train.set.1[,-1], train.set.1[,1])
p1.1 <- predict(data_classifier.1, val.set.1[,-1])
tt.1 <- table((p1.1), (unlist(val.set.1[,1])))
1 - sum(diag(tt.1))/ sum(tt.1)

# SVM
classifier = svm(train.set[,-1], train.set[,1], type = "C-classification", scale = T)
p1 <- predict(classifier, val.set[,-1])
tt <- table((p1), (unlist(val.set[,1])))
1 - sum(diag(tt))/ sum(tt)
```

```{r Bigram Test 1}
train.set    <- bag.of.words.bi[bag.of.words.bi$ids_ %in% train, -1]
val.set      <- bag.of.words.bi[bag.of.words.bi$ids_ %in% val, -1]
test.set     <- bag.of.words.bi[bag.of.words.bi$ids_ %in% test, -1]


# Bayes Classifier
data_classifier <- naiveBayes(train.set[,-1], train.set[,1])
p1 <- predict(data_classifier, val.set[,-1])
tt <- table((p1), (unlist(val.set[,1])))
1 - sum(diag(tt))/ sum(tt)

# SVM
classifier = svm(train.set[,-1], train.set[,1], type = "C-classification", scale = T)
p1 <- predict(classifier, val.set[,-1])
tt <- table((p1), (unlist(val.set[,1])))
1 - sum(diag(tt))/ sum(tt)

save(classifier, file = "Test1.RData")

```

What we see above is that for a word bag of 200 most frequent content words that the bigram does indeed perform worse. Content words are the opposite of functional words, where functional words are used to very abundandtly in style-based classification. These functional words are often removed in topic-based classification ([\@Stylometry]()) (there are more papers)..

Although the missclassification error here is already quite poor, we can use this as a platform on which to launch the rest of the analysis. It is essential that we observe whether allowing functional words will improve the statement made earlier, that this is more directed towards style-based classification.

It should also be noted that the unigram performining better than the bigram could be due to the size of the word bag.

First let's look into whether using functional words will be useful to the analysis.

```{r Unigram 2}
uni.2 <- data_count %>%
  unnest_tokens(token, speech, token = "words")
  
word.bag.uni.2 <- uni.2 %>%
  group_by(token) %>%
  count() %>%
  ungroup() %>%
  top_n(200, wt = n) %>%
  select(-n)

uni.tdf.2 <- uni.2 %>%
  inner_join(word.bag.uni.2) %>%
  group_by(ids_,token) %>%
  count() %>%  
  group_by(ids_) %>%
  mutate(total = sum(n)) %>%
  ungroup()


bag.of.words.uni.2 <- uni.tdf.2 %>% 
  select(ids_, token, n) %>% 
  pivot_wider(names_from = token, values_from = n, values_fill = 0) 


bag.of.words.uni.2 <- data_count %>%
  select(ids_, president) %>%
  rename (Response_President = president) %>%
  right_join(bag.of.words.uni.2, join_by("ids_")) %>%
  select(ids_, Response_President, everything()) 
```

```{r Bigram 2}
bi.2 <- data_count %>%
  unnest_tokens(token, speech, token = "ngrams", n = 2)
  
word.bag.bi.2 <- bi.2 %>%
  group_by(token) %>%
  count() %>%
  ungroup() %>%
  top_n(200, wt = n) %>%
  select(-n)

bi.tdf.2 <- bi.2 %>%
  inner_join(word.bag.bi.2) %>%
  group_by(ids_,token) %>%
  count() %>%  
  group_by(ids_) %>%
  mutate(total = sum(n)) %>%
  ungroup()


bag.of.words.bi.2 <- bi.tdf.2 %>% 
  select(ids_, token, n) %>% 
  pivot_wider(names_from = token, values_from = n, values_fill = 0) 


bag.of.words.bi.2 <- data_count %>%
  select(ids_, president) %>%
  rename (Response_President = president) %>%
  right_join(bag.of.words.bi.2, join_by("ids_")) %>%
  select(ids_, Response_President, everything()) 
```

```{r Unigram Test 2}
train.set    <- bag.of.words.uni.2[bag.of.words.uni.2$ids_ %in% train, -1]
val.set      <- bag.of.words.uni.2[bag.of.words.uni.2$ids_ %in% val, -1]
test.set     <- bag.of.words.uni.2[bag.of.words.uni.2$ids_ %in% test, -1]

# Bayes classifier
data_classifier <- naiveBayes(train.set[,-1], train.set[,1])
p1 <- predict(data_classifier, val.set[,-1])
tt <- table((p1), (unlist(val.set[,1])))
1 - sum(diag(tt))/ sum(tt)

# SVM
classifier = svm(train.set[,-1], train.set[,1], type = "C-classification", scale = T)
p1 <- predict(classifier, val.set[,-1])
tt <- table((p1), (unlist(val.set[,1])))
1 - sum(diag(tt))/ sum(tt)
```

```{r Bigram Test 2}
train.set    <- bag.of.words.bi.2[bag.of.words.bi.2$ids_ %in% train, -1]
val.set      <- bag.of.words.bi.2[bag.of.words.bi.2$ids_ %in% val, -1]
test.set     <- bag.of.words.bi.2[bag.of.words.bi.2$ids_ %in% test, -1]


# Bayes Classifier
data_classifier <- naiveBayes(train.set[,-1], train.set[,1])
p1 <- predict(data_classifier, val.set[,-1])
tt <- table((p1), (unlist(val.set[,1])))
1 - sum(diag(tt))/ sum(tt)

# SVM
classifier = svm(train.set[,-1], train.set[,1], type = "C-classification", scale = T)
p1 <- predict(classifier, val.set[,-1])
tt <- table((p1), (unlist(val.set[,1])))
1 - sum(diag(tt))/ sum(tt)
```

Here we see that this is indeed more of style-based classification than a topic-based classification, this is shown in how the missclassification in general does decrease in both cases of of the bigram and unigram. As mentioned above, this is so as each of the SONA speeches, although done by different presidents, in general speaks on very similar topics and ideas, therefore, their styles are more important than their topics.

In this second set of classifications we see that the unigram actually outperforms the bigram, whilst also having a missclassification that is lower than when stop words were removed.

At this stage, I would suggest using unigrams and not using stop words for the analysis.

```{r Unigram 3}
sentences <- sona %>%
  unnest_sentences(speech, speech, strip_punct = F) #%>%
  #mutate(ids = row_number())

sentences$speech <- unlist(lapply(sentences$speech, clean1))

data <- sentences %>%
  filter(!president %in% c("deKlerk", "Motlanthe"))
data$president <- as.factor(data$president)

data_count <- data

data_count <- data_count %>%
  mutate(ids_ = row_number())

uni.3 <- data_count %>%
  unnest_tokens(token, speech, token = "words", strip_punct =FALSE, strip_numeric = TRUE)
  
word.bag.uni.3 <- uni.3 %>%
  group_by(token) %>%
  count() %>%
  ungroup() %>%
  top_n(200, wt = n) %>%
  select(-n)

uni.tdf.3 <- uni.3 %>%
  inner_join(word.bag.uni.3) %>%
  group_by(ids_,token) %>%
  count() %>%  
  group_by(ids_) %>%
  mutate(total = sum(n)) %>%
  ungroup()


bag.of.words.uni.3 <- uni.tdf.3 %>% 
  select(ids_, token, n) %>% 
  pivot_wider(names_from = token, values_from = n, values_fill = 0) 


bag.of.words.uni.3 <- data_count %>%
  select(ids_, president) %>%
  rename (Response_President = president) %>%
  right_join(bag.of.words.uni.3, join_by("ids_")) %>%
  select(ids_, Response_President, everything()) 
```

```{r Unigram Test 3}
train.set    <- bag.of.words.uni.3[bag.of.words.uni.3$ids_ %in% train, -1]
val.set      <- bag.of.words.uni.3[bag.of.words.uni.3$ids_ %in% val, -1]
test.set     <- bag.of.words.uni.3[bag.of.words.uni.3$ids_ %in% test, -1]

# Bayes classifier
data_classifier <- naiveBayes(train.set[,-1], train.set[,1])
p1 <- predict(data_classifier, val.set[,-1])
tt <- table((p1), (unlist(val.set[,1])))
1 - sum(diag(tt))/ sum(tt)

# SVM
classifier = svm(train.set[,-1], train.set[,1], type = "C-classification", scale = T)
p1 <- predict(classifier, val.set[,-1])
tt <- table((p1), (unlist(val.set[,1])))
1 - sum(diag(tt))/ sum(tt)
```

However, since this is now a style-based classification task, using character features has in previous works been shown to capture style, lexical information and some contextual information. It is also able use punctuation and capitalization to infer stylistic approaches.

### Character Features

```{r 3 Characters}
uni <- data_count %>%
  unnest_character_shingles(token, speech, n = 3) 
  
word.bag.uni <- uni %>%
  group_by(token) %>%
  count() %>%
  ungroup() %>%
  top_n(200, wt = n) %>%
  select(-n)

uni.tdf <- uni %>%
  inner_join(word.bag.uni) %>%
  group_by(ids_,token) %>%
  count() %>%  
  group_by(ids_) %>%
  mutate(total = sum(n)) %>%
  ungroup()


bag.of.words.uni <- uni.tdf %>% 
  select(ids_, token, n) %>% 
  pivot_wider(names_from = token, values_from = n, values_fill = 0) 


bag.of.words.uni <- data_count %>%
  select(ids_, president) %>%
  rename (Response_President = president) %>%
  right_join(bag.of.words.uni, join_by("ids_")) %>%
  select(ids_, Response_President, everything()) 
```

```{r 4 Characters}
bi <- data_count %>%
  unnest_character_shingles(token, speech, n = 4) 
  
word.bag.bi <- bi %>%
  group_by(token) %>%
  count() %>%
  ungroup() %>%
  top_n(200, wt = n) %>%
  select(-n)

bi.tdf <- bi %>%
  inner_join(word.bag.bi) %>%
  group_by(ids_,token) %>%
  count() %>%  
  group_by(ids_) %>%
  mutate(total = sum(n)) %>%
  ungroup()


bag.of.words.bi <- bi.tdf %>% 
  select(ids_, token, n) %>% 
  pivot_wider(names_from = token, values_from = n, values_fill = 0) 


bag.of.words.bi <- data_count %>%
  select(ids_, president) %>%
  rename (Response_President = president) %>%
  right_join(bag.of.words.bi, join_by("ids_")) %>%
  select(ids_, Response_President, everything()) 
```

```{r 3 Char Test}
train.set    <- bag.of.words.uni[bag.of.words.uni$ids_ %in% train, -1]
val.set      <- bag.of.words.uni[bag.of.words.uni$ids_ %in% val, -1]
test.set     <- bag.of.words.uni[bag.of.words.uni$ids_ %in% test, -1]

# Bayes classifier
data_classifier <- naiveBayes(train.set[,-1], train.set[,1])
p1 <- predict(data_classifier, val.set[,-1])
tt <- table((p1), (unlist(val.set[,1])))
1 - sum(diag(tt))/ sum(tt)

# SVM
classifier = svm(train.set[,-1], train.set[,1], type = "C-classification", scale = T)
p1 <- predict(classifier, val.set[,-1])
tt <- table((p1), (unlist(val.set[,1])))
1 - sum(diag(tt))/ sum(tt)
```

```{r 4 Char Test}
train.set    <- bag.of.words.bi[bag.of.words.bi$ids_ %in% train, -1]
val.set      <- bag.of.words.bi[bag.of.words.bi$ids_ %in% val, -1]
test.set     <- bag.of.words.bi[bag.of.words.bi$ids_ %in% test, -1]


# Bayes Classifier
data_classifier <- naiveBayes(train.set[,-1], train.set[,1])
p1 <- predict(data_classifier, val.set[,-1])
tt <- table((p1), (unlist(val.set[,1])))
1 - sum(diag(tt))/ sum(tt)

# SVM
classifier = svm(train.set[,-1], train.set[,1], type = "C-classification", scale = T)
p1 <- predict(classifier, val.set[,-1])
tt <- table((p1), (unlist(val.set[,1])))
1 - sum(diag(tt))/ sum(tt)
```

At this stage we can see that the classification is not that much different from a unigram, however, points of which to consider, is that when using character features certain items become necessary and deserve exploration: the number of characters, the size of the word bag, the inclusion of punctuation.

The number of characters have been chosen to be either 3 or 4, which is consistent with the study ([\@Stylometry]()). The size of the bag and the inclusion of punctuation are two points which need to be further explored.

```{r 3 Characters Bigger Bag}
uni <- data_count %>%
  unnest_character_shingles(token, speech, n = 3) 
  
word.bag.uni <- uni %>%
  group_by(token) %>%
  count() %>%
  ungroup() %>%
  top_n(2500, wt = n) %>%
  select(-n)

uni.tdf <- uni %>%
  inner_join(word.bag.uni) %>%
  group_by(ids_,token) %>%
  count() %>%  
  group_by(ids_) %>%
  mutate(total = sum(n)) %>%
  ungroup()


bag.of.words.uni <- uni.tdf %>% 
  select(ids_, token, n) %>% 
  pivot_wider(names_from = token, values_from = n, values_fill = 0) 


bag.of.words.uni <- data_count %>%
  select(ids_, president) %>%
  rename (Response_President = president) %>%
  right_join(bag.of.words.uni, join_by("ids_")) %>%
  select(ids_, Response_President, everything()) 
```

```{r 4 Characters Bigger Bag}
bi <- data_count %>%
  unnest_character_shingles(token, speech, n = 4) 
  
word.bag.bi <- bi %>%
  group_by(token) %>%
  count() %>%
  ungroup() %>%
  top_n(2500, wt = n) %>%
  select(-n)

bi.tdf <- bi %>%
  inner_join(word.bag.bi) %>%
  group_by(ids_,token) %>%
  count() %>%  
  group_by(ids_) %>%
  mutate(total = sum(n)) %>%
  ungroup()


bag.of.words.bi <- bi.tdf %>% 
  select(ids_, token, n) %>% 
  pivot_wider(names_from = token, values_from = n, values_fill = 0) 


bag.of.words.bi <- data_count %>%
  select(ids_, president) %>%
  rename (Response_President = president) %>%
  right_join(bag.of.words.bi, join_by("ids_")) %>%
  select(ids_, Response_President, everything()) 
```

```{r 3 Char Test Bigger Bag}
train.set    <- bag.of.words.uni[bag.of.words.uni$ids_ %in% train, -1]
val.set      <- bag.of.words.uni[bag.of.words.uni$ids_ %in% val, -1]
test.set     <- bag.of.words.uni[bag.of.words.uni$ids_ %in% test, -1]

# Bayes classifier
data_classifier <- naiveBayes(train.set[,-1], train.set[,1])
p1 <- predict(data_classifier, val.set[,-1])
tt <- table((p1), (unlist(val.set[,1])))
1 - sum(diag(tt))/ sum(tt)

# SVM
classifier = svm(train.set[,-1], train.set[,1], type = "C-classification", scale = T)
p1 <- predict(classifier, val.set[,-1])
tt <- table((p1), (unlist(val.set[,1])))
1 - sum(diag(tt))/ sum(tt)
```

```{r 4 Char Test Bigger Bag}
train.set    <- bag.of.words.bi[bag.of.words.bi$ids_ %in% train, -1]
val.set      <- bag.of.words.bi[bag.of.words.bi$ids_ %in% val, -1]
test.set     <- bag.of.words.bi[bag.of.words.bi$ids_ %in% test, -1]


# Bayes Classifier
data_classifier <- naiveBayes(train.set[,-1], train.set[,1])
p1 <- predict(data_classifier, val.set[,-1])
tt <- table((p1), (unlist(val.set[,1])))
1 - sum(diag(tt))/ sum(tt)

# SVM
classifier = svm(train.set[,-1], train.set[,1], type = "C-classification", scale = T)
p1 <- predict(classifier, val.set[,-1])
tt <- table((p1), (unlist(val.set[,1])))
1 - sum(diag(tt))/ sum(tt)
```

The missclassification has decreased tremendously from when the word bag contained 200 words.

Now below we look at the inclusion of punctuation

```{r Including Punctuation}
sentences <- sona %>%
  unnest_sentences(speech, speech, strip_punct = F) #%>%
  #mutate(ids = row_number())

clean <- function(x){
  # remove numbers
  x <- gsub("\\d+", "", x)
  # lemmatization
  x <- lemmatize_strings(x)
}

sentences$speech <- unlist(lapply(sentences$speech, clean))

data <- sentences %>%
  filter(!president %in% c("deKlerk", "Motlanthe"))
data$president <- as.factor(data$president)

data_count <- data

data_count <- data_count %>%
  mutate(ids_ = row_number())
```

```{r 4 Characters Bigger Bag Punc}
bi <- data_count %>%
  unnest_character_shingles(token, speech, n = 4, strip_non_alphanum = F) 
  
word.bag.bi <- bi %>%
  group_by(token) %>%
  count() %>%
  ungroup() %>%
  top_n(4500, wt = n) %>%
  select(-n)

bi.tdf <- bi %>%
  inner_join(word.bag.bi) %>%
  group_by(ids_,token) %>%
  count() %>%  
  group_by(ids_) %>%
  mutate(total = sum(n)) %>%
  ungroup()


bag.of.words.bi <- bi.tdf %>% 
  select(ids_, token, n) %>% 
  pivot_wider(names_from = token, values_from = n, values_fill = 0) 


bag.of.words.bi <- data_count %>%
  select(ids_, president) %>%
  rename (Response_President = president) %>%
  right_join(bag.of.words.bi, join_by("ids_")) %>%
  select(ids_, Response_President, everything()) 
```

<!-- For nchar = 4, when bag size = 4500, naive bayes = 0.47 and svm = 0.58 -->

A word bag of 2000, 2500, 3000, 4000, 4500 and 5000 were looked at, these tests will not be shown, but can be replicated. The best performing model came out to be with a bag size of 4500 of the configurations tried. Unfortunately, character sequences such as: "..." and "..j" were created when looking at a number of characters of 3, this can not be run by these models. So for simplicity, only when the number of characters equal to 4 was observed here.

```{r 4 Char Test Bigger Bag No Punc}
train.set    <- bag.of.words.bi[bag.of.words.bi$ids_ %in% train, -1]
val.set      <- bag.of.words.bi[bag.of.words.bi$ids_ %in% val, -1]
test.set     <- bag.of.words.bi[bag.of.words.bi$ids_ %in% test, -1]


# Bayes Classifier
data_classifier <- naiveBayes(train.set[,-1], train.set[,1])
p1 <- predict(data_classifier, val.set[,-1])
tt <- table((p1), (unlist(val.set[,1])))
1 - sum(diag(tt))/ sum(tt)

# SVM
classifier = svm(train.set[,-1], train.set[,1], type = "C-classification", scale = T)
p1 <- predict(classifier, val.set[,-1])
tt <- table((p1), (unlist(val.set[,1])))
1 - sum(diag(tt))/ sum(tt)
```

<!-- With spaces: naive = 0.463; svm = 0.418 -->

<!-- Without spaces: naive = 0.47 svm = 0.42 -->

We clearly see here that there is an improvement when punctuation is included.

Need to check if case matters too. But I don't think it will matter

```{r Including Case}
sentences <- sona %>%
  unnest_sentences(speech, speech, to_lower = F, strip_punct = F)

clean <- function(x){
  # remove numbers
  x <- gsub("\\d+", "", x)
  # lemmatization
  x <- lemmatize_strings(x)
}

sentences$speech <- unlist(lapply(sentences$speech, clean))

data <- sentences %>%
  filter(!president %in% c("deKlerk", "Motlanthe"))
data$president <- as.factor(data$president)

data_count <- data

data_count <- data_count %>%
  mutate(ids_ = row_number())
```

```{r 4 Characters Case}
bi <- data_count %>%
  unnest_character_shingles(token, speech, n = 4, strip_non_alphanum = F, to_lower = F) 
  
word.bag.bi <- bi %>%
  group_by(token) %>%
  count() %>%
  ungroup() %>%
  top_n(4500, wt = n) %>%
  select(-n)

bi.tdf <- bi %>%
  inner_join(word.bag.bi) %>%
  group_by(ids_,token) %>%
  count() %>%  
  group_by(ids_) %>%
  mutate(total = sum(n)) %>%
  ungroup()


bag.of.words.bi <- bi.tdf %>% 
  select(ids_, token, n) %>% 
  pivot_wider(names_from = token, values_from = n, values_fill = 0) 


bag.of.words.bi <- data_count %>%
  select(ids_, president) %>%
  rename (Response_President = president) %>%
  right_join(bag.of.words.bi, join_by("ids_")) %>%
  select(ids_, Response_President, everything()) 
```

```{r 4 Char Test With Case}
train.set    <- bag.of.words.bi[bag.of.words.bi$ids_ %in% train, -1]
val.set      <- bag.of.words.bi[bag.of.words.bi$ids_ %in% val, -1]
test.set     <- bag.of.words.bi[bag.of.words.bi$ids_ %in% test, -1]


# Bayes Classifier
data_classifier <- naiveBayes(train.set[,-1], train.set[,1])
p1 <- predict(data_classifier, val.set[,-1])
tt <- table((p1), (unlist(val.set[,1])))
1 - sum(diag(tt))/ sum(tt)

# SVM
classifier = svm(train.set[,-1], train.set[,1], type = "C-classification", scale = T)
p1 <- predict(classifier, val.set[,-1])
tt <- table((p1), (unlist(val.set[,1])))
1 - sum(diag(tt))/ sum(tt)
```

We see here that if the capitalization is maintained we actually get the best performing svm, and a decently performing naive bayes classifier. With a missclassification of 0.4 for the SVM.

The next step we can look into to improve the the In-sample error, is to look at feature extraction. This would also be helpful in deciding the size of the word bag that is necessary. Until now, the word bag has been decided by trial and error, and by the assumption that a more simplistic representation, i.e using characters, not removing punctuation, not removing capitalization, would require a larger word bad to attain more contextual information.

```{r FFNN}
# When training is done on all possible variables we get an accuracy of 62%
y_train <- as.integer(unlist(train.set[,1])) # - 1
x_train <- as.matrix(train.set[,-1])

y_test <- as.integer(unlist(val.set[,1])) # - 1 
x_test <- as.matrix(val.set[,-1])


 y_train <- to_categorical(y_train)
# y_test_original <- y_test
 y_test <- to_categorical(y_test)

model <- keras_model_sequential() %>%
   layer_dense(units = 32, activation = 'relu', input_shape = c(ncol(x_train))) %>% 
   layer_dropout(rate = 0.5) %>%
   layer_dense(units = 5, activation = 'softmax')

summary(model)

model %>% compile(
  loss = "categorical_crossentropy",
  optimizer = optimizer_adam(learning_rate = 0.01),
  metrics = "categorical_accuracy"
)

history <- model %>% fit(
  x_train, y_train, 
  epochs = 50, batch_size = 100,verbose = 0) 

plot(history)

results <- model %>% evaluate(x_test, y_test, batch_size=100, verbose = 2)
results
```

## Topic-Based Classification

As previously mentioned, it may be more beneficial to look at stlye-based classification over a text-based classification, as the topics within these documents are mostly similar with some differences. However, to further prove this, TF-IDF will be explored as this measurement is perfect for the exploratioon of document topic. However, this will be briefly explored here to see whether it is still a viable option.

### TF-IDF

<!-- https://www.researchgate.net/publication/270761963_Improving_Native_Language_Identification_with_TF-IDF_Weighting -->

TF-IDF is able to take on both complex word tokens as well as character tokens. The best models above will be compared here. TF-IDF, cannot be divided up into the top n words by TF-IDF in this case, as these types of words only appear once or twice within each document.

```{r}
data <- cleaned.sentences %>%
  filter(!president %in% c("deKlerk", "Motlanthe"))
data$president <- as.factor(data$president)

data_count <- data

data_count <- data_count %>%
  mutate(ids_ = row_number())
```

```{r}
uni_ <- data_count %>%
  unnest_tokens(token, speech)

uni_tdf <-  uni_ %>%
  group_by(ids_, token) %>%
  count() %>%
  group_by(ids_) %>%
  mutate(total = sum(n)) %>%
  ungroup()

uni_tdf <- uni_tdf %>%
  bind_tf_idf(token, ids_, n)

tfidf <- uni_tdf %>% 
  select(ids_, token, tf_idf) %>%  # note the change, using tf-idf
  pivot_wider(names_from = token, values_from = tf_idf, values_fill = 0)


tfidf <- data_count %>%
  select(ids_, president) %>%
  rename (Response_President = president) %>%
  right_join(tfidf, join_by("ids_")) %>%
  select(ids_, Response_President, everything()) 
```

```{r}
train.set    <- tfidf[tfidf$ids_ %in% train, -1]
val.set      <- tfidf[tfidf$ids_ %in% val, -1]
test.set     <- tfidf[tfidf$ids_ %in% test, -1]


# Bayes Classifier
data_classifier <- naiveBayes(train.set[,-1], train.set[,1])
p1 <- predict(data_classifier, val.set[,-1])
tt <- table((p1), (unlist(val.set[,1])))
1 - sum(diag(tt))/ sum(tt)

# SVM
classifier = svm(train.set[,-1], train.set[,1], type = "C-classification", scale = F)
p1 <- predict(classifier, val.set[,-1])
tt <- table((p1), (unlist(val.set[,1])))
1 - sum(diag(tt))/ sum(tt)
```

```{r TFIDF Including Case}
sentences <- sona %>%
  unnest_sentences(speech, speech, to_lower = F)

clean <- function(x){
  # remove numbers
  x <- gsub("\\d+", "", x)
  # lemmatization
  x <- lemmatize_strings(x)
}

sentences$speech <- unlist(lapply(sentences$speech, clean))

data <- sentences %>%
  filter(!president %in% c("deKlerk", "Motlanthe"))
data$president <- as.factor(data$president)

data_count <- data

data_count <- data_count %>%
  mutate(ids_ = row_number())
```

```{r}
uni_ <- data_count %>%
  unnest_character_shingles(token, speech, n = 4, strip_non_alphanum = F, to_lower = F) 

uni_tdf <-  uni_ %>%
  group_by(ids_, token) %>%
  count() %>%
  group_by(ids_) %>%
  mutate(total = sum(n)) %>%
  ungroup()

uni_tdf <- uni_tdf %>%
  bind_tf_idf(token, ids_, n)

tfidf <- uni_tdf %>% 
  select(ids_, token, tf_idf) %>%  # note the change, using tf-idf
  pivot_wider(names_from = token, values_from = tf_idf, values_fill = 0)


tfidf <- data_count %>%
  select(ids_, president) %>%
  rename (Response_President = president) %>%
  right_join(tfidf, join_by("ids_")) %>%
  select(ids_, Response_President, everything()) 
```

```{r}
train.set    <- tfidf[tfidf$ids_ %in% train, -1]
val.set      <- tfidf[tfidf$ids_ %in% val, -1]
test.set     <- tfidf[tfidf$ids_ %in% test, -1]


# Bayes Classifier
data_classifier <- naiveBayes(train.set[,-1], train.set[,1])
p1 <- predict(data_classifier, val.set[,-1])
tt <- table((p1), (unlist(val.set[,1])))
1 - sum(diag(tt))/ sum(tt)

# SVM
classifier = svm(train.set[,-1], train.set[,1], type = "C-classification", scale = F)
p1 <- predict(classifier, val.set[,-1])
tt <- table((p1), (unlist(val.set[,1])))

tail(p1)
str((unlist(val.set[,1])))

1 - sum(diag(tt))/ sum(tt)
```

# Results

<!-- Create one big table to show results -->

# Class Imbalance

Look out for speaker disfluencies, such things may be able to aid in the identification of speech

The idea here is that each president should be represented equally in their training and validation. It is not neccessary to concern oneself with the equality of representation in the test set. For this reason, it would make sense to extract a sample of the same size for each president, regardless of how many speeches they've done.

Another thing to try is to simply remove the outliers Mothlale and deKlerk as they each only provided one speech. It is possible that looking into such characters may in fact skew the nature of this report in the predicting ability of the models.

Another method is to look into inversely proportional. This is almost the same idea as having equal representations.

Let's train the data on a sample of 1000 for each president and see how it fairs. Howver, I do know that these 1000 samples cannot feature the opening statements because they all say the openning statements
